{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"udspy","text":"<p>A minimal DSPy-inspired library with native OpenAI tool calling.</p>"},{"location":"#overview","title":"Overview","text":"<p>udspy provides a clean, minimal abstraction for building LLM-powered applications with structured inputs and outputs. Inspired by DSPy, it focuses on simplicity and leverages OpenAI's native tool calling capabilities.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Pydantic-based Signatures: Define clear input/output contracts using Pydantic models</li> <li>Native Tool Calling: First-class support for OpenAI's function calling API</li> <li>Module Abstraction: Compose LLM calls into reusable, testable modules</li> <li>Streaming Support: Stream reasoning and outputs incrementally for better UX</li> <li>Minimal Dependencies: Only requires <code>openai</code> and <code>pydantic</code></li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install udspy\n</code></pre> <p>Or with uv:</p> <pre><code>uv add udspy\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import udspy\nfrom udspy import Signature, InputField, OutputField, Predict\n\n# Configure OpenAI client\nudspy.settings.configure(api_key=\"your-api-key\")\n\n# Define a signature\nclass QA(Signature):\n    \"\"\"Answer questions concisely.\"\"\"\n    question: str = InputField(description=\"Question to answer\")\n    answer: str = OutputField(description=\"Concise answer\")\n\n# Create and use a predictor\npredictor = Predict(QA)\nresult = predictor(question=\"What is the capital of France?\")\nprint(result.answer)  # \"Paris\"\n</code></pre>"},{"location":"#philosophy","title":"Philosophy","text":"<p>udspy is designed with these principles:</p> <ol> <li>Simplicity First: Start minimal, iterate based on real needs</li> <li>Type Safety: Leverage Pydantic for runtime validation</li> <li>Native Integration: Use platform features (like OpenAI tools) instead of reinventing</li> <li>Testability: Make it easy to test LLM-powered code</li> <li>Composability: Build complex behavior from simple, reusable modules</li> </ol>"},{"location":"#comparison-with-dspy","title":"Comparison with DSPy","text":"Feature udspy DSPy Input/Output Definition Pydantic models Custom signatures Tool Calling Native OpenAI tools Custom adapter layer Streaming Built-in async support Complex callback system Dependencies 2 (openai, pydantic) Many Focus Minimal, opinionated Full-featured framework"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Read the Architecture Overview</li> <li>Check out Examples</li> <li>Browse the API Reference</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE file for details.</p>"},{"location":"api/adapter/","title":"API Reference: Adapters","text":""},{"location":"api/adapter/#udspy.adapter","title":"<code>udspy.adapter</code>","text":"<p>Adapter for formatting LLM inputs/outputs with Pydantic models.</p>"},{"location":"api/adapter/#udspy.adapter-classes","title":"Classes","text":""},{"location":"api/adapter/#udspy.adapter.ChatAdapter","title":"<code>ChatAdapter</code>","text":"<p>Adapter for formatting signatures into OpenAI chat messages.</p> <p>This adapter converts Signature inputs into properly formatted chat messages and parses LLM responses back into structured outputs.</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>class ChatAdapter:\n    \"\"\"Adapter for formatting signatures into OpenAI chat messages.\n\n    This adapter converts Signature inputs into properly formatted\n    chat messages and parses LLM responses back into structured outputs.\n    \"\"\"\n\n    def format_instructions(self, signature: type[Signature]) -&gt; str:\n        \"\"\"Format signature instructions and field descriptions.\n\n        Args:\n            signature: The signature to format\n\n        Returns:\n            Formatted instruction string\n        \"\"\"\n        parts = []\n\n        # Add main instructions\n        instructions = signature.get_instructions()\n        if instructions:\n            parts.append(instructions)\n\n        # Add input field descriptions\n        input_fields = signature.get_input_fields()\n        if input_fields:\n            parts.append(\"\\n**Inputs:**\")\n            for name, field_info in input_fields.items():\n                desc = field_info.description or \"\"\n                parts.append(f\"- `{name}`: {desc}\")\n\n        # Add output field descriptions\n        output_fields = signature.get_output_fields()\n        if output_fields:\n            parts.append(\"\\n**Required Outputs:**\")\n            for name, field_info in output_fields.items():\n                desc = field_info.description or \"\"\n                parts.append(f\"- `{name}`: {desc}\")\n\n        # Add output format instructions\n        parts.append(\"\\n**Output Format:**\\nStructure your response with clear field markers:\\n\")\n        for name in output_fields:\n            parts.append(f\"[[ ## {name} ## ]]\\n&lt;your {name} here&gt;\")\n\n        return \"\\n\".join(parts)\n\n    def format_inputs(\n        self,\n        signature: type[Signature],\n        inputs: dict[str, Any],\n    ) -&gt; str:\n        \"\"\"Format input values into a message.\n\n        Args:\n            signature: The signature defining expected inputs\n            inputs: Dictionary of input values\n\n        Returns:\n            Formatted input string\n        \"\"\"\n        parts = []\n        input_fields = signature.get_input_fields()\n\n        for name, _field_info in input_fields.items():\n            if name in inputs:\n                value = inputs[name]\n                formatted = format_value(value)\n                parts.append(f\"[[ ## {name} ## ]]\\n{formatted}\")\n\n        return \"\\n\\n\".join(parts)\n\n    def parse_outputs(\n        self,\n        signature: type[Signature],\n        completion: str,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Parse LLM completion into structured outputs.\n\n        Uses regex to extract field content, ignoring any text before/after markers.\n        Field content is stripped of leading/trailing whitespace (including newlines).\n\n        Args:\n            signature: The signature defining expected outputs\n            completion: Raw completion string from LLM\n\n        Returns:\n            Dictionary of parsed output values\n        \"\"\"\n        output_fields = signature.get_output_fields()\n        outputs: dict[str, Any] = {}\n\n        # Pattern: [[ ## field_name ## ]] followed by content until next marker or end\n        # (?:...) = non-capturing group\n        # [\\s\\S]*? = match any character (including newlines) non-greedily\n        pattern = r\"\\[\\[\\s*##\\s*(\\w+)\\s*##\\s*\\]\\]\\s*\\n?([\\s\\S]*?)(?=\\[\\[\\s*##\\s*\\w+\\s*##\\s*\\]\\]|$)\"\n\n        for match in re.finditer(pattern, completion):\n            field_name = match.group(1).strip()\n            content = match.group(\n                2\n            ).strip()  # strip() removes leading/trailing whitespace and newlines\n\n            if field_name in output_fields:\n                field_info = output_fields[field_name]\n\n                # Parse according to field type\n                try:\n                    outputs[field_name] = parse_value(content, field_info.annotation)  # type: ignore[arg-type]\n                except Exception:\n                    # Fallback: keep as string\n                    outputs[field_name] = content\n\n        return outputs\n\n    def format_tool_schemas(self, tools: list[Any]) -&gt; list[dict[str, Any]]:\n        \"\"\"Convert Tool objects or Pydantic models to OpenAI tool schemas.\n\n        Args:\n            tools: List of Tool objects or Pydantic model classes\n\n        Returns:\n            List of OpenAI tool schema dictionaries\n        \"\"\"\n        from udspy.tool import Tool\n\n        tool_schemas = []\n\n        for tool_item in tools:\n            if isinstance(tool_item, Tool):\n                # Tool decorator - use its built-in schema conversion\n                tool_schemas.append(tool_item.to_openai_schema())\n            else:\n                # Pydantic model - convert using existing logic\n                tool_model = tool_item\n                schema = tool_model.model_json_schema()\n\n                # Extract description from docstring or schema\n                description = (\n                    tool_model.__doc__.strip()\n                    if tool_model.__doc__\n                    else schema.get(\"description\", f\"Use {tool_model.__name__}\")\n                )\n\n                # Build OpenAI function schema\n                tool_schema = {\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": schema.get(\"title\", tool_model.__name__),\n                        \"description\": description,\n                        \"parameters\": {\n                            \"type\": \"object\",\n                            \"properties\": schema.get(\"properties\", {}),\n                            \"required\": schema.get(\"required\", []),\n                            \"additionalProperties\": False,\n                        },\n                    },\n                }\n\n                # Remove $defs if present (internal Pydantic references)\n                if \"$defs\" in tool_schema[\"function\"][\"parameters\"]:  # type: ignore[index]\n                    del tool_schema[\"function\"][\"parameters\"][\"$defs\"]  # type: ignore[index]\n\n                tool_schemas.append(tool_schema)\n\n        return tool_schemas\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter-functions","title":"Functions","text":""},{"location":"api/adapter/#udspy.adapter.ChatAdapter.format_inputs","title":"<code>format_inputs(signature, inputs)</code>","text":"<p>Format input values into a message.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature]</code> <p>The signature defining expected inputs</p> required <code>inputs</code> <code>dict[str, Any]</code> <p>Dictionary of input values</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted input string</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def format_inputs(\n    self,\n    signature: type[Signature],\n    inputs: dict[str, Any],\n) -&gt; str:\n    \"\"\"Format input values into a message.\n\n    Args:\n        signature: The signature defining expected inputs\n        inputs: Dictionary of input values\n\n    Returns:\n        Formatted input string\n    \"\"\"\n    parts = []\n    input_fields = signature.get_input_fields()\n\n    for name, _field_info in input_fields.items():\n        if name in inputs:\n            value = inputs[name]\n            formatted = format_value(value)\n            parts.append(f\"[[ ## {name} ## ]]\\n{formatted}\")\n\n    return \"\\n\\n\".join(parts)\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.format_instructions","title":"<code>format_instructions(signature)</code>","text":"<p>Format signature instructions and field descriptions.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature]</code> <p>The signature to format</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted instruction string</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def format_instructions(self, signature: type[Signature]) -&gt; str:\n    \"\"\"Format signature instructions and field descriptions.\n\n    Args:\n        signature: The signature to format\n\n    Returns:\n        Formatted instruction string\n    \"\"\"\n    parts = []\n\n    # Add main instructions\n    instructions = signature.get_instructions()\n    if instructions:\n        parts.append(instructions)\n\n    # Add input field descriptions\n    input_fields = signature.get_input_fields()\n    if input_fields:\n        parts.append(\"\\n**Inputs:**\")\n        for name, field_info in input_fields.items():\n            desc = field_info.description or \"\"\n            parts.append(f\"- `{name}`: {desc}\")\n\n    # Add output field descriptions\n    output_fields = signature.get_output_fields()\n    if output_fields:\n        parts.append(\"\\n**Required Outputs:**\")\n        for name, field_info in output_fields.items():\n            desc = field_info.description or \"\"\n            parts.append(f\"- `{name}`: {desc}\")\n\n    # Add output format instructions\n    parts.append(\"\\n**Output Format:**\\nStructure your response with clear field markers:\\n\")\n    for name in output_fields:\n        parts.append(f\"[[ ## {name} ## ]]\\n&lt;your {name} here&gt;\")\n\n    return \"\\n\".join(parts)\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.format_tool_schemas","title":"<code>format_tool_schemas(tools)</code>","text":"<p>Convert Tool objects or Pydantic models to OpenAI tool schemas.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>list[Any]</code> <p>List of Tool objects or Pydantic model classes</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of OpenAI tool schema dictionaries</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def format_tool_schemas(self, tools: list[Any]) -&gt; list[dict[str, Any]]:\n    \"\"\"Convert Tool objects or Pydantic models to OpenAI tool schemas.\n\n    Args:\n        tools: List of Tool objects or Pydantic model classes\n\n    Returns:\n        List of OpenAI tool schema dictionaries\n    \"\"\"\n    from udspy.tool import Tool\n\n    tool_schemas = []\n\n    for tool_item in tools:\n        if isinstance(tool_item, Tool):\n            # Tool decorator - use its built-in schema conversion\n            tool_schemas.append(tool_item.to_openai_schema())\n        else:\n            # Pydantic model - convert using existing logic\n            tool_model = tool_item\n            schema = tool_model.model_json_schema()\n\n            # Extract description from docstring or schema\n            description = (\n                tool_model.__doc__.strip()\n                if tool_model.__doc__\n                else schema.get(\"description\", f\"Use {tool_model.__name__}\")\n            )\n\n            # Build OpenAI function schema\n            tool_schema = {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": schema.get(\"title\", tool_model.__name__),\n                    \"description\": description,\n                    \"parameters\": {\n                        \"type\": \"object\",\n                        \"properties\": schema.get(\"properties\", {}),\n                        \"required\": schema.get(\"required\", []),\n                        \"additionalProperties\": False,\n                    },\n                },\n            }\n\n            # Remove $defs if present (internal Pydantic references)\n            if \"$defs\" in tool_schema[\"function\"][\"parameters\"]:  # type: ignore[index]\n                del tool_schema[\"function\"][\"parameters\"][\"$defs\"]  # type: ignore[index]\n\n            tool_schemas.append(tool_schema)\n\n    return tool_schemas\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.parse_outputs","title":"<code>parse_outputs(signature, completion)</code>","text":"<p>Parse LLM completion into structured outputs.</p> <p>Uses regex to extract field content, ignoring any text before/after markers. Field content is stripped of leading/trailing whitespace (including newlines).</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature]</code> <p>The signature defining expected outputs</p> required <code>completion</code> <code>str</code> <p>Raw completion string from LLM</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of parsed output values</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def parse_outputs(\n    self,\n    signature: type[Signature],\n    completion: str,\n) -&gt; dict[str, Any]:\n    \"\"\"Parse LLM completion into structured outputs.\n\n    Uses regex to extract field content, ignoring any text before/after markers.\n    Field content is stripped of leading/trailing whitespace (including newlines).\n\n    Args:\n        signature: The signature defining expected outputs\n        completion: Raw completion string from LLM\n\n    Returns:\n        Dictionary of parsed output values\n    \"\"\"\n    output_fields = signature.get_output_fields()\n    outputs: dict[str, Any] = {}\n\n    # Pattern: [[ ## field_name ## ]] followed by content until next marker or end\n    # (?:...) = non-capturing group\n    # [\\s\\S]*? = match any character (including newlines) non-greedily\n    pattern = r\"\\[\\[\\s*##\\s*(\\w+)\\s*##\\s*\\]\\]\\s*\\n?([\\s\\S]*?)(?=\\[\\[\\s*##\\s*\\w+\\s*##\\s*\\]\\]|$)\"\n\n    for match in re.finditer(pattern, completion):\n        field_name = match.group(1).strip()\n        content = match.group(\n            2\n        ).strip()  # strip() removes leading/trailing whitespace and newlines\n\n        if field_name in output_fields:\n            field_info = output_fields[field_name]\n\n            # Parse according to field type\n            try:\n                outputs[field_name] = parse_value(content, field_info.annotation)  # type: ignore[arg-type]\n            except Exception:\n                # Fallback: keep as string\n                outputs[field_name] = content\n\n    return outputs\n</code></pre>"},{"location":"api/adapter/#udspy.adapter-functions","title":"Functions","text":""},{"location":"api/adapter/#udspy.adapter.format_value","title":"<code>format_value(value)</code>","text":"<p>Format a value for inclusion in a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to format</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted string representation</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def format_value(value: Any) -&gt; str:\n    \"\"\"Format a value for inclusion in a prompt.\n\n    Args:\n        value: The value to format\n\n    Returns:\n        Formatted string representation\n    \"\"\"\n    if isinstance(value, str):\n        return value\n    elif isinstance(value, (list, dict)):\n        return json.dumps(value, indent=2)\n    elif isinstance(value, BaseModel):\n        return value.model_dump_json(indent=2)\n    else:\n        return str(value)\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.parse_value","title":"<code>parse_value(value_str, type_)</code>","text":"<p>Parse a string value into the specified type.</p> <p>Parameters:</p> Name Type Description Default <code>value_str</code> <code>str</code> <p>String value to parse</p> required <code>type_</code> <code>type</code> <p>Target type</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Parsed value</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def parse_value(value_str: str, type_: type) -&gt; Any:\n    \"\"\"Parse a string value into the specified type.\n\n    Args:\n        value_str: String value to parse\n        type_: Target type\n\n    Returns:\n        Parsed value\n    \"\"\"\n    # Handle strings\n    if type_ is str:\n        return value_str.strip()\n\n    # Handle numeric types\n    if type_ is int:\n        return int(value_str.strip())\n    if type_ is float:\n        return float(value_str.strip())\n    if type_ is bool:\n        return value_str.strip().lower() in (\"true\", \"yes\", \"1\")\n\n    # Handle Pydantic models\n    try:\n        if isinstance(type_, type) and issubclass(type_, BaseModel):\n            # Try parsing as JSON first\n            try:\n                data = json.loads(value_str)\n                return type_.model_validate(data)\n            except json.JSONDecodeError:\n                # Fallback: treat as JSON string\n                return type_.model_validate_json(value_str)\n    except (TypeError, ValueError):\n        pass\n\n    # Handle lists and dicts\n    try:\n        parsed = json.loads(value_str)\n        if isinstance(parsed, (list, dict)):\n            return parsed\n    except json.JSONDecodeError:\n        pass\n\n    # Fallback: return as string\n    return value_str.strip()\n</code></pre>"},{"location":"api/history/","title":"History API Reference","text":"<p>The <code>History</code> class manages conversation history for multi-turn interactions.</p>"},{"location":"api/history/#class-history","title":"Class: History","text":"<pre><code>from udspy import History\n</code></pre>"},{"location":"api/history/#constructor","title":"Constructor","text":"<pre><code>History(messages: list[dict[str, Any]] | None = None)\n</code></pre> <p>Create a new History instance.</p> <p>Parameters: - <code>messages</code> (optional): Initial list of messages in OpenAI format</p> <p>Example: <pre><code># Empty history\nhistory = History()\n\n# With initial messages\nhistory = History(messages=[\n    {\"role\": \"system\", \"content\": \"You are helpful\"},\n    {\"role\": \"user\", \"content\": \"Hello\"}\n])\n</code></pre></p>"},{"location":"api/history/#attributes","title":"Attributes","text":""},{"location":"api/history/#messages","title":"messages","text":"<pre><code>history.messages: list[dict[str, Any]]\n</code></pre> <p>List of conversation messages in OpenAI format. Each message is a dictionary with at minimum: - <code>role</code>: One of \"system\", \"user\", \"assistant\", or \"tool\" - <code>content</code>: Message content string</p> <p>Assistant messages with tool calls also include: - <code>tool_calls</code>: List of tool call dictionaries</p> <p>Tool messages also include: - <code>tool_call_id</code>: ID of the tool call this result is for</p> <p>Example: <pre><code>for msg in history.messages:\n    print(f\"{msg['role']}: {msg['content']}\")\n</code></pre></p>"},{"location":"api/history/#methods","title":"Methods","text":""},{"location":"api/history/#add_message","title":"add_message","text":"<pre><code>add_message(\n    role: str,\n    content: str,\n    *,\n    tool_calls: list[dict[str, Any]] | None = None\n) -&gt; None\n</code></pre> <p>Add a message to the history.</p> <p>Parameters: - <code>role</code>: Message role (\"system\", \"user\", \"assistant\", \"tool\") - <code>content</code>: Message content - <code>tool_calls</code> (optional): Tool calls for assistant messages</p> <p>Example: <pre><code>history.add_message(\"user\", \"What is AI?\")\nhistory.add_message(\"assistant\", \"AI is...\", tool_calls=[...])\n</code></pre></p>"},{"location":"api/history/#add_user_message","title":"add_user_message","text":"<pre><code>add_user_message(content: str) -&gt; None\n</code></pre> <p>Add a user message.</p> <p>Parameters: - <code>content</code>: User message content</p> <p>Example: <pre><code>history.add_user_message(\"Tell me about Python\")\n</code></pre></p>"},{"location":"api/history/#add_assistant_message","title":"add_assistant_message","text":"<pre><code>add_assistant_message(\n    content: str,\n    tool_calls: list[dict[str, Any]] | None = None\n) -&gt; None\n</code></pre> <p>Add an assistant message.</p> <p>Parameters: - <code>content</code>: Assistant message content - <code>tool_calls</code> (optional): Tool calls made by assistant</p> <p>Example: <pre><code>history.add_assistant_message(\"Python is a programming language...\")\n</code></pre></p>"},{"location":"api/history/#add_system_message","title":"add_system_message","text":"<pre><code>add_system_message(content: str) -&gt; None\n</code></pre> <p>Add a system message.</p> <p>Parameters: - <code>content</code>: System message content</p> <p>Example: <pre><code>history.add_system_message(\"You are a helpful coding tutor\")\n</code></pre></p>"},{"location":"api/history/#add_tool_result","title":"add_tool_result","text":"<pre><code>add_tool_result(tool_call_id: str, content: str) -&gt; None\n</code></pre> <p>Add a tool result message.</p> <p>Parameters: - <code>tool_call_id</code>: ID of the tool call this result is for - <code>content</code>: Tool result content</p> <p>Example: <pre><code>history.add_tool_result(\"call_123\", \"Result: 42\")\n</code></pre></p>"},{"location":"api/history/#clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear all messages from history.</p> <p>Example: <pre><code>history.clear()\nprint(len(history))  # 0\n</code></pre></p>"},{"location":"api/history/#copy","title":"copy","text":"<pre><code>copy() -&gt; History\n</code></pre> <p>Create a copy of this history.</p> <p>Returns: - New <code>History</code> instance with copied messages</p> <p>Example: <pre><code>branch = history.copy()\n# Modify branch without affecting original\nbranch.add_user_message(\"New question\")\n</code></pre></p>"},{"location":"api/history/#magic-methods","title":"Magic Methods","text":""},{"location":"api/history/#__len__","title":"<code>__len__</code>","text":"<pre><code>len(history) -&gt; int\n</code></pre> <p>Get number of messages in history.</p> <p>Example: <pre><code>print(len(history))  # e.g., 5\n</code></pre></p>"},{"location":"api/history/#__repr__","title":"<code>__repr__</code>","text":"<pre><code>repr(history) -&gt; str\n</code></pre> <p>String representation showing number of messages.</p> <p>Example: <pre><code>print(repr(history))  # \"History(5 messages)\"\n</code></pre></p>"},{"location":"api/history/#__str__","title":"<code>__str__</code>","text":"<pre><code>str(history) -&gt; str\n</code></pre> <p>Human-readable formatted conversation history.</p> <p>Example: <pre><code>print(history)\n# History (3 messages):\n#   1. [user] What is Python?\n#   2. [assistant] Python is a programming language...\n#   3. [user] What are its features?\n</code></pre></p>"},{"location":"api/history/#usage-with-predict","title":"Usage with Predict","text":"<p>History integrates seamlessly with <code>Predict</code>:</p> <pre><code>from udspy import Predict, History\n\npredictor = Predict(QA)\nhistory = History()\n\n# History is automatically updated with each call\nresult = predictor(question=\"First question\", history=history)\nresult = predictor(question=\"Follow-up question\", history=history)\n</code></pre> <p>See History Examples for more usage patterns.</p>"},{"location":"api/interrupt/","title":"Interrupt API Reference","text":"<p>API documentation for the interrupt system that enables human-in-the-loop workflows.</p>"},{"location":"api/interrupt/#module-udspyinterrupt","title":"Module: <code>udspy.interrupt</code>","text":"<p>The interrupt system provides a general-purpose mechanism for pausing execution to request human input or approval. It's designed to be: - Thread-safe: Works correctly with multi-threaded applications - Task-safe: Compatible with asyncio concurrent tasks - Module-agnostic: Can be used by any module, not just ReAct</p>"},{"location":"api/interrupt/#humaninthelooprequired","title":"<code>HumanInTheLoopRequired</code>","text":"<pre><code>class HumanInTheLoopRequired(Exception):\n    \"\"\"Raised when human input is needed to proceed.\"\"\"\n</code></pre> <p>Exception that pauses execution and saves state for resumption. This is the core mechanism for implementing human-in-the-loop workflows.</p>"},{"location":"api/interrupt/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    question: str,\n    *,\n    interrupt_id: str | None = None,\n    tool_call: ToolCall | None = None,\n    context: dict[str, Any] | None = None,\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>question</code> (<code>str</code>): The question to ask the user</li> <li>Should be clear and actionable</li> <li>Example: \"Confirm execution of delete_file with args: {'path': '/tmp/test.txt'}?\"</li> <li><code>interrupt_id</code> (<code>str | None</code>, optional): Unique interrupt identifier</li> <li>Auto-generated UUID if not provided</li> <li>Used to track interrupt status</li> <li><code>tool_call</code> (<code>ToolCall | None</code>, optional): Information about the tool call that triggered this interrupt</li> <li>Contains tool name, arguments, and optional call ID</li> <li>Can be <code>None</code> for non-tool interrupts</li> <li><code>context</code> (<code>dict[str, Any] | None</code>, optional): Module-specific state dictionary</li> <li>Used to save execution state for resumption</li> <li>Each module defines its own context structure</li> </ul> <p>Example:</p> <pre><code>from udspy.interrupt import HumanInTheLoopRequired, ToolCall\n\n# Simple interrupt with just a question\nraise HumanInTheLoopRequired(\"Do you want to proceed?\")\n\n# Interrupt with tool call information\nraise HumanInTheLoopRequired(\n    question=\"Confirm deletion?\",\n    tool_call=ToolCall(\n        name=\"delete_file\",\n        args={\"path\": \"/tmp/test.txt\"}\n    )\n)\n\n# Interrupt with module state\nraise HumanInTheLoopRequired(\n    question=\"Need clarification\",\n    context={\n        \"iteration\": 5,\n        \"trajectory\": {...},\n        \"input_args\": {...}\n    }\n)\n</code></pre>"},{"location":"api/interrupt/#attributes","title":"Attributes","text":""},{"location":"api/interrupt/#question","title":"<code>question</code>","text":"<pre><code>question: str\n</code></pre> <p>The question being asked to the user.</p>"},{"location":"api/interrupt/#interrupt_id","title":"<code>interrupt_id</code>","text":"<pre><code>interrupt_id: str\n</code></pre> <p>Unique identifier for this interrupt. Use with <code>get_interrupt_status()</code> and <code>set_interrupt_approval()</code>.</p>"},{"location":"api/interrupt/#tool_call","title":"<code>tool_call</code>","text":"<pre><code>tool_call: ToolCall | None\n</code></pre> <p>Optional tool call information. See <code>ToolCall</code> class below.</p>"},{"location":"api/interrupt/#context","title":"<code>context</code>","text":"<pre><code>context: dict[str, Any]\n</code></pre> <p>Module-specific state dictionary. Structure depends on the module that raised the exception.</p>"},{"location":"api/interrupt/#toolcall","title":"<code>ToolCall</code>","text":"<pre><code>class ToolCall:\n    \"\"\"Information about a tool call that triggered an interrupt.\"\"\"\n</code></pre> <p>Encapsulates information about a tool invocation.</p>"},{"location":"api/interrupt/#constructor_1","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    name: str,\n    args: dict[str, Any],\n    call_id: str | None = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>name</code> (<code>str</code>): Tool name</li> <li><code>args</code> (<code>dict[str, Any]</code>): Tool arguments as keyword arguments</li> <li><code>call_id</code> (<code>str | None</code>, optional): Call ID from the LLM provider (e.g., OpenAI)</li> </ul> <p>Example:</p> <pre><code>from udspy.interrupt import ToolCall\n\ntool_call = ToolCall(\n    name=\"search\",\n    args={\"query\": \"Python tutorials\"},\n    call_id=\"call_abc123\"\n)\n</code></pre>"},{"location":"api/interrupt/#attributes_1","title":"Attributes","text":""},{"location":"api/interrupt/#name","title":"<code>name</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the tool.</p>"},{"location":"api/interrupt/#args","title":"<code>args</code>","text":"<pre><code>args: dict[str, Any]\n</code></pre> <p>The tool arguments as a dictionary.</p>"},{"location":"api/interrupt/#call_id","title":"<code>call_id</code>","text":"<pre><code>call_id: str | None\n</code></pre> <p>Optional call ID from the LLM provider.</p>"},{"location":"api/interrupt/#interruptible","title":"<code>@interruptible</code>","text":"<pre><code>def interruptible(func: Callable) -&gt; Callable:\n    \"\"\"Decorator that makes a function require approval before execution.\"\"\"\n</code></pre> <p>Decorator that wraps a function to require human approval on first call. Subsequent calls with the same arguments proceed normally after approval.</p> <p>How it works:</p> <ol> <li>First call: Raises <code>HumanInTheLoopRequired</code> with tool call information</li> <li>User approves: Call <code>set_interrupt_approval(interrupt_id, approved=True)</code></li> <li>Subsequent calls: Execute normally if approved</li> </ol> <p>Supports: - Sync and async functions - Positional and keyword arguments - Thread-safe and asyncio task-safe execution</p> <p>Example:</p> <pre><code>from udspy.interrupt import interruptible, HumanInTheLoopRequired, set_interrupt_approval\n\n@interruptible\ndef delete_file(path: str) -&gt; str:\n    os.remove(path)\n    return f\"Deleted {path}\"\n\n# First call raises\ntry:\n    delete_file(\"/tmp/test.txt\")\nexcept HumanInTheLoopRequired as e:\n    print(e.question)  # \"Confirm execution of delete_file...\"\n    interrupt_id = e.interrupt_id\n\n    # User approves\n    set_interrupt_approval(interrupt_id, approved=True)\n\n    # Second call succeeds\n    result = delete_file(\"/tmp/test.txt\")\n    print(result)  # \"Deleted /tmp/test.txt\"\n</code></pre> <p>With async functions:</p> <pre><code>@interruptible\nasync def async_delete(path: str) -&gt; str:\n    await asyncio.sleep(0.1)\n    os.remove(path)\n    return f\"Deleted {path}\"\n\ntry:\n    await async_delete(\"/tmp/test.txt\")\nexcept HumanInTheLoopRequired as e:\n    set_interrupt_approval(e.interrupt_id, approved=True)\n    result = await async_delete(\"/tmp/test.txt\")\n</code></pre> <p>Modifying arguments:</p> <pre><code>try:\n    delete_file(\"/tmp/test.txt\")\nexcept HumanInTheLoopRequired as e:\n    # Approve with modified arguments\n    modified_args = {\"path\": \"/tmp/safe.txt\"}\n    set_interrupt_approval(e.interrupt_id, approved=True, data=modified_args)\n\n    # Next call uses modified args\n    result = delete_file(\"/tmp/test.txt\")\n    print(result)  # \"Deleted /tmp/safe.txt\"\n</code></pre>"},{"location":"api/interrupt/#get_interrupt_status","title":"<code>get_interrupt_status()</code>","text":"<pre><code>def get_interrupt_status(interrupt_id: str) -&gt; str | None:\n    \"\"\"Get the status of an interrupt.\"\"\"\n</code></pre> <p>Returns the current status of an interrupt by its ID.</p> <p>Parameters:</p> <ul> <li><code>interrupt_id</code> (<code>str</code>): The interrupt ID to query</li> </ul> <p>Returns:</p> <ul> <li><code>str | None</code>: One of:</li> <li><code>\"pending\"</code>: No decision made yet (or ID not found)</li> <li><code>\"approved\"</code>: User approved the action</li> <li><code>\"rejected\"</code>: User rejected the action</li> <li><code>\"edited\"</code>: User approved with modifications</li> <li><code>\"feedback\"</code>: User provided feedback for the agent</li> </ul> <p>Example:</p> <pre><code>from udspy.interrupt import get_interrupt_status, HumanInTheLoopRequired\n\ntry:\n    agent(question=\"Delete files\")\nexcept HumanInTheLoopRequired as e:\n    status = get_interrupt_status(e.interrupt_id)\n    print(status)  # \"pending\"\n\n    # After user responds\n    agent.resume(\"yes\", e)\n    status = get_interrupt_status(e.interrupt_id)\n    print(status)  # \"approved\"\n</code></pre>"},{"location":"api/interrupt/#set_interrupt_approval","title":"<code>set_interrupt_approval()</code>","text":"<pre><code>def set_interrupt_approval(\n    interrupt_id: str,\n    approved: bool = True,\n    data: Any = None,\n    status: str | None = None\n) -&gt; None:\n    \"\"\"Mark an interrupt as approved or rejected.\"\"\"\n</code></pre> <p>Sets the approval status for an interrupt, optionally providing modified data.</p> <p>Parameters:</p> <ul> <li><code>interrupt_id</code> (<code>str</code>): The interrupt ID to update</li> <li><code>approved</code> (<code>bool</code>, default: <code>True</code>): Whether to approve or reject</li> <li><code>True</code>: Allow execution to proceed</li> <li><code>False</code>: Block execution</li> <li><code>data</code> (<code>Any</code>, optional): Modified arguments or feedback data</li> <li>For <code>@interruptible</code> functions: Dict with modified arguments</li> <li>For modules: Any data to pass back</li> <li><code>status</code> (<code>str | None</code>, optional): Explicit status to set</li> <li>If not provided, inferred from <code>approved</code> and <code>data</code></li> <li>Can be: \"approved\", \"rejected\", \"edited\", \"feedback\"</li> </ul> <p>Example:</p> <pre><code>from udspy.interrupt import set_interrupt_approval\n\n# Simple approval\nset_interrupt_approval(\"abc-123\", approved=True)\n\n# Rejection\nset_interrupt_approval(\"abc-123\", approved=False)\n\n# Approval with modified arguments\nset_interrupt_approval(\n    \"abc-123\",\n    approved=True,\n    data={\"path\": \"/safe/location.txt\"}\n)\n\n# Explicit status\nset_interrupt_approval(\n    \"abc-123\",\n    approved=True,\n    status=\"feedback\"\n)\n</code></pre>"},{"location":"api/interrupt/#get_interrupt_context","title":"<code>get_interrupt_context()</code>","text":"<pre><code>def get_interrupt_context() -&gt; dict[str, dict[str, Any]]:\n    \"\"\"Get the current interrupt context dictionary.\"\"\"\n</code></pre> <p>Returns the complete interrupt context for the current thread/task. Mostly used for debugging.</p> <p>Returns:</p> <ul> <li><code>dict[str, dict[str, Any]]</code>: Dictionary mapping interrupt IDs to their state:   <pre><code>{\n    \"interrupt-id-1\": {\n        \"approved\": True,\n        \"data\": {...},\n        \"status\": \"approved\"\n    },\n    \"interrupt-id-2\": {\n        \"approved\": False,\n        \"status\": \"rejected\"\n    }\n}\n</code></pre></li> </ul> <p>Example:</p> <pre><code>from udspy.interrupt import get_interrupt_context\n\ncontext = get_interrupt_context()\nprint(f\"Active interrupts: {len(context)}\")\nfor interrupt_id, state in context.items():\n    print(f\"{interrupt_id}: {state['status']}\")\n</code></pre>"},{"location":"api/interrupt/#clear_interrupt","title":"<code>clear_interrupt()</code>","text":"<pre><code>def clear_interrupt(interrupt_id: str) -&gt; None:\n    \"\"\"Remove an interrupt from the context.\"\"\"\n</code></pre> <p>Clears a specific interrupt from the context. Usually done automatically after successful execution.</p> <p>Parameters:</p> <ul> <li><code>interrupt_id</code> (<code>str</code>): The interrupt ID to clear</li> </ul> <p>Example:</p> <pre><code>from udspy.interrupt import clear_interrupt\n\nclear_interrupt(\"abc-123\")\n</code></pre>"},{"location":"api/interrupt/#clear_all_interrupts","title":"<code>clear_all_interrupts()</code>","text":"<pre><code>def clear_all_interrupts() -&gt; None:\n    \"\"\"Clear all interrupts from the context.\"\"\"\n</code></pre> <p>Removes all interrupts from the current context. Useful for cleanup or testing.</p> <p>Example:</p> <pre><code>from udspy.interrupt import clear_all_interrupts\n\n# Start fresh\nclear_all_interrupts()\n</code></pre>"},{"location":"api/interrupt/#interrupt-status-lifecycle","title":"Interrupt Status Lifecycle","text":"<p>The status of an interrupt follows this lifecycle:</p> <pre><code>pending (initial)\n    \u2193\n    \u251c\u2192 approved (user said \"yes\")\n    \u251c\u2192 rejected (user said \"no\")\n    \u251c\u2192 edited (user modified args)\n    \u2514\u2192 feedback (user provided feedback)\n</code></pre> <p>Status Meanings:</p> <ul> <li>pending: Initial state, no decision made</li> <li>approved: User approved the action as-is</li> <li>rejected: User rejected the action</li> <li>edited: User approved with modifications to arguments</li> <li>feedback: User provided textual feedback (not yes/no)</li> </ul>"},{"location":"api/interrupt/#thread-safety","title":"Thread Safety","text":"<p>The interrupt system uses <code>contextvars.ContextVar</code> for thread-safe and asyncio task-safe storage:</p> <ul> <li>Each thread has its own interrupt context</li> <li>Each asyncio task inherits parent task's context</li> <li>No cross-contamination between threads/tasks</li> </ul> <p>Example:</p> <pre><code>import threading\nfrom udspy.interrupt import interruptible, HumanInTheLoopRequired, set_interrupt_approval\n\n@interruptible\ndef thread_func(thread_id: int) -&gt; str:\n    return f\"Thread {thread_id}\"\n\ndef worker(thread_id: int):\n    try:\n        thread_func(thread_id)\n    except HumanInTheLoopRequired as e:\n        # Each thread has its own interrupt context\n        set_interrupt_approval(e.interrupt_id, approved=True)\n        result = thread_func(thread_id)\n        print(result)\n\nthreads = [threading.Thread(target=worker, args=(i,)) for i in range(3)]\nfor t in threads:\n    t.start()\nfor t in threads:\n    t.join()\n</code></pre>"},{"location":"api/interrupt/#integration-with-modules","title":"Integration with Modules","text":"<p>Modules can use the interrupt system by:</p> <ol> <li>Raising <code>HumanInTheLoopRequired</code> when human input is needed</li> <li>Saving state in the <code>context</code> dict</li> <li>Implementing <code>resume()</code> and <code>aresume()</code> methods to restore state</li> </ol> <p>Example Module:</p> <pre><code>from udspy import Module, Prediction\nfrom udspy.interrupt import HumanInTheLoopRequired\n\nclass MyModule(Module):\n    def forward(self, input: str) -&gt; Prediction:\n        # ... some work ...\n\n        if needs_human_input:\n            raise HumanInTheLoopRequired(\n                question=\"Please confirm\",\n                context={\n                    \"current_step\": 5,\n                    \"partial_result\": \"...\",\n                    \"input\": input\n                }\n            )\n\n        # ... continue work ...\n        return Prediction(output=\"result\")\n\n    def resume(self, user_response: str, saved_state: HumanInTheLoopRequired) -&gt; Prediction:\n        # Restore state from context\n        current_step = saved_state.context[\"current_step\"]\n        partial_result = saved_state.context[\"partial_result\"]\n        input = saved_state.context[\"input\"]\n\n        # Process user response\n        if user_response.lower() == \"yes\":\n            # Continue from where we left off\n            pass\n\n        # ... complete work ...\n        return Prediction(output=\"final result\")\n</code></pre>"},{"location":"api/interrupt/#integration-with-tools","title":"Integration with Tools","text":"<p>Tools can use <code>interruptible=True</code> parameter to require confirmation:</p> <pre><code>from udspy import tool\nfrom pydantic import Field\n\n@tool(\n    name=\"delete_file\",\n    description=\"Delete a file\",\n    interruptible=True  # Wraps function with @interruptible decorator\n)\ndef delete_file(path: str = Field(...)) -&gt; str:\n    os.remove(path)\n    return f\"Deleted {path}\"\n</code></pre> <p>The <code>@tool</code> decorator automatically wraps the function with <code>@interruptible</code> when this parameter is set.</p>"},{"location":"api/interrupt/#see-also","title":"See Also","text":"<ul> <li>ReAct API - ReAct agent that uses the interrupt system</li> <li>Tool API - Creating interruptible tools</li> <li>Module API - Base module with suspend/resume methods</li> </ul>"},{"location":"api/module/","title":"API Reference: Modules","text":""},{"location":"api/module/#udspy.module","title":"<code>udspy.module</code>","text":"<p>Module package for composable LLM calls.</p>"},{"location":"api/module/#udspy.module-classes","title":"Classes","text":""},{"location":"api/module/#udspy.module.ChainOfThought","title":"<code>ChainOfThought</code>","text":"<p>               Bases: <code>Module</code></p> <p>Chain of Thought reasoning module.</p> <p>Automatically adds a reasoning step before generating outputs. This encourages the LLM to think step-by-step, improving answer quality.</p> Example <pre><code>class QA(Signature):\n    '''Answer questions.'''\n    question: str = InputField()\n    answer: str = OutputField()\n\n# Creates predictor with automatic reasoning\npredictor = ChainOfThought(QA)\nresult = predictor(question=\"What is 2+2?\")\n\nprint(result.reasoning)  # \"Let's think step by step...\"\nprint(result.answer)     # \"4\"\n</code></pre> Source code in <code>src/udspy/module/chain_of_thought.py</code> <pre><code>class ChainOfThought(Module):\n    \"\"\"Chain of Thought reasoning module.\n\n    Automatically adds a reasoning step before generating outputs.\n    This encourages the LLM to think step-by-step, improving answer quality.\n\n    Example:\n        ```python\n        class QA(Signature):\n            '''Answer questions.'''\n            question: str = InputField()\n            answer: str = OutputField()\n\n        # Creates predictor with automatic reasoning\n        predictor = ChainOfThought(QA)\n        result = predictor(question=\"What is 2+2?\")\n\n        print(result.reasoning)  # \"Let's think step by step...\"\n        print(result.answer)     # \"4\"\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        signature: type[Signature] | str,\n        *,\n        reasoning_description: str = \"Step-by-step reasoning process\",\n        model: str | None = None,\n        tools: list[Tool] | None = None,\n        adapter: ChatAdapter | None = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Initialize a Chain of Thought module.\n\n        Args:\n            signature: Signature defining inputs and final outputs, or a string in\n                      format \"inputs -&gt; outputs\" (e.g., \"question -&gt; answer\")\n            reasoning_description: Description for the reasoning field\n            model: Model name (overrides global default)\n            tools: List of Pydantic tool models\n            adapter: Custom adapter\n            **kwargs: Additional arguments for chat completion\n        \"\"\"\n        # Convert string signature to Signature class\n        if isinstance(signature, str):\n            signature = Signature.from_string(signature)\n\n        self.original_signature = signature\n\n        # Create extended signature with reasoning field\n        input_fields = {\n            name: field.annotation for name, field in signature.get_input_fields().items()\n        }\n        output_fields = {\n            name: field.annotation for name, field in signature.get_output_fields().items()\n        }\n\n        # Prepend reasoning to outputs\n        extended_outputs = {\"reasoning\": str, **output_fields}\n\n        # Create new signature with reasoning\n        extended_signature = make_signature(\n            input_fields,  # type: ignore[arg-type]\n            extended_outputs,  # type: ignore[arg-type]\n            signature.get_instructions(),\n        )\n\n        # Override reasoning field description\n        extended_signature.model_fields[\"reasoning\"].description = reasoning_description\n\n        # Create predictor with extended signature\n        self.predict = Predict(\n            extended_signature,\n            model=model,\n            tools=tools,\n            adapter=adapter,\n            **kwargs,\n        )\n\n    async def aexecute(  # type: ignore[override]\n        self, *, stream: bool = False, **inputs: Any\n    ) -&gt; AsyncGenerator[StreamEvent, None]:\n        \"\"\"Execute chain of thought prediction.\n\n        Delegates to the wrapped Predict module's aexecute method.\n\n        Args:\n            stream: If True, request streaming from LLM provider\n            **inputs: Input values matching the signature's input fields\n\n        Returns:\n            Prediction with reasoning and other output fields\n        \"\"\"\n        return await self.predict.aexecute(stream=stream, **inputs)\n</code></pre>"},{"location":"api/module/#udspy.module.ChainOfThought-functions","title":"Functions","text":""},{"location":"api/module/#udspy.module.ChainOfThought.__init__","title":"<code>__init__(signature, *, reasoning_description='Step-by-step reasoning process', model=None, tools=None, adapter=None, **kwargs)</code>","text":"<p>Initialize a Chain of Thought module.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature] | str</code> <p>Signature defining inputs and final outputs, or a string in       format \"inputs -&gt; outputs\" (e.g., \"question -&gt; answer\")</p> required <code>reasoning_description</code> <code>str</code> <p>Description for the reasoning field</p> <code>'Step-by-step reasoning process'</code> <code>model</code> <code>str | None</code> <p>Model name (overrides global default)</p> <code>None</code> <code>tools</code> <code>list[Tool] | None</code> <p>List of Pydantic tool models</p> <code>None</code> <code>adapter</code> <code>ChatAdapter | None</code> <p>Custom adapter</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for chat completion</p> <code>{}</code> Source code in <code>src/udspy/module/chain_of_thought.py</code> <pre><code>def __init__(\n    self,\n    signature: type[Signature] | str,\n    *,\n    reasoning_description: str = \"Step-by-step reasoning process\",\n    model: str | None = None,\n    tools: list[Tool] | None = None,\n    adapter: ChatAdapter | None = None,\n    **kwargs: Any,\n):\n    \"\"\"Initialize a Chain of Thought module.\n\n    Args:\n        signature: Signature defining inputs and final outputs, or a string in\n                  format \"inputs -&gt; outputs\" (e.g., \"question -&gt; answer\")\n        reasoning_description: Description for the reasoning field\n        model: Model name (overrides global default)\n        tools: List of Pydantic tool models\n        adapter: Custom adapter\n        **kwargs: Additional arguments for chat completion\n    \"\"\"\n    # Convert string signature to Signature class\n    if isinstance(signature, str):\n        signature = Signature.from_string(signature)\n\n    self.original_signature = signature\n\n    # Create extended signature with reasoning field\n    input_fields = {\n        name: field.annotation for name, field in signature.get_input_fields().items()\n    }\n    output_fields = {\n        name: field.annotation for name, field in signature.get_output_fields().items()\n    }\n\n    # Prepend reasoning to outputs\n    extended_outputs = {\"reasoning\": str, **output_fields}\n\n    # Create new signature with reasoning\n    extended_signature = make_signature(\n        input_fields,  # type: ignore[arg-type]\n        extended_outputs,  # type: ignore[arg-type]\n        signature.get_instructions(),\n    )\n\n    # Override reasoning field description\n    extended_signature.model_fields[\"reasoning\"].description = reasoning_description\n\n    # Create predictor with extended signature\n    self.predict = Predict(\n        extended_signature,\n        model=model,\n        tools=tools,\n        adapter=adapter,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/module/#udspy.module.ChainOfThought.aexecute","title":"<code>aexecute(*, stream=False, **inputs)</code>  <code>async</code>","text":"<p>Execute chain of thought prediction.</p> <p>Delegates to the wrapped Predict module's aexecute method.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>bool</code> <p>If True, request streaming from LLM provider</p> <code>False</code> <code>**inputs</code> <code>Any</code> <p>Input values matching the signature's input fields</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncGenerator[StreamEvent, None]</code> <p>Prediction with reasoning and other output fields</p> Source code in <code>src/udspy/module/chain_of_thought.py</code> <pre><code>async def aexecute(  # type: ignore[override]\n    self, *, stream: bool = False, **inputs: Any\n) -&gt; AsyncGenerator[StreamEvent, None]:\n    \"\"\"Execute chain of thought prediction.\n\n    Delegates to the wrapped Predict module's aexecute method.\n\n    Args:\n        stream: If True, request streaming from LLM provider\n        **inputs: Input values matching the signature's input fields\n\n    Returns:\n        Prediction with reasoning and other output fields\n    \"\"\"\n    return await self.predict.aexecute(stream=stream, **inputs)\n</code></pre>"},{"location":"api/module/#udspy.module.HumanInTheLoopRequired","title":"<code>HumanInTheLoopRequired</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when human input is needed to proceed.</p> <p>This exception pauses execution and allows modules to save state for resumption. It can be raised by: - Tools decorated with @interruptible - Modules that need user input (e.g., ask_to_user) - Custom code requiring human interaction</p> <p>Attributes:</p> Name Type Description <code>question</code> <p>The question being asked to the user</p> <code>interrupt_id</code> <p>Unique ID for this interrupt</p> <code>tool_call</code> <p>Optional ToolCall information if raised by a tool</p> <code>context</code> <p>General-purpose context dictionary for module state</p> Source code in <code>src/udspy/interrupt.py</code> <pre><code>class HumanInTheLoopRequired(Exception):\n    \"\"\"Raised when human input is needed to proceed.\n\n    This exception pauses execution and allows modules to save state for resumption.\n    It can be raised by:\n    - Tools decorated with @interruptible\n    - Modules that need user input (e.g., ask_to_user)\n    - Custom code requiring human interaction\n\n    Attributes:\n        question: The question being asked to the user\n        interrupt_id: Unique ID for this interrupt\n        tool_call: Optional ToolCall information if raised by a tool\n        context: General-purpose context dictionary for module state\n    \"\"\"\n\n    def __init__(\n        self,\n        question: str,\n        *,\n        interrupt_id: str | None = None,\n        tool_call: ToolCall | None = None,\n        context: dict[str, Any] | None = None,\n    ):\n        \"\"\"Initialize HumanInTheLoopRequired exception.\n\n        Args:\n            question: Question to ask the user\n            interrupt_id: Unique ID for this interrupt (auto-generated if not provided)\n            tool_call: Optional tool call information\n            context: Optional context dictionary for module-specific state\n        \"\"\"\n        super().__init__(question)\n        self.question = question\n        self.interrupt_id = interrupt_id or str(uuid.uuid4())\n        self.tool_call = tool_call\n        self.context = context or {}\n</code></pre>"},{"location":"api/module/#udspy.module.HumanInTheLoopRequired-functions","title":"Functions","text":""},{"location":"api/module/#udspy.module.HumanInTheLoopRequired.__init__","title":"<code>__init__(question, *, interrupt_id=None, tool_call=None, context=None)</code>","text":"<p>Initialize HumanInTheLoopRequired exception.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>Question to ask the user</p> required <code>interrupt_id</code> <code>str | None</code> <p>Unique ID for this interrupt (auto-generated if not provided)</p> <code>None</code> <code>tool_call</code> <code>ToolCall | None</code> <p>Optional tool call information</p> <code>None</code> <code>context</code> <code>dict[str, Any] | None</code> <p>Optional context dictionary for module-specific state</p> <code>None</code> Source code in <code>src/udspy/interrupt.py</code> <pre><code>def __init__(\n    self,\n    question: str,\n    *,\n    interrupt_id: str | None = None,\n    tool_call: ToolCall | None = None,\n    context: dict[str, Any] | None = None,\n):\n    \"\"\"Initialize HumanInTheLoopRequired exception.\n\n    Args:\n        question: Question to ask the user\n        interrupt_id: Unique ID for this interrupt (auto-generated if not provided)\n        tool_call: Optional tool call information\n        context: Optional context dictionary for module-specific state\n    \"\"\"\n    super().__init__(question)\n    self.question = question\n    self.interrupt_id = interrupt_id or str(uuid.uuid4())\n    self.tool_call = tool_call\n    self.context = context or {}\n</code></pre>"},{"location":"api/module/#udspy.module.Module","title":"<code>Module</code>","text":"<p>Base class for all udspy modules.</p> <p>Modules are composable async-first units. The core method is <code>aexecute()</code> which handles both streaming and non-streaming execution. Public methods <code>astream()</code> and <code>aforward()</code> are thin wrappers around <code>aexecute()</code>.</p> <p>Subclasses should implement <code>aexecute()</code> to define their behavior.</p> Example <pre><code># Async streaming (real-time)\nasync for event in module.astream(question=\"What is AI?\"):\n    if isinstance(event, StreamChunk):\n        print(event.delta, end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        result = event\n\n# Async non-streaming\nresult = await module.aforward(question=\"What is AI?\")\n\n# Sync (for scripts, notebooks)\nresult = module(question=\"What is AI?\")\nresult = module.forward(question=\"What is AI?\")\n</code></pre> Source code in <code>src/udspy/module/base.py</code> <pre><code>class Module:\n    \"\"\"Base class for all udspy modules.\n\n    Modules are composable async-first units. The core method is `aexecute()`\n    which handles both streaming and non-streaming execution. Public methods\n    `astream()` and `aforward()` are thin wrappers around `aexecute()`.\n\n    Subclasses should implement `aexecute()` to define their behavior.\n\n    Example:\n        ```python\n        # Async streaming (real-time)\n        async for event in module.astream(question=\"What is AI?\"):\n            if isinstance(event, StreamChunk):\n                print(event.delta, end=\"\", flush=True)\n            elif isinstance(event, Prediction):\n                result = event\n\n        # Async non-streaming\n        result = await module.aforward(question=\"What is AI?\")\n\n        # Sync (for scripts, notebooks)\n        result = module(question=\"What is AI?\")\n        result = module.forward(question=\"What is AI?\")\n        ```\n    \"\"\"\n\n    async def aexecute(self, *, stream: bool = False, **inputs: Any) -&gt; Prediction:\n        \"\"\"Core execution method. Must be implemented by subclasses.\n\n        This is the single implementation point for both streaming and non-streaming\n        execution. It always returns a Prediction, and optionally emits StreamEvent\n        objects to the active queue (if one exists in the context).\n\n        Args:\n            stream: If True, request streaming from LLM provider. If False, use\n                non-streaming API calls.\n            **inputs: Input values for the module\n\n        Returns:\n            Final Prediction object\n\n        Behavior:\n            - Checks for active stream queue via _stream_queue.get()\n            - If queue exists: emits StreamChunk and Prediction events\n            - Always returns final Prediction (even in streaming mode)\n            - This enables composability: nested modules emit events automatically\n\n        Raises:\n            NotImplementedError: If not implemented by subclass\n        \"\"\"\n        raise NotImplementedError(f\"{self.__class__.__name__} must implement aexecute() method\")\n\n    async def astream(self, **inputs: Any) -&gt; AsyncGenerator[StreamEvent, None]:\n        \"\"\"Async streaming method. Sets up queue and yields events.\n\n        This method sets up the stream queue context, calls aexecute() with\n        streaming enabled, and yields all events from the queue.\n\n        Args:\n            **inputs: Input values for the module\n\n        Yields:\n            StreamEvent objects (StreamChunk, Prediction, and custom events)\n        \"\"\"\n        from udspy.streaming import _stream_queue\n\n        queue: asyncio.Queue[StreamEvent | None] = asyncio.Queue()\n        token = _stream_queue.set(queue)\n\n        try:\n            task = asyncio.create_task(self.aexecute(stream=True, **inputs))\n\n            while True:\n                if task.done():\n                    try:\n                        await task\n                    except Exception:\n                        raise\n\n                    while not queue.empty():\n                        event = queue.get_nowait()\n                        if event is not None:\n                            yield event\n                    break\n\n                try:\n                    event = await asyncio.wait_for(queue.get(), timeout=0.1)\n                except TimeoutError:\n                    continue\n\n                if event is None:\n                    break\n                yield event\n\n        finally:\n            try:\n                _stream_queue.reset(token)\n            except (ValueError, LookupError):\n                pass\n\n    async def aforward(self, **inputs: Any) -&gt; Prediction:\n        \"\"\"Async non-streaming method. Returns final result directly.\n\n        This method calls aexecute() with streaming disabled. If called from\n        within a streaming context (i.e., another module is streaming), events\n        will still be emitted to the active queue.\n\n        Args:\n            **inputs: Input values for the module\n\n        Returns:\n            Final Prediction object\n        \"\"\"\n        return await self.aexecute(stream=False, **inputs)\n\n    def forward(self, **inputs: Any) -&gt; Prediction:\n        \"\"\"Sync non-streaming method. Wraps aforward() with async_to_sync.\n\n        This provides sync compatibility for scripts and notebooks. Cannot be\n        called from within an async context (use aforward() instead).\n\n        Args:\n            **inputs: Input values for the module (includes both input fields\n                and any module-specific parameters like auto_execute_tools)\n\n        Returns:\n            Final Prediction object\n\n        Raises:\n            RuntimeError: If called from within an async context\n        \"\"\"\n        # Check if we're already in an async context\n        try:\n            asyncio.get_running_loop()\n            raise RuntimeError(\n                f\"Cannot call {self.__class__.__name__}.forward() from async context. \"\n                f\"Use 'await {self.__class__.__name__[0].lower() + self.__class__.__name__[1:]}.aforward(...)' instead.\"\n            )\n        except RuntimeError as e:\n            # No running loop - we're in sync context, proceed\n            if \"no running event loop\" not in str(e).lower():\n                raise\n\n        # Run async code from sync context\n        return asyncio.run(self.aforward(**inputs))\n\n    def __call__(self, **inputs: Any) -&gt; Prediction:\n        \"\"\"Sync convenience method. Calls forward().\n\n        Args:\n            **inputs: Input values for the module\n\n        Returns:\n            Final Prediction object\n        \"\"\"\n        return self.forward(**inputs)\n\n    async def asuspend(self, exception: HumanInTheLoopRequired) -&gt; Any:\n        \"\"\"Async suspend execution and save state.\n\n        Called when HumanInTheLoopRequired is raised. Subclasses should override\n        to save any module-specific state needed for resumption.\n\n        Args:\n            exception: The HumanInTheLoopRequired exception that was raised\n\n        Returns:\n            Saved state (can be any type, will be passed to aresume)\n        \"\"\"\n        # Default implementation returns the exception itself as state\n        return exception\n\n    def suspend(self, exception: HumanInTheLoopRequired) -&gt; Any:\n        \"\"\"Sync suspend execution and save state.\n\n        Wraps asuspend() with async_to_sync.\n\n        Args:\n            exception: The HumanInTheLoopRequired exception that was raised\n\n        Returns:\n            Saved state (can be any type, will be passed to resume)\n        \"\"\"\n        try:\n            asyncio.get_running_loop()\n            raise RuntimeError(\n                f\"Cannot call {self.__class__.__name__}.suspend() from async context. \"\n                f\"Use 'await {self.__class__.__name__[0].lower() + self.__class__.__name__[1:]}.asuspend(...)' instead.\"\n            )\n        except RuntimeError as e:\n            if \"no running event loop\" not in str(e).lower():\n                raise\n\n        return asyncio.run(self.asuspend(exception))\n\n    async def aresume(self, user_response: str, saved_state: Any) -&gt; Prediction:\n        \"\"\"Async resume execution after user input.\n\n        Called to resume execution after a HumanInTheLoopRequired exception.\n        Subclasses must override to implement resumption logic.\n\n        Args:\n            user_response: The user's response. Can be:\n                - \"yes\"/\"y\" to approve the action\n                - \"no\"/\"n\" to reject the action\n                - \"feedback\" to provide feedback for LLM re-reasoning\n                - JSON string with \"edit\" to modify tool arguments\n            saved_state: State returned from asuspend()\n\n        Returns:\n            Final Prediction object\n\n        Raises:\n            NotImplementedError: If not implemented by subclass\n        \"\"\"\n        raise NotImplementedError(f\"{self.__class__.__name__} must implement aresume() method\")\n\n    def resume(self, user_response: str, saved_state: Any) -&gt; Prediction:\n        \"\"\"Sync resume execution after user input.\n\n        Wraps aresume() with async_to_sync.\n\n        Args:\n            user_response: The user's response\n            saved_state: State returned from suspend()\n\n        Returns:\n            Final Prediction object\n        \"\"\"\n        try:\n            asyncio.get_running_loop()\n            raise RuntimeError(\n                f\"Cannot call {self.__class__.__name__}.resume() from async context. \"\n                f\"Use 'await {self.__class__.__name__[0].lower() + self.__class__.__name__[1:]}.aresume(...)' instead.\"\n            )\n        except RuntimeError as e:\n            if \"no running event loop\" not in str(e).lower():\n                raise\n\n        return asyncio.run(self.aresume(user_response, saved_state))\n</code></pre>"},{"location":"api/module/#udspy.module.Module-functions","title":"Functions","text":""},{"location":"api/module/#udspy.module.Module.__call__","title":"<code>__call__(**inputs)</code>","text":"<p>Sync convenience method. Calls forward().</p> <p>Parameters:</p> Name Type Description Default <code>**inputs</code> <code>Any</code> <p>Input values for the module</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>def __call__(self, **inputs: Any) -&gt; Prediction:\n    \"\"\"Sync convenience method. Calls forward().\n\n    Args:\n        **inputs: Input values for the module\n\n    Returns:\n        Final Prediction object\n    \"\"\"\n    return self.forward(**inputs)\n</code></pre>"},{"location":"api/module/#udspy.module.Module.aexecute","title":"<code>aexecute(*, stream=False, **inputs)</code>  <code>async</code>","text":"<p>Core execution method. Must be implemented by subclasses.</p> <p>This is the single implementation point for both streaming and non-streaming execution. It always returns a Prediction, and optionally emits StreamEvent objects to the active queue (if one exists in the context).</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>bool</code> <p>If True, request streaming from LLM provider. If False, use non-streaming API calls.</p> <code>False</code> <code>**inputs</code> <code>Any</code> <p>Input values for the module</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object</p> Behavior <ul> <li>Checks for active stream queue via _stream_queue.get()</li> <li>If queue exists: emits StreamChunk and Prediction events</li> <li>Always returns final Prediction (even in streaming mode)</li> <li>This enables composability: nested modules emit events automatically</li> </ul> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by subclass</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>async def aexecute(self, *, stream: bool = False, **inputs: Any) -&gt; Prediction:\n    \"\"\"Core execution method. Must be implemented by subclasses.\n\n    This is the single implementation point for both streaming and non-streaming\n    execution. It always returns a Prediction, and optionally emits StreamEvent\n    objects to the active queue (if one exists in the context).\n\n    Args:\n        stream: If True, request streaming from LLM provider. If False, use\n            non-streaming API calls.\n        **inputs: Input values for the module\n\n    Returns:\n        Final Prediction object\n\n    Behavior:\n        - Checks for active stream queue via _stream_queue.get()\n        - If queue exists: emits StreamChunk and Prediction events\n        - Always returns final Prediction (even in streaming mode)\n        - This enables composability: nested modules emit events automatically\n\n    Raises:\n        NotImplementedError: If not implemented by subclass\n    \"\"\"\n    raise NotImplementedError(f\"{self.__class__.__name__} must implement aexecute() method\")\n</code></pre>"},{"location":"api/module/#udspy.module.Module.aforward","title":"<code>aforward(**inputs)</code>  <code>async</code>","text":"<p>Async non-streaming method. Returns final result directly.</p> <p>This method calls aexecute() with streaming disabled. If called from within a streaming context (i.e., another module is streaming), events will still be emitted to the active queue.</p> <p>Parameters:</p> Name Type Description Default <code>**inputs</code> <code>Any</code> <p>Input values for the module</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>async def aforward(self, **inputs: Any) -&gt; Prediction:\n    \"\"\"Async non-streaming method. Returns final result directly.\n\n    This method calls aexecute() with streaming disabled. If called from\n    within a streaming context (i.e., another module is streaming), events\n    will still be emitted to the active queue.\n\n    Args:\n        **inputs: Input values for the module\n\n    Returns:\n        Final Prediction object\n    \"\"\"\n    return await self.aexecute(stream=False, **inputs)\n</code></pre>"},{"location":"api/module/#udspy.module.Module.aresume","title":"<code>aresume(user_response, saved_state)</code>  <code>async</code>","text":"<p>Async resume execution after user input.</p> <p>Called to resume execution after a HumanInTheLoopRequired exception. Subclasses must override to implement resumption logic.</p> <p>Parameters:</p> Name Type Description Default <code>user_response</code> <code>str</code> <p>The user's response. Can be: - \"yes\"/\"y\" to approve the action - \"no\"/\"n\" to reject the action - \"feedback\" to provide feedback for LLM re-reasoning - JSON string with \"edit\" to modify tool arguments</p> required <code>saved_state</code> <code>Any</code> <p>State returned from asuspend()</p> required <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by subclass</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>async def aresume(self, user_response: str, saved_state: Any) -&gt; Prediction:\n    \"\"\"Async resume execution after user input.\n\n    Called to resume execution after a HumanInTheLoopRequired exception.\n    Subclasses must override to implement resumption logic.\n\n    Args:\n        user_response: The user's response. Can be:\n            - \"yes\"/\"y\" to approve the action\n            - \"no\"/\"n\" to reject the action\n            - \"feedback\" to provide feedback for LLM re-reasoning\n            - JSON string with \"edit\" to modify tool arguments\n        saved_state: State returned from asuspend()\n\n    Returns:\n        Final Prediction object\n\n    Raises:\n        NotImplementedError: If not implemented by subclass\n    \"\"\"\n    raise NotImplementedError(f\"{self.__class__.__name__} must implement aresume() method\")\n</code></pre>"},{"location":"api/module/#udspy.module.Module.astream","title":"<code>astream(**inputs)</code>  <code>async</code>","text":"<p>Async streaming method. Sets up queue and yields events.</p> <p>This method sets up the stream queue context, calls aexecute() with streaming enabled, and yields all events from the queue.</p> <p>Parameters:</p> Name Type Description Default <code>**inputs</code> <code>Any</code> <p>Input values for the module</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[StreamEvent, None]</code> <p>StreamEvent objects (StreamChunk, Prediction, and custom events)</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>async def astream(self, **inputs: Any) -&gt; AsyncGenerator[StreamEvent, None]:\n    \"\"\"Async streaming method. Sets up queue and yields events.\n\n    This method sets up the stream queue context, calls aexecute() with\n    streaming enabled, and yields all events from the queue.\n\n    Args:\n        **inputs: Input values for the module\n\n    Yields:\n        StreamEvent objects (StreamChunk, Prediction, and custom events)\n    \"\"\"\n    from udspy.streaming import _stream_queue\n\n    queue: asyncio.Queue[StreamEvent | None] = asyncio.Queue()\n    token = _stream_queue.set(queue)\n\n    try:\n        task = asyncio.create_task(self.aexecute(stream=True, **inputs))\n\n        while True:\n            if task.done():\n                try:\n                    await task\n                except Exception:\n                    raise\n\n                while not queue.empty():\n                    event = queue.get_nowait()\n                    if event is not None:\n                        yield event\n                break\n\n            try:\n                event = await asyncio.wait_for(queue.get(), timeout=0.1)\n            except TimeoutError:\n                continue\n\n            if event is None:\n                break\n            yield event\n\n    finally:\n        try:\n            _stream_queue.reset(token)\n        except (ValueError, LookupError):\n            pass\n</code></pre>"},{"location":"api/module/#udspy.module.Module.asuspend","title":"<code>asuspend(exception)</code>  <code>async</code>","text":"<p>Async suspend execution and save state.</p> <p>Called when HumanInTheLoopRequired is raised. Subclasses should override to save any module-specific state needed for resumption.</p> <p>Parameters:</p> Name Type Description Default <code>exception</code> <code>HumanInTheLoopRequired</code> <p>The HumanInTheLoopRequired exception that was raised</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Saved state (can be any type, will be passed to aresume)</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>async def asuspend(self, exception: HumanInTheLoopRequired) -&gt; Any:\n    \"\"\"Async suspend execution and save state.\n\n    Called when HumanInTheLoopRequired is raised. Subclasses should override\n    to save any module-specific state needed for resumption.\n\n    Args:\n        exception: The HumanInTheLoopRequired exception that was raised\n\n    Returns:\n        Saved state (can be any type, will be passed to aresume)\n    \"\"\"\n    # Default implementation returns the exception itself as state\n    return exception\n</code></pre>"},{"location":"api/module/#udspy.module.Module.forward","title":"<code>forward(**inputs)</code>","text":"<p>Sync non-streaming method. Wraps aforward() with async_to_sync.</p> <p>This provides sync compatibility for scripts and notebooks. Cannot be called from within an async context (use aforward() instead).</p> <p>Parameters:</p> Name Type Description Default <code>**inputs</code> <code>Any</code> <p>Input values for the module (includes both input fields and any module-specific parameters like auto_execute_tools)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called from within an async context</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>def forward(self, **inputs: Any) -&gt; Prediction:\n    \"\"\"Sync non-streaming method. Wraps aforward() with async_to_sync.\n\n    This provides sync compatibility for scripts and notebooks. Cannot be\n    called from within an async context (use aforward() instead).\n\n    Args:\n        **inputs: Input values for the module (includes both input fields\n            and any module-specific parameters like auto_execute_tools)\n\n    Returns:\n        Final Prediction object\n\n    Raises:\n        RuntimeError: If called from within an async context\n    \"\"\"\n    # Check if we're already in an async context\n    try:\n        asyncio.get_running_loop()\n        raise RuntimeError(\n            f\"Cannot call {self.__class__.__name__}.forward() from async context. \"\n            f\"Use 'await {self.__class__.__name__[0].lower() + self.__class__.__name__[1:]}.aforward(...)' instead.\"\n        )\n    except RuntimeError as e:\n        # No running loop - we're in sync context, proceed\n        if \"no running event loop\" not in str(e).lower():\n            raise\n\n    # Run async code from sync context\n    return asyncio.run(self.aforward(**inputs))\n</code></pre>"},{"location":"api/module/#udspy.module.Module.resume","title":"<code>resume(user_response, saved_state)</code>","text":"<p>Sync resume execution after user input.</p> <p>Wraps aresume() with async_to_sync.</p> <p>Parameters:</p> Name Type Description Default <code>user_response</code> <code>str</code> <p>The user's response</p> required <code>saved_state</code> <code>Any</code> <p>State returned from suspend()</p> required <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>def resume(self, user_response: str, saved_state: Any) -&gt; Prediction:\n    \"\"\"Sync resume execution after user input.\n\n    Wraps aresume() with async_to_sync.\n\n    Args:\n        user_response: The user's response\n        saved_state: State returned from suspend()\n\n    Returns:\n        Final Prediction object\n    \"\"\"\n    try:\n        asyncio.get_running_loop()\n        raise RuntimeError(\n            f\"Cannot call {self.__class__.__name__}.resume() from async context. \"\n            f\"Use 'await {self.__class__.__name__[0].lower() + self.__class__.__name__[1:]}.aresume(...)' instead.\"\n        )\n    except RuntimeError as e:\n        if \"no running event loop\" not in str(e).lower():\n            raise\n\n    return asyncio.run(self.aresume(user_response, saved_state))\n</code></pre>"},{"location":"api/module/#udspy.module.Module.suspend","title":"<code>suspend(exception)</code>","text":"<p>Sync suspend execution and save state.</p> <p>Wraps asuspend() with async_to_sync.</p> <p>Parameters:</p> Name Type Description Default <code>exception</code> <code>HumanInTheLoopRequired</code> <p>The HumanInTheLoopRequired exception that was raised</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Saved state (can be any type, will be passed to resume)</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>def suspend(self, exception: HumanInTheLoopRequired) -&gt; Any:\n    \"\"\"Sync suspend execution and save state.\n\n    Wraps asuspend() with async_to_sync.\n\n    Args:\n        exception: The HumanInTheLoopRequired exception that was raised\n\n    Returns:\n        Saved state (can be any type, will be passed to resume)\n    \"\"\"\n    try:\n        asyncio.get_running_loop()\n        raise RuntimeError(\n            f\"Cannot call {self.__class__.__name__}.suspend() from async context. \"\n            f\"Use 'await {self.__class__.__name__[0].lower() + self.__class__.__name__[1:]}.asuspend(...)' instead.\"\n        )\n    except RuntimeError as e:\n        if \"no running event loop\" not in str(e).lower():\n            raise\n\n    return asyncio.run(self.asuspend(exception))\n</code></pre>"},{"location":"api/module/#udspy.module.Predict","title":"<code>Predict</code>","text":"<p>               Bases: <code>Module</code></p> <p>Module for making LLM predictions based on a signature.</p> <p>This is an async-first module. The core method is <code>astream()</code> which yields StreamEvent objects. Use <code>aforward()</code> for async non-streaming, or <code>forward()</code> for sync usage.</p> Example <pre><code>from udspy import Predict, Signature, InputField, OutputField\n\nclass QA(Signature):\n    '''Answer questions.'''\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\n\n# Sync usage\nresult = predictor(question=\"What is 2+2?\")\nprint(result.answer)\n\n# Async non-streaming\nresult = await predictor.aforward(question=\"What is 2+2?\")\n\n# Async streaming\nasync for event in predictor.astream(question=\"What is 2+2?\"):\n    if isinstance(event, StreamChunk):\n        print(event.delta, end=\"\", flush=True)\n</code></pre> Source code in <code>src/udspy/module/predict.py</code> <pre><code>class Predict(Module):\n    \"\"\"Module for making LLM predictions based on a signature.\n\n    This is an async-first module. The core method is `astream()` which yields\n    StreamEvent objects. Use `aforward()` for async non-streaming, or `forward()`\n    for sync usage.\n\n    Example:\n        ```python\n        from udspy import Predict, Signature, InputField, OutputField\n\n        class QA(Signature):\n            '''Answer questions.'''\n            question: str = InputField()\n            answer: str = OutputField()\n\n        predictor = Predict(QA)\n\n        # Sync usage\n        result = predictor(question=\"What is 2+2?\")\n        print(result.answer)\n\n        # Async non-streaming\n        result = await predictor.aforward(question=\"What is 2+2?\")\n\n        # Async streaming\n        async for event in predictor.astream(question=\"What is 2+2?\"):\n            if isinstance(event, StreamChunk):\n                print(event.delta, end=\"\", flush=True)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        signature: type[Signature] | str,\n        *,\n        model: str | None = None,\n        tools: list[\"Tool\"] | None = None,\n        max_turns: int = 10,\n        adapter: ChatAdapter | None = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Initialize a Predict module.\n\n        Args:\n            signature: Signature defining inputs and outputs, or a string in\n                      format \"inputs -&gt; outputs\" (e.g., \"question -&gt; answer\")\n            model: Model name (overrides global default)\n            tools: List of tool functions (decorated with @tool) or Pydantic models\n            max_turns: Maximum number of LLM calls for tool execution loop (default: 10)\n            adapter: Custom adapter (defaults to ChatAdapter)\n            **kwargs: Additional arguments for chat completion (temperature, etc.)\n        \"\"\"\n        from udspy.tool import Tool\n\n        # Convert string signature to Signature class\n        if isinstance(signature, str):\n            signature = Signature.from_string(signature)\n\n        self.signature = signature\n        self.model = model or settings.default_model\n        self.max_turns = max_turns\n        self.adapter = adapter or ChatAdapter()\n        self.kwargs = {**settings.default_kwargs, **kwargs}\n\n        self.tool_callables: dict[str, Tool] = {}\n        self.tool_schemas: list[Any] = []\n\n        for tool in tools or []:\n            self.tool_callables[tool.name] = tool\n            self.tool_schemas.append(tool)\n\n    async def aexecute(\n        self,\n        *,\n        stream: bool = False,\n        auto_execute_tools: bool = True,\n        history: Optional[\"History\"] = None,\n        **inputs: Any,\n    ) -&gt; Prediction:\n        \"\"\"Core execution method - handles both streaming and non-streaming.\n\n        This is the single implementation point for LLM interaction. It always\n        returns a Prediction, and emits events to the queue if one is active.\n\n        Args:\n            stream: If True, request streaming from OpenAI. If False, use regular API.\n            auto_execute_tools: If True, automatically execute tools and continue.\n                If False, return Prediction with tool_calls for manual handling.\n            history: Optional History object for multi-turn conversations.\n            **inputs: Input values matching the signature's input fields\n\n        Returns:\n            Final Prediction object (after all tool executions if auto_execute_tools=True)\n        \"\"\"\n        from udspy.streaming import _stream_queue\n\n        queue = _stream_queue.get()\n        should_emit = queue is not None\n\n        self._validate_inputs(inputs)\n        messages = self._build_initial_messages(inputs, history)\n\n        final_prediction = await self._execute_with_tools(\n            messages,\n            stream=stream,\n            should_emit=should_emit,\n            auto_execute_tools=auto_execute_tools,\n            history=history,\n        )\n\n        if should_emit and queue:\n            await queue.put(final_prediction)\n\n        return final_prediction\n\n    async def astream(\n        self, *, auto_execute_tools: bool = True, history: Optional[\"History\"] = None, **inputs: Any\n    ) -&gt; AsyncGenerator[StreamEvent, None]:\n        \"\"\"Async streaming method with optional automatic tool execution.\n\n        Sets up streaming queue and yields events. Automatically handles multi-turn\n        conversation when tools are present.\n\n        Args:\n            auto_execute_tools: If True, automatically execute tools and continue.\n                If False, return Prediction with tool_calls for manual handling.\n            history: Optional History object for multi-turn conversations.\n            **inputs: Input values matching the signature's input fields\n\n        Yields:\n            StreamEvent objects (StreamChunk, Prediction, custom events)\n        \"\"\"\n        async for event in super().astream(\n            auto_execute_tools=auto_execute_tools, history=history, **inputs\n        ):\n            yield event\n\n    def _validate_inputs(self, inputs: dict[str, Any]) -&gt; None:\n        \"\"\"Validate that all required inputs are provided.\"\"\"\n        input_fields = self.signature.get_input_fields()\n        for field_name in input_fields:\n            if field_name not in inputs:\n                raise ValueError(f\"Missing required input field: {field_name}\")\n\n    def _build_initial_messages(\n        self, inputs: dict[str, Any], history: Any = None\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Build initial messages from inputs and optional history.\n\n        Args:\n            inputs: Input values from user\n            history: Optional History object with existing conversation\n\n        Returns:\n            List of messages including history (if provided) and new user input\n        \"\"\"\n        from udspy.history import History\n\n        messages: list[dict[str, Any]] = []\n\n        if history is not None:\n            if isinstance(history, History):\n                messages.extend(history.messages)\n            else:\n                raise TypeError(f\"history must be a History object, got {type(history)}\")\n\n        if not messages or messages[0][\"role\"] != \"system\":\n            messages.insert(\n                0, {\"role\": \"system\", \"content\": self.adapter.format_instructions(self.signature)}\n            )\n\n        user_content = self.adapter.format_inputs(self.signature, inputs)\n        user_msg = {\"role\": \"user\", \"content\": user_content}\n        messages.append(user_msg)\n\n        if history is not None and isinstance(history, History):\n            history.messages.append(user_msg)\n\n        return messages\n\n    async def _execute_with_tools(\n        self,\n        messages: list[dict[str, Any]],\n        stream: bool,\n        should_emit: bool,\n        auto_execute_tools: bool,\n        history: Any = None,\n    ) -&gt; Prediction:\n        \"\"\"Execute multi-turn conversation with optional automatic tool execution.\n\n        This is the core execution loop that handles both streaming and non-streaming.\n        It always returns a final Prediction, and emits events if should_emit is True.\n\n        Args:\n            messages: Conversation messages\n            stream: If True, request streaming from OpenAI\n            should_emit: If True, emit events to active queue\n            auto_execute_tools: If True, automatically execute tools. If False,\n                return after first tool call.\n            history: Optional History object to update with conversation\n\n        Returns:\n            Final Prediction object\n        \"\"\"\n        final_prediction: Prediction | None = None\n\n        for turn in range(self.max_turns):\n            final_prediction = await self._execute_one_turn(\n                messages, turn, stream=stream, should_emit=should_emit\n            )\n\n            if history is not None and final_prediction:\n                self._update_history_with_prediction(history, final_prediction)\n\n            if not (final_prediction and \"tool_calls\" in final_prediction):\n                break\n\n            if not auto_execute_tools:\n                break\n\n            if not self.tool_callables:\n                break\n\n            self._execute_tool_calls(messages, final_prediction.tool_calls, history)\n\n        if turn &gt;= self.max_turns - 1 and final_prediction and \"tool_calls\" in final_prediction:\n            raise RuntimeError(f\"Max turns ({self.max_turns}) reached without final answer\")\n\n        if final_prediction is None:\n            raise RuntimeError(\"No prediction generated\")\n\n        return final_prediction\n\n    async def _execute_one_turn(\n        self, messages: list[dict[str, Any]], turn: int, stream: bool, should_emit: bool\n    ) -&gt; Prediction:\n        \"\"\"Execute one LLM turn (streaming or non-streaming).\n\n        Args:\n            messages: Conversation messages\n            turn: Current turn number (0-indexed)\n            stream: If True, request streaming from OpenAI\n            should_emit: If True, emit events to active queue\n\n        Returns:\n            Prediction object for this turn\n        \"\"\"\n        completion_kwargs: dict[str, Any] = {\n            \"model\": self.model,\n            \"messages\": messages,\n            \"stream\": stream,\n            **self.kwargs,\n        }\n\n        if turn == 0 and self.tool_schemas:\n            tool_schemas = self.adapter.format_tool_schemas(self.tool_schemas)\n            completion_kwargs[\"tools\"] = tool_schemas\n\n        if stream:\n            return await self._process_streaming(completion_kwargs, should_emit)\n        else:\n            return await self._process_nonstreaming(completion_kwargs, should_emit)\n\n    def _execute_tool_calls(\n        self, messages: list[dict[str, Any]], tool_calls: list[dict[str, Any]], history: Any = None\n    ) -&gt; None:\n        \"\"\"Execute tool calls and add results to messages.\n\n        Args:\n            messages: Conversation messages\n            tool_calls: List of tool calls to execute\n            history: Optional History object to update\n        \"\"\"\n        import json\n\n        assistant_msg = {\n            \"role\": \"assistant\",\n            \"content\": \"\",\n            \"tool_calls\": [\n                {\n                    \"id\": tc[\"id\"],\n                    \"type\": \"function\",\n                    \"function\": {\"name\": tc[\"name\"], \"arguments\": tc[\"arguments\"]},\n                }\n                for tc in tool_calls\n            ],\n        }\n        messages.append(assistant_msg)\n\n        if history is not None:\n            from udspy.history import History\n\n            if isinstance(history, History):\n                history.messages.append(assistant_msg)\n\n        for tool_call in tool_calls:\n            tool_name = tool_call[\"name\"]\n            arguments = json.loads(tool_call[\"arguments\"])\n\n            if tool_name in self.tool_callables:\n                try:\n                    result = self.tool_callables[tool_name](**arguments)\n                    content = str(result)\n                except Exception as e:\n                    content = f\"Error executing tool: {e}\"\n            else:\n                content = f\"Error: Tool {tool_name} not found\"\n\n            tool_msg = {\"role\": \"tool\", \"tool_call_id\": tool_call[\"id\"], \"content\": content}\n            messages.append(tool_msg)\n\n            if history is not None:\n                from udspy.history import History\n\n                if isinstance(history, History):\n                    history.messages.append(tool_msg)\n\n    def _update_history_with_prediction(self, history: Any, prediction: Prediction) -&gt; None:\n        \"\"\"Update history with assistant's prediction.\n\n        Args:\n            history: History object to update\n            prediction: Prediction from assistant\n        \"\"\"\n        from udspy.history import History\n\n        if not isinstance(history, History):\n            return\n\n        output_fields = self.signature.get_output_fields()\n        content_parts = []\n\n        for field_name in output_fields:\n            if hasattr(prediction, field_name):\n                value = getattr(prediction, field_name)\n                if value:\n                    content_parts.append(f\"[[ ## {field_name} ## ]]\\n{value}\")\n\n        content = \"\\n\".join(content_parts) if content_parts else \"\"\n\n        if hasattr(prediction, \"tool_calls\") and prediction.tool_calls:\n            pass\n        else:\n            history.add_assistant_message(content)\n\n    async def aforward(\n        self, *, auto_execute_tools: bool = True, history: Any = None, **inputs: Any\n    ) -&gt; Prediction:\n        \"\"\"Async non-streaming method. Returns the final Prediction.\n\n        Calls aexecute() with streaming disabled. If called from within a\n        streaming context (i.e., another module is streaming), events will\n        still be emitted to the active queue.\n\n        When tools are used with auto_execute_tools=True (default), this returns\n        the LAST prediction (after tool execution), not the first one (which might\n        only contain tool_calls). When auto_execute_tools=False, returns the first\n        Prediction with tool_calls for manual handling.\n\n        Args:\n            auto_execute_tools: If True, automatically execute tools and return\n                final answer. If False, return Prediction with tool_calls for\n                manual execution. Default: True.\n            history: Optional History object for multi-turn conversations.\n            **inputs: Input values for the module\n\n        Returns:\n            Final Prediction object (after all tool executions if auto_execute_tools=True)\n        \"\"\"\n        return await self.aexecute(\n            stream=False, auto_execute_tools=auto_execute_tools, history=history, **inputs\n        )\n\n    def _process_tool_call_delta(\n        self, tool_calls: dict[int, dict[str, Any]], delta_tool_calls: list[Any]\n    ) -&gt; None:\n        \"\"\"Process tool call deltas and accumulate them.\n\n        Args:\n            tool_calls: Dictionary to accumulate tool calls in\n            delta_tool_calls: List of tool call deltas from the chunk\n        \"\"\"\n        for tool_call in delta_tool_calls:\n            idx = tool_call.index\n            if idx not in tool_calls:\n                tool_calls[idx] = {\n                    \"id\": tool_call.id or \"\",\n                    \"type\": tool_call.type or \"function\",\n                    \"function\": {\n                        \"name\": tool_call.function.name if tool_call.function else \"\",\n                        \"arguments\": \"\",\n                    },\n                }\n\n            if tool_call.function and tool_call.function.arguments:\n                tool_calls[idx][\"function\"][\"arguments\"] += tool_call.function.arguments\n\n    async def _process_content_delta(\n        self,\n        delta: str,\n        acc_delta: str,\n        current_field: str | None,\n        accumulated_content: dict[str, list[str]],\n        output_fields: dict[str, Any],\n        field_pattern: re.Pattern[str],\n        queue: asyncio.Queue[StreamEvent | None],\n    ) -&gt; tuple[str, str | None]:\n        \"\"\"Process content delta and stream field chunks.\n\n        Args:\n            delta: New content delta\n            acc_delta: Accumulated delta so far\n            current_field: Current field being processed\n            accumulated_content: Dictionary of accumulated content per field\n            output_fields: Output fields from signature\n            field_pattern: Regex pattern for field markers\n            queue: Event queue to put chunks in\n\n        Returns:\n            Tuple of (updated acc_delta, updated current_field)\n        \"\"\"\n        acc_delta += delta\n\n        if not acc_delta:\n            return acc_delta, current_field\n\n        match = field_pattern.search(acc_delta)\n        if match:\n            if current_field:\n                field_content = \"\".join(accumulated_content[current_field])\n                await queue.put(\n                    StreamChunk(self, current_field, \"\", field_content, is_complete=True)\n                )\n\n            current_field = match.group(1)\n            acc_delta = field_pattern.sub(\"\", acc_delta)\n            if acc_delta.startswith(\"\\n\"):\n                acc_delta = acc_delta[1:]\n\n        if (\n            current_field\n            and current_field in output_fields\n            and not field_pattern.match(acc_delta, partial=True)\n        ):\n            accumulated_content[current_field].append(acc_delta)\n            field_content = \"\".join(accumulated_content[current_field])\n            await queue.put(\n                StreamChunk(self, current_field, acc_delta, field_content, is_complete=False)\n            )\n            acc_delta = \"\"\n\n        return acc_delta, current_field\n\n    async def _process_nonstreaming(\n        self, completion_kwargs: dict[str, Any], should_emit: bool\n    ) -&gt; Prediction:\n        \"\"\"Process non-streaming LLM call.\n\n        Args:\n            completion_kwargs: Arguments for the completion API call\n            should_emit: If True, emit events to active queue\n\n        Returns:\n            Prediction object\n        \"\"\"\n        from udspy.streaming import _stream_queue\n\n        client = settings.aclient\n        response = await client.chat.completions.create(**completion_kwargs)\n\n        message = response.choices[0].message\n        completion_text = message.content or \"\"\n        tool_calls_data = message.tool_calls\n\n        outputs = self.adapter.parse_outputs(self.signature, completion_text)\n\n        if tool_calls_data:\n            outputs[\"tool_calls\"] = [\n                {\n                    \"id\": tc.id,\n                    \"name\": tc.function.name,\n                    \"arguments\": tc.function.arguments,\n                }\n                for tc in tool_calls_data\n            ]\n\n        prediction = Prediction(**outputs)\n\n        if should_emit:\n            queue = _stream_queue.get()\n            if queue is not None:\n                output_fields = self.signature.get_output_fields()\n                for field_name in output_fields:\n                    if hasattr(prediction, field_name):\n                        field_value = getattr(prediction, field_name)\n                        if field_value:\n                            from udspy.streaming import StreamChunk\n\n                            await queue.put(\n                                StreamChunk(\n                                    self, field_name, field_value, field_value, is_complete=True\n                                )\n                            )\n\n                await queue.put(prediction)\n\n        return prediction\n\n    async def _process_streaming(\n        self, completion_kwargs: dict[str, Any], should_emit: bool\n    ) -&gt; Prediction:\n        \"\"\"Process streaming LLM call.\n\n        Args:\n            completion_kwargs: Arguments for the completion API call\n            should_emit: If True, emit events to active queue from context\n\n        Returns:\n            Prediction object\n        \"\"\"\n        from udspy.streaming import _stream_queue\n\n        active_queue = _stream_queue.get()\n\n        if should_emit and active_queue is not None:\n            return await self._process_llm_stream(\n                active_queue, completion_kwargs, emit_sentinel=False, emit_prediction=False\n            )\n        else:\n            queue: asyncio.Queue[StreamEvent | None] = asyncio.Queue()\n            llm_task = asyncio.create_task(\n                self._process_llm_stream(queue, completion_kwargs, emit_sentinel=True)\n            )\n\n            final_prediction: Prediction | None = None\n            while True:\n                event = await queue.get()\n                if event is None:\n                    break\n\n                if isinstance(event, Prediction):\n                    final_prediction = event\n\n            await llm_task\n\n            if final_prediction is None:\n                raise RuntimeError(\"No prediction generated from stream\")\n\n            return final_prediction\n\n    async def _process_llm_stream(\n        self,\n        queue: asyncio.Queue[StreamEvent | None],\n        completion_kwargs: dict[str, Any],\n        emit_sentinel: bool = True,\n        emit_prediction: bool = True,\n    ) -&gt; Prediction:\n        \"\"\"Background task to process LLM stream and put events in queue.\n\n        Args:\n            queue: Event queue to put events in\n            completion_kwargs: Arguments for the completion API call\n            emit_sentinel: If True, emit None sentinel at the end\n            emit_prediction: If True, emit final Prediction to queue\n\n        Returns:\n            Final Prediction object\n        \"\"\"\n        try:\n            client = settings.aclient\n            stream: AsyncStream[ChatCompletionChunk] = await client.chat.completions.create(\n                **completion_kwargs\n            )\n\n            output_fields = self.signature.get_output_fields()\n            field_pattern = re.compile(r\"\\[\\[ ## (\\w+) ## \\]\\]\")\n            current_field: str | None = None\n            accumulated_content: dict[str, list[str]] = {name: [] for name in output_fields}\n            full_completion: list[str] = []\n            acc_delta: str = \"\"\n            tool_calls: dict[int, dict[str, Any]] = {}\n\n            async for chunk in stream:\n                choice = chunk.choices[0]\n\n                if choice.delta.tool_calls:\n                    self._process_tool_call_delta(tool_calls, choice.delta.tool_calls)\n\n                delta = choice.delta.content or \"\"\n                if delta:\n                    full_completion.append(delta)\n                    acc_delta, current_field = await self._process_content_delta(\n                        delta,\n                        acc_delta,\n                        current_field,\n                        accumulated_content,\n                        output_fields,\n                        field_pattern,\n                        queue,\n                    )\n\n            if current_field:\n                field_content = \"\".join(accumulated_content[current_field])\n                await queue.put(\n                    StreamChunk(self, current_field, \"\", field_content, is_complete=True)\n                )\n\n            completion_text = \"\".join(full_completion)\n            outputs = self.adapter.parse_outputs(self.signature, completion_text)\n\n            if tool_calls:\n                outputs[\"tool_calls\"] = [\n                    {\n                        \"id\": tc[\"id\"],\n                        \"name\": tc[\"function\"][\"name\"],\n                        \"arguments\": tc[\"function\"][\"arguments\"],\n                    }\n                    for tc in tool_calls.values()\n                ]\n\n            prediction = Prediction(**outputs)\n            if emit_prediction:\n                await queue.put(prediction)\n            return prediction\n\n        except Exception as e:\n            import traceback\n\n            error_event = type(\n                \"StreamError\",\n                (StreamEvent,),\n                {\"error\": str(e), \"traceback\": traceback.format_exc()},\n            )()\n            await queue.put(error_event)\n            raise\n        finally:\n            if emit_sentinel:\n                await queue.put(None)\n</code></pre>"},{"location":"api/module/#udspy.module.Predict-functions","title":"Functions","text":""},{"location":"api/module/#udspy.module.Predict.__init__","title":"<code>__init__(signature, *, model=None, tools=None, max_turns=10, adapter=None, **kwargs)</code>","text":"<p>Initialize a Predict module.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature] | str</code> <p>Signature defining inputs and outputs, or a string in       format \"inputs -&gt; outputs\" (e.g., \"question -&gt; answer\")</p> required <code>model</code> <code>str | None</code> <p>Model name (overrides global default)</p> <code>None</code> <code>tools</code> <code>list[Tool] | None</code> <p>List of tool functions (decorated with @tool) or Pydantic models</p> <code>None</code> <code>max_turns</code> <code>int</code> <p>Maximum number of LLM calls for tool execution loop (default: 10)</p> <code>10</code> <code>adapter</code> <code>ChatAdapter | None</code> <p>Custom adapter (defaults to ChatAdapter)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for chat completion (temperature, etc.)</p> <code>{}</code> Source code in <code>src/udspy/module/predict.py</code> <pre><code>def __init__(\n    self,\n    signature: type[Signature] | str,\n    *,\n    model: str | None = None,\n    tools: list[\"Tool\"] | None = None,\n    max_turns: int = 10,\n    adapter: ChatAdapter | None = None,\n    **kwargs: Any,\n):\n    \"\"\"Initialize a Predict module.\n\n    Args:\n        signature: Signature defining inputs and outputs, or a string in\n                  format \"inputs -&gt; outputs\" (e.g., \"question -&gt; answer\")\n        model: Model name (overrides global default)\n        tools: List of tool functions (decorated with @tool) or Pydantic models\n        max_turns: Maximum number of LLM calls for tool execution loop (default: 10)\n        adapter: Custom adapter (defaults to ChatAdapter)\n        **kwargs: Additional arguments for chat completion (temperature, etc.)\n    \"\"\"\n    from udspy.tool import Tool\n\n    # Convert string signature to Signature class\n    if isinstance(signature, str):\n        signature = Signature.from_string(signature)\n\n    self.signature = signature\n    self.model = model or settings.default_model\n    self.max_turns = max_turns\n    self.adapter = adapter or ChatAdapter()\n    self.kwargs = {**settings.default_kwargs, **kwargs}\n\n    self.tool_callables: dict[str, Tool] = {}\n    self.tool_schemas: list[Any] = []\n\n    for tool in tools or []:\n        self.tool_callables[tool.name] = tool\n        self.tool_schemas.append(tool)\n</code></pre>"},{"location":"api/module/#udspy.module.Predict.aexecute","title":"<code>aexecute(*, stream=False, auto_execute_tools=True, history=None, **inputs)</code>  <code>async</code>","text":"<p>Core execution method - handles both streaming and non-streaming.</p> <p>This is the single implementation point for LLM interaction. It always returns a Prediction, and emits events to the queue if one is active.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>bool</code> <p>If True, request streaming from OpenAI. If False, use regular API.</p> <code>False</code> <code>auto_execute_tools</code> <code>bool</code> <p>If True, automatically execute tools and continue. If False, return Prediction with tool_calls for manual handling.</p> <code>True</code> <code>history</code> <code>Optional[History]</code> <p>Optional History object for multi-turn conversations.</p> <code>None</code> <code>**inputs</code> <code>Any</code> <p>Input values matching the signature's input fields</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object (after all tool executions if auto_execute_tools=True)</p> Source code in <code>src/udspy/module/predict.py</code> <pre><code>async def aexecute(\n    self,\n    *,\n    stream: bool = False,\n    auto_execute_tools: bool = True,\n    history: Optional[\"History\"] = None,\n    **inputs: Any,\n) -&gt; Prediction:\n    \"\"\"Core execution method - handles both streaming and non-streaming.\n\n    This is the single implementation point for LLM interaction. It always\n    returns a Prediction, and emits events to the queue if one is active.\n\n    Args:\n        stream: If True, request streaming from OpenAI. If False, use regular API.\n        auto_execute_tools: If True, automatically execute tools and continue.\n            If False, return Prediction with tool_calls for manual handling.\n        history: Optional History object for multi-turn conversations.\n        **inputs: Input values matching the signature's input fields\n\n    Returns:\n        Final Prediction object (after all tool executions if auto_execute_tools=True)\n    \"\"\"\n    from udspy.streaming import _stream_queue\n\n    queue = _stream_queue.get()\n    should_emit = queue is not None\n\n    self._validate_inputs(inputs)\n    messages = self._build_initial_messages(inputs, history)\n\n    final_prediction = await self._execute_with_tools(\n        messages,\n        stream=stream,\n        should_emit=should_emit,\n        auto_execute_tools=auto_execute_tools,\n        history=history,\n    )\n\n    if should_emit and queue:\n        await queue.put(final_prediction)\n\n    return final_prediction\n</code></pre>"},{"location":"api/module/#udspy.module.Predict.aforward","title":"<code>aforward(*, auto_execute_tools=True, history=None, **inputs)</code>  <code>async</code>","text":"<p>Async non-streaming method. Returns the final Prediction.</p> <p>Calls aexecute() with streaming disabled. If called from within a streaming context (i.e., another module is streaming), events will still be emitted to the active queue.</p> <p>When tools are used with auto_execute_tools=True (default), this returns the LAST prediction (after tool execution), not the first one (which might only contain tool_calls). When auto_execute_tools=False, returns the first Prediction with tool_calls for manual handling.</p> <p>Parameters:</p> Name Type Description Default <code>auto_execute_tools</code> <code>bool</code> <p>If True, automatically execute tools and return final answer. If False, return Prediction with tool_calls for manual execution. Default: True.</p> <code>True</code> <code>history</code> <code>Any</code> <p>Optional History object for multi-turn conversations.</p> <code>None</code> <code>**inputs</code> <code>Any</code> <p>Input values for the module</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object (after all tool executions if auto_execute_tools=True)</p> Source code in <code>src/udspy/module/predict.py</code> <pre><code>async def aforward(\n    self, *, auto_execute_tools: bool = True, history: Any = None, **inputs: Any\n) -&gt; Prediction:\n    \"\"\"Async non-streaming method. Returns the final Prediction.\n\n    Calls aexecute() with streaming disabled. If called from within a\n    streaming context (i.e., another module is streaming), events will\n    still be emitted to the active queue.\n\n    When tools are used with auto_execute_tools=True (default), this returns\n    the LAST prediction (after tool execution), not the first one (which might\n    only contain tool_calls). When auto_execute_tools=False, returns the first\n    Prediction with tool_calls for manual handling.\n\n    Args:\n        auto_execute_tools: If True, automatically execute tools and return\n            final answer. If False, return Prediction with tool_calls for\n            manual execution. Default: True.\n        history: Optional History object for multi-turn conversations.\n        **inputs: Input values for the module\n\n    Returns:\n        Final Prediction object (after all tool executions if auto_execute_tools=True)\n    \"\"\"\n    return await self.aexecute(\n        stream=False, auto_execute_tools=auto_execute_tools, history=history, **inputs\n    )\n</code></pre>"},{"location":"api/module/#udspy.module.Predict.astream","title":"<code>astream(*, auto_execute_tools=True, history=None, **inputs)</code>  <code>async</code>","text":"<p>Async streaming method with optional automatic tool execution.</p> <p>Sets up streaming queue and yields events. Automatically handles multi-turn conversation when tools are present.</p> <p>Parameters:</p> Name Type Description Default <code>auto_execute_tools</code> <code>bool</code> <p>If True, automatically execute tools and continue. If False, return Prediction with tool_calls for manual handling.</p> <code>True</code> <code>history</code> <code>Optional[History]</code> <p>Optional History object for multi-turn conversations.</p> <code>None</code> <code>**inputs</code> <code>Any</code> <p>Input values matching the signature's input fields</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[StreamEvent, None]</code> <p>StreamEvent objects (StreamChunk, Prediction, custom events)</p> Source code in <code>src/udspy/module/predict.py</code> <pre><code>async def astream(\n    self, *, auto_execute_tools: bool = True, history: Optional[\"History\"] = None, **inputs: Any\n) -&gt; AsyncGenerator[StreamEvent, None]:\n    \"\"\"Async streaming method with optional automatic tool execution.\n\n    Sets up streaming queue and yields events. Automatically handles multi-turn\n    conversation when tools are present.\n\n    Args:\n        auto_execute_tools: If True, automatically execute tools and continue.\n            If False, return Prediction with tool_calls for manual handling.\n        history: Optional History object for multi-turn conversations.\n        **inputs: Input values matching the signature's input fields\n\n    Yields:\n        StreamEvent objects (StreamChunk, Prediction, custom events)\n    \"\"\"\n    async for event in super().astream(\n        auto_execute_tools=auto_execute_tools, history=history, **inputs\n    ):\n        yield event\n</code></pre>"},{"location":"api/module/#udspy.module.Prediction","title":"<code>Prediction</code>","text":"<p>               Bases: <code>StreamEvent</code>, <code>dict[str, Any]</code></p> <p>Final prediction result with attribute access.</p> <p>This is both a StreamEvent (can be yielded from astream) and a dict (for convenient attribute access to outputs).</p> Example <pre><code>pred = Prediction(answer=\"Paris\", reasoning=\"France's capital\")\nprint(pred.answer)  # \"Paris\"\nprint(pred[\"answer\"])  # \"Paris\"\n</code></pre> Source code in <code>src/udspy/streaming.py</code> <pre><code>class Prediction(StreamEvent, dict[str, Any]):\n    \"\"\"Final prediction result with attribute access.\n\n    This is both a StreamEvent (can be yielded from astream) and a dict\n    (for convenient attribute access to outputs).\n\n    Example:\n        ```python\n        pred = Prediction(answer=\"Paris\", reasoning=\"France's capital\")\n        print(pred.answer)  # \"Paris\"\n        print(pred[\"answer\"])  # \"Paris\"\n        ```\n    \"\"\"\n\n    def __getattr__(self, name: str) -&gt; Any:\n        try:\n            return self[name]\n        except KeyError:\n            raise AttributeError(f\"Prediction has no attribute '{name}'\") from None\n\n    def __setattr__(self, name: str, value: Any) -&gt; None:\n        self[name] = value\n</code></pre>"},{"location":"api/module/#udspy.module.ReAct","title":"<code>ReAct</code>","text":"<p>               Bases: <code>Module</code></p> <p>ReAct (Reasoning and Acting) module for tool-using agents.</p> <p>ReAct iteratively reasons about the current situation and decides whether to call a tool or finish the task. Key features:</p> <ul> <li>Iterative reasoning with tool execution</li> <li>Built-in ask_to_user tool for clarification</li> <li>Tool interruption support for confirmations</li> <li>State saving/restoration for resumption after interrupts</li> <li>Real-time streaming of reasoning and tool usage</li> </ul> <p>Example (Basic Usage):     <pre><code>from udspy import ReAct, Signature, InputField, OutputField, tool\nfrom pydantic import Field\n\n@tool(name=\"search\", description=\"Search for information\")\ndef search(query: str = Field(...)) -&gt; str:\n    return f\"Results for: {query}\"\n\nclass QA(Signature):\n    '''Answer questions using available tools.'''\n    question: str = InputField()\n    answer: str = OutputField()\n\nreact = ReAct(QA, tools=[search])\nresult = react(question=\"What is the weather in Tokyo?\")\n</code></pre></p> <p>Example (Streaming):     <pre><code># Stream the agent's reasoning process in real-time\nasync for event in react.astream(question=\"What is Python?\"):\n    if isinstance(event, StreamChunk):\n        print(event.delta, end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        print(f\"Answer: {event.answer}\")\n</code></pre></p> <pre><code>See examples/react_streaming.py for a complete streaming example.\n</code></pre> <p>Example (Interruptible Tools):     <pre><code>from udspy import HumanInTheLoopRequired, InterruptRejected\n\n@tool(name=\"delete_file\", interruptible=True)\ndef delete_file(path: str = Field(...)) -&gt; str:\n    return f\"Deleted {path}\"\n\nreact = ReAct(QA, tools=[delete_file])\n\ntry:\n    result = await react.aforward(question=\"Delete /tmp/test.txt\")\nexcept HumanInTheLoopRequired as e:\n    # User is asked for confirmation\n    print(f\"Confirm: {e.question}\")\n    # Approve: set_interrupt_approval(e.interrupt_id, approved=True)\n    # Reject: set_interrupt_approval(e.interrupt_id, approved=False, status=\"rejected\")\n    result = await react.aresume(\"yes\", e)\n</code></pre></p> Source code in <code>src/udspy/module/react.py</code> <pre><code>class ReAct(Module):\n    \"\"\"ReAct (Reasoning and Acting) module for tool-using agents.\n\n    ReAct iteratively reasons about the current situation and decides whether\n    to call a tool or finish the task. Key features:\n\n    - Iterative reasoning with tool execution\n    - Built-in ask_to_user tool for clarification\n    - Tool interruption support for confirmations\n    - State saving/restoration for resumption after interrupts\n    - Real-time streaming of reasoning and tool usage\n\n    Example (Basic Usage):\n        ```python\n        from udspy import ReAct, Signature, InputField, OutputField, tool\n        from pydantic import Field\n\n        @tool(name=\"search\", description=\"Search for information\")\n        def search(query: str = Field(...)) -&gt; str:\n            return f\"Results for: {query}\"\n\n        class QA(Signature):\n            '''Answer questions using available tools.'''\n            question: str = InputField()\n            answer: str = OutputField()\n\n        react = ReAct(QA, tools=[search])\n        result = react(question=\"What is the weather in Tokyo?\")\n        ```\n\n    Example (Streaming):\n        ```python\n        # Stream the agent's reasoning process in real-time\n        async for event in react.astream(question=\"What is Python?\"):\n            if isinstance(event, StreamChunk):\n                print(event.delta, end=\"\", flush=True)\n            elif isinstance(event, Prediction):\n                print(f\"Answer: {event.answer}\")\n        ```\n\n        See examples/react_streaming.py for a complete streaming example.\n\n    Example (Interruptible Tools):\n        ```python\n        from udspy import HumanInTheLoopRequired, InterruptRejected\n\n        @tool(name=\"delete_file\", interruptible=True)\n        def delete_file(path: str = Field(...)) -&gt; str:\n            return f\"Deleted {path}\"\n\n        react = ReAct(QA, tools=[delete_file])\n\n        try:\n            result = await react.aforward(question=\"Delete /tmp/test.txt\")\n        except HumanInTheLoopRequired as e:\n            # User is asked for confirmation\n            print(f\"Confirm: {e.question}\")\n            # Approve: set_interrupt_approval(e.interrupt_id, approved=True)\n            # Reject: set_interrupt_approval(e.interrupt_id, approved=False, status=\"rejected\")\n            result = await react.aresume(\"yes\", e)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        signature: type[Signature] | str,\n        tools: list[Callable | Tool],\n        *,\n        max_iters: int = 10,\n        enable_ask_to_user: bool = True,\n    ):\n        \"\"\"Initialize ReAct module.\n\n        Args:\n            signature: Signature defining inputs and outputs, or signature string\n            tools: List of tool functions (decorated with @tool) or Tool objects\n            max_iters: Maximum number of reasoning iterations (default: 10)\n            enable_ask_to_user: Whether to enable ask_to_user tool (default: True)\n        \"\"\"\n        from udspy.tool import Tool\n\n        super().__init__()\n\n        # Convert string signature to Signature class\n        if isinstance(signature, str):\n            signature = Signature.from_string(signature)\n\n        self.signature = signature\n\n        self.max_iters = max_iters\n        self.enable_ask_to_user = enable_ask_to_user\n\n        tool_list = [t if isinstance(t, Tool) else Tool(t) for t in tools]\n        self.tools: dict[str, Tool] = {tool.name: tool for tool in tool_list}\n\n        inputs = \", \".join([f\"`{k}`\" for k in self.signature.get_input_fields().keys()])\n        outputs = \", \".join([f\"`{k}`\" for k in self.signature.get_output_fields().keys()])\n\n        base_instructions = getattr(self.signature, \"__doc__\", \"\")\n        instr = [f\"{base_instructions}\\n\"] if base_instructions else []\n\n        instr.extend(\n            [\n                f\"You are an Agent. You will be given {inputs} as input.\",\n                f\"Your goal is to use the supplied tools to accomplish the task and produce {outputs}.\\n\",\n                \"Think step-by-step about what to do next, then call the appropriate tool.\",\n                \"Always explain your reasoning before calling a tool.\",\n                \"When you have enough information, call the 'finish' tool to complete the task.\",\n            ]\n        )\n\n        def finish_tool(**_kwargs: Any) -&gt; str:  # pyright: ignore[reportUnusedParameter]\n            \"\"\"Finish tool that accepts and ignores any arguments.\"\"\"\n            return \"Task completed\"\n\n        self.tools[\"finish\"] = Tool(\n            func=finish_tool,\n            name=\"finish\",\n            desc=f\"Call this when you have all information needed to produce {outputs}\",\n            args={},\n        )\n\n        if self.enable_ask_to_user:\n\n            def ask_to_user_impl(question: str) -&gt; str:  # noqa: ARG001\n                \"\"\"Ask the user for clarification.\"\"\"\n                return \"\"\n\n            self.tools[\"ask_to_user\"] = Tool(\n                func=ask_to_user_impl,\n                name=\"ask_to_user\",\n                description=\"Ask the user for clarification when needed. Use this when you need more information or when the request is ambiguous.\",\n                interruptible=True,\n            )\n\n        react_input_fields: dict[str, type] = {}\n        for name, field_info in self.signature.get_input_fields().items():\n            react_input_fields[name] = field_info.annotation or str\n\n        react_input_fields[\"trajectory\"] = str\n\n        react_output_fields: dict[str, type] = {\n            \"reasoning\": str,\n        }\n\n        self.react_signature = make_signature(\n            react_input_fields,\n            react_output_fields,\n            \"\\n\".join(instr),\n        )\n\n        extract_input_fields: dict[str, type] = {}\n        extract_output_fields: dict[str, type] = {}\n\n        for name, field_info in self.signature.get_input_fields().items():\n            extract_input_fields[name] = field_info.annotation or str\n\n        for name, field_info in self.signature.get_output_fields().items():\n            extract_output_fields[name] = field_info.annotation or str\n\n        extract_input_fields[\"trajectory\"] = str\n\n        self.extract_signature = make_signature(\n            extract_input_fields,\n            extract_output_fields,\n            base_instructions or \"Extract the final answer from the trajectory\",\n        )\n\n        self.react_module = Predict(self.react_signature, tools=list(self.tools.values()))\n        self.extract_module = ChainOfThought(self.extract_signature)\n\n    def _format_trajectory(self, trajectory: dict[str, Any]) -&gt; str:\n        \"\"\"Format trajectory as a string for the LLM.\n\n        Args:\n            trajectory: Dictionary with reasoning_N, tool_name_N, tool_args_N, observation_N keys\n\n        Returns:\n            Formatted string representation\n        \"\"\"\n        if not trajectory:\n            return \"No actions taken yet.\"\n\n        lines = []\n        iteration = 0\n        while f\"observation_{iteration}\" in trajectory:\n            lines.append(f\"\\n--- Step {iteration + 1} ---\")\n            if f\"reasoning_{iteration}\" in trajectory:\n                lines.append(f\"Reasoning: {trajectory[f'reasoning_{iteration}']}\")\n            if f\"tool_name_{iteration}\" in trajectory:\n                lines.append(f\"Tool: {trajectory[f'tool_name_{iteration}']}\")\n            if f\"tool_args_{iteration}\" in trajectory:\n                lines.append(f\"Args: {json.dumps(trajectory[f'tool_args_{iteration}'])}\")\n            lines.append(f\"Observation: {trajectory[f'observation_{iteration}']}\")\n            iteration += 1\n\n        return \"\\n\".join(lines)\n\n    async def _execute_iteration(\n        self,\n        idx: int,\n        trajectory: dict[str, Any],\n        input_args: dict[str, Any],\n        *,\n        stream: bool = False,\n        pending_tool_call: dict[str, Any] | None = None,\n    ) -&gt; bool:\n        \"\"\"Execute a single ReAct iteration.\n\n        Args:\n            idx: Current iteration index\n            trajectory: Current trajectory state\n            input_args: Original input arguments\n            pending_tool_call: Optional pending tool call to execute (for resumption)\n                             Format: {\"name\": str, \"args\": dict, \"id\": str}\n\n        Returns:\n            should_stop: Whether to stop the ReAct loop\n\n        Raises:\n            HumanInTheLoopRequired: When human input is needed\n        \"\"\"\n        if pending_tool_call:\n            tool_name = pending_tool_call[\"name\"]\n            tool_args = pending_tool_call[\"args\"]\n            tool_call_id = pending_tool_call.get(\"id\", \"\")\n\n            trajectory[f\"tool_name_{idx}\"] = tool_name\n            trajectory[f\"tool_args_{idx}\"] = tool_args\n\n            try:\n                tool = self.tools[tool_name]\n                if inspect.iscoroutinefunction(tool.func):\n                    observation = await tool.func(**tool_args)\n                elif tool.interruptible:\n                    observation = tool.func(**tool_args)\n                else:\n                    import asyncio\n\n                    loop = asyncio.get_event_loop()\n                    observation = await loop.run_in_executor(None, lambda: tool.func(**tool_args))\n            except HumanInTheLoopRequired as e:\n                e.context = {\n                    \"trajectory\": trajectory.copy(),\n                    \"iteration\": idx,\n                    \"input_args\": input_args.copy(),\n                }\n                if e.tool_call and tool_call_id:\n                    e.tool_call.call_id = tool_call_id\n                raise\n            except Exception as e:\n                observation = f\"Error executing {tool_name}: {str(e)}\"\n                logger.warning(f\"Tool execution failed: {e}\")\n\n            trajectory[f\"observation_{idx}\"] = str(observation)\n            should_stop = tool_name == \"finish\"\n            return should_stop\n\n        formatted_trajectory = self._format_trajectory(trajectory)\n        pred = await self.react_module.aexecute(\n            stream=stream,\n            **input_args,\n            trajectory=formatted_trajectory,\n            auto_execute_tools=False,\n        )\n\n        reasoning = pred.get(\"reasoning\", \"\")\n        trajectory[f\"reasoning_{idx}\"] = reasoning\n\n        if \"tool_calls\" not in pred or not pred.tool_calls:\n            logger.debug(\n                f\"No tool calls in prediction. Keys: {list(pred.keys())}, pred: {dict(pred)}\"\n            )\n            raise ValueError(\"LLM did not call any tools\")\n\n        tool_call = pred.tool_calls[0]\n        tool_name = tool_call.get(\"name\", \"\")\n        tool_call_id = tool_call.get(\"id\", \"\")\n\n        tool_args_str = tool_call.get(\"arguments\", \"{}\")\n        try:\n            tool_args = json.loads(tool_args_str) if tool_args_str else {}\n        except json.JSONDecodeError:\n            logger.warning(f\"Failed to parse tool arguments: {tool_args_str}\")\n            tool_args = {}\n\n        logger.debug(f\"Tool call - name: {tool_name}, args: {tool_args}, id: {tool_call_id}\")\n\n        trajectory[f\"tool_name_{idx}\"] = tool_name\n        trajectory[f\"tool_args_{idx}\"] = tool_args\n\n        try:\n            tool = self.tools[tool_name]\n            observation = await tool.acall(**tool_args)\n        except HumanInTheLoopRequired as e:\n            e.context = {\n                \"trajectory\": trajectory.copy(),\n                \"iteration\": idx,\n                \"input_args\": input_args.copy(),\n            }\n            if e.tool_call and tool_call_id:\n                e.tool_call.call_id = tool_call_id\n            raise\n        except Exception as e:\n            observation = f\"Error executing {tool_name}: {str(e)}\"\n            logger.warning(f\"Tool execution failed: {e}\")\n\n        trajectory[f\"observation_{idx}\"] = str(observation)\n\n        should_stop = tool_name == \"finish\"\n        return should_stop\n\n    async def aexecute(self, *, stream: bool = False, **input_args: Any) -&gt; Prediction:\n        \"\"\"Execute ReAct loop.\n\n        Args:\n            stream: Passed to sub-modules\n            **input_args: Input values matching signature's input fields\n\n        Returns:\n            Prediction with trajectory and output fields\n\n        Raises:\n            HumanInTheLoopRequired: When human input is needed\n        \"\"\"\n        max_iters = input_args.pop(\"max_iters\", self.max_iters)\n        trajectory: dict[str, Any] = {}\n\n        for idx in range(max_iters):\n            try:\n                should_stop = await self._execute_iteration(\n                    idx,\n                    trajectory,\n                    input_args,\n                    stream=stream,\n                )\n                if should_stop:\n                    break\n\n            except ValueError as e:\n                logger.warning(f\"Agent failed to select valid tool: {e}\")\n                trajectory[f\"observation_{idx}\"] = f\"Error: {e}\"\n                break\n\n        formatted_trajectory = self._format_trajectory(trajectory)\n        extract = await self.extract_module.aexecute(\n            stream=stream,\n            **input_args,\n            trajectory=formatted_trajectory,\n        )\n\n        result_dict = {\"trajectory\": trajectory}\n        for field_name in self.signature.get_output_fields():\n            if hasattr(extract, field_name):\n                result_dict[field_name] = getattr(extract, field_name)\n\n        return Prediction(**result_dict)\n\n    async def aforward(self, **input_args: Any) -&gt; Prediction:\n        return await self.aexecute(stream=False, **input_args)\n\n    async def aresume(\n        self,\n        user_response: str,\n        saved_state: Any,\n    ) -&gt; Prediction:\n        \"\"\"Async resume execution after user input.\n\n        Args:\n            user_response: The user's response. Can be:\n                - \"yes\"/\"y\" to confirm tool execution with original args\n                - \"no\"/\"n\" to reject and continue\n                - JSON dict string to execute tool with modified args\n                - Any other text is treated as user feedback for LLM to re-reason\n\n        Returns:\n            Final prediction after resuming\n\n        Raises:\n            HumanInTheLoopRequired: If another human input is needed\n        \"\"\"\n        trajectory = saved_state.context.get(\"trajectory\", {}).copy()\n        start_idx = saved_state.context.get(\"iteration\", 0)\n        input_args = saved_state.context.get(\"input_args\", {}).copy()\n\n        user_response_lower = user_response.lower().strip()\n        pending_tool_call: dict[str, Any] | None = None\n\n        if user_response_lower in (\"yes\", \"y\"):\n            if saved_state.interrupt_id:\n                set_interrupt_approval(saved_state.interrupt_id, approved=True, status=\"approved\")\n            if saved_state.tool_call:\n                pending_tool_call = {\n                    \"name\": saved_state.tool_call.name,\n                    \"args\": saved_state.tool_call.args.copy(),\n                    \"id\": saved_state.tool_call.call_id or \"\",\n                }\n            else:\n                pending_tool_call = None\n        elif user_response_lower in (\"no\", \"n\"):\n            if saved_state.interrupt_id:\n                set_interrupt_approval(saved_state.interrupt_id, approved=False, status=\"rejected\")\n            trajectory[f\"observation_{start_idx}\"] = \"User rejected the operation\"\n            start_idx += 1\n        else:\n            try:\n                modified_args = json.loads(user_response)\n                if isinstance(modified_args, dict):\n                    if saved_state.interrupt_id:\n                        set_interrupt_approval(\n                            saved_state.interrupt_id,\n                            approved=True,\n                            data=modified_args,\n                            status=\"edited\",\n                        )\n                    if saved_state.tool_call:\n                        pending_tool_call = {\n                            \"name\": saved_state.tool_call.name,\n                            \"args\": modified_args,\n                            \"id\": saved_state.tool_call.call_id or \"\",\n                        }\n                    else:\n                        pending_tool_call = None\n                else:\n                    if saved_state.interrupt_id:\n                        set_interrupt_approval(\n                            saved_state.interrupt_id, approved=False, status=\"feedback\"\n                        )\n                    trajectory[f\"observation_{start_idx}\"] = f\"User feedback: {user_response}\"\n                    start_idx += 1\n            except json.JSONDecodeError:\n                if saved_state.interrupt_id:\n                    set_interrupt_approval(\n                        saved_state.interrupt_id, approved=False, status=\"feedback\"\n                    )\n                trajectory[f\"observation_{start_idx}\"] = f\"User feedback: {user_response}\"\n                start_idx += 1\n\n        for idx in range(start_idx, self.max_iters):\n            try:\n                should_stop = await self._execute_iteration(\n                    idx,\n                    trajectory,\n                    input_args,\n                    pending_tool_call=pending_tool_call,\n                )\n                pending_tool_call = None\n\n                if should_stop:\n                    break\n\n            except ValueError as e:\n                logger.warning(f\"Agent failed: {e}\")\n                trajectory[f\"observation_{idx}\"] = f\"Error: {e}\"\n                break\n\n        formatted_trajectory = self._format_trajectory(trajectory)\n        extract = await self.extract_module.aforward(\n            **input_args,\n            trajectory=formatted_trajectory,\n        )\n\n        result_dict = {\"trajectory\": trajectory}\n        for field_name in self.signature.get_output_fields():\n            if hasattr(extract, field_name):\n                result_dict[field_name] = getattr(extract, field_name)\n\n        return Prediction(**result_dict)\n</code></pre>"},{"location":"api/module/#udspy.module.ReAct-functions","title":"Functions","text":""},{"location":"api/module/#udspy.module.ReAct.__init__","title":"<code>__init__(signature, tools, *, max_iters=10, enable_ask_to_user=True)</code>","text":"<p>Initialize ReAct module.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature] | str</code> <p>Signature defining inputs and outputs, or signature string</p> required <code>tools</code> <code>list[Callable | Tool]</code> <p>List of tool functions (decorated with @tool) or Tool objects</p> required <code>max_iters</code> <code>int</code> <p>Maximum number of reasoning iterations (default: 10)</p> <code>10</code> <code>enable_ask_to_user</code> <code>bool</code> <p>Whether to enable ask_to_user tool (default: True)</p> <code>True</code> Source code in <code>src/udspy/module/react.py</code> <pre><code>def __init__(\n    self,\n    signature: type[Signature] | str,\n    tools: list[Callable | Tool],\n    *,\n    max_iters: int = 10,\n    enable_ask_to_user: bool = True,\n):\n    \"\"\"Initialize ReAct module.\n\n    Args:\n        signature: Signature defining inputs and outputs, or signature string\n        tools: List of tool functions (decorated with @tool) or Tool objects\n        max_iters: Maximum number of reasoning iterations (default: 10)\n        enable_ask_to_user: Whether to enable ask_to_user tool (default: True)\n    \"\"\"\n    from udspy.tool import Tool\n\n    super().__init__()\n\n    # Convert string signature to Signature class\n    if isinstance(signature, str):\n        signature = Signature.from_string(signature)\n\n    self.signature = signature\n\n    self.max_iters = max_iters\n    self.enable_ask_to_user = enable_ask_to_user\n\n    tool_list = [t if isinstance(t, Tool) else Tool(t) for t in tools]\n    self.tools: dict[str, Tool] = {tool.name: tool for tool in tool_list}\n\n    inputs = \", \".join([f\"`{k}`\" for k in self.signature.get_input_fields().keys()])\n    outputs = \", \".join([f\"`{k}`\" for k in self.signature.get_output_fields().keys()])\n\n    base_instructions = getattr(self.signature, \"__doc__\", \"\")\n    instr = [f\"{base_instructions}\\n\"] if base_instructions else []\n\n    instr.extend(\n        [\n            f\"You are an Agent. You will be given {inputs} as input.\",\n            f\"Your goal is to use the supplied tools to accomplish the task and produce {outputs}.\\n\",\n            \"Think step-by-step about what to do next, then call the appropriate tool.\",\n            \"Always explain your reasoning before calling a tool.\",\n            \"When you have enough information, call the 'finish' tool to complete the task.\",\n        ]\n    )\n\n    def finish_tool(**_kwargs: Any) -&gt; str:  # pyright: ignore[reportUnusedParameter]\n        \"\"\"Finish tool that accepts and ignores any arguments.\"\"\"\n        return \"Task completed\"\n\n    self.tools[\"finish\"] = Tool(\n        func=finish_tool,\n        name=\"finish\",\n        desc=f\"Call this when you have all information needed to produce {outputs}\",\n        args={},\n    )\n\n    if self.enable_ask_to_user:\n\n        def ask_to_user_impl(question: str) -&gt; str:  # noqa: ARG001\n            \"\"\"Ask the user for clarification.\"\"\"\n            return \"\"\n\n        self.tools[\"ask_to_user\"] = Tool(\n            func=ask_to_user_impl,\n            name=\"ask_to_user\",\n            description=\"Ask the user for clarification when needed. Use this when you need more information or when the request is ambiguous.\",\n            interruptible=True,\n        )\n\n    react_input_fields: dict[str, type] = {}\n    for name, field_info in self.signature.get_input_fields().items():\n        react_input_fields[name] = field_info.annotation or str\n\n    react_input_fields[\"trajectory\"] = str\n\n    react_output_fields: dict[str, type] = {\n        \"reasoning\": str,\n    }\n\n    self.react_signature = make_signature(\n        react_input_fields,\n        react_output_fields,\n        \"\\n\".join(instr),\n    )\n\n    extract_input_fields: dict[str, type] = {}\n    extract_output_fields: dict[str, type] = {}\n\n    for name, field_info in self.signature.get_input_fields().items():\n        extract_input_fields[name] = field_info.annotation or str\n\n    for name, field_info in self.signature.get_output_fields().items():\n        extract_output_fields[name] = field_info.annotation or str\n\n    extract_input_fields[\"trajectory\"] = str\n\n    self.extract_signature = make_signature(\n        extract_input_fields,\n        extract_output_fields,\n        base_instructions or \"Extract the final answer from the trajectory\",\n    )\n\n    self.react_module = Predict(self.react_signature, tools=list(self.tools.values()))\n    self.extract_module = ChainOfThought(self.extract_signature)\n</code></pre>"},{"location":"api/module/#udspy.module.ReAct.aexecute","title":"<code>aexecute(*, stream=False, **input_args)</code>  <code>async</code>","text":"<p>Execute ReAct loop.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>bool</code> <p>Passed to sub-modules</p> <code>False</code> <code>**input_args</code> <code>Any</code> <p>Input values matching signature's input fields</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Prediction with trajectory and output fields</p> <p>Raises:</p> Type Description <code>HumanInTheLoopRequired</code> <p>When human input is needed</p> Source code in <code>src/udspy/module/react.py</code> <pre><code>async def aexecute(self, *, stream: bool = False, **input_args: Any) -&gt; Prediction:\n    \"\"\"Execute ReAct loop.\n\n    Args:\n        stream: Passed to sub-modules\n        **input_args: Input values matching signature's input fields\n\n    Returns:\n        Prediction with trajectory and output fields\n\n    Raises:\n        HumanInTheLoopRequired: When human input is needed\n    \"\"\"\n    max_iters = input_args.pop(\"max_iters\", self.max_iters)\n    trajectory: dict[str, Any] = {}\n\n    for idx in range(max_iters):\n        try:\n            should_stop = await self._execute_iteration(\n                idx,\n                trajectory,\n                input_args,\n                stream=stream,\n            )\n            if should_stop:\n                break\n\n        except ValueError as e:\n            logger.warning(f\"Agent failed to select valid tool: {e}\")\n            trajectory[f\"observation_{idx}\"] = f\"Error: {e}\"\n            break\n\n    formatted_trajectory = self._format_trajectory(trajectory)\n    extract = await self.extract_module.aexecute(\n        stream=stream,\n        **input_args,\n        trajectory=formatted_trajectory,\n    )\n\n    result_dict = {\"trajectory\": trajectory}\n    for field_name in self.signature.get_output_fields():\n        if hasattr(extract, field_name):\n            result_dict[field_name] = getattr(extract, field_name)\n\n    return Prediction(**result_dict)\n</code></pre>"},{"location":"api/module/#udspy.module.ReAct.aresume","title":"<code>aresume(user_response, saved_state)</code>  <code>async</code>","text":"<p>Async resume execution after user input.</p> <p>Parameters:</p> Name Type Description Default <code>user_response</code> <code>str</code> <p>The user's response. Can be: - \"yes\"/\"y\" to confirm tool execution with original args - \"no\"/\"n\" to reject and continue - JSON dict string to execute tool with modified args - Any other text is treated as user feedback for LLM to re-reason</p> required <p>Returns:</p> Type Description <code>Prediction</code> <p>Final prediction after resuming</p> <p>Raises:</p> Type Description <code>HumanInTheLoopRequired</code> <p>If another human input is needed</p> Source code in <code>src/udspy/module/react.py</code> <pre><code>async def aresume(\n    self,\n    user_response: str,\n    saved_state: Any,\n) -&gt; Prediction:\n    \"\"\"Async resume execution after user input.\n\n    Args:\n        user_response: The user's response. Can be:\n            - \"yes\"/\"y\" to confirm tool execution with original args\n            - \"no\"/\"n\" to reject and continue\n            - JSON dict string to execute tool with modified args\n            - Any other text is treated as user feedback for LLM to re-reason\n\n    Returns:\n        Final prediction after resuming\n\n    Raises:\n        HumanInTheLoopRequired: If another human input is needed\n    \"\"\"\n    trajectory = saved_state.context.get(\"trajectory\", {}).copy()\n    start_idx = saved_state.context.get(\"iteration\", 0)\n    input_args = saved_state.context.get(\"input_args\", {}).copy()\n\n    user_response_lower = user_response.lower().strip()\n    pending_tool_call: dict[str, Any] | None = None\n\n    if user_response_lower in (\"yes\", \"y\"):\n        if saved_state.interrupt_id:\n            set_interrupt_approval(saved_state.interrupt_id, approved=True, status=\"approved\")\n        if saved_state.tool_call:\n            pending_tool_call = {\n                \"name\": saved_state.tool_call.name,\n                \"args\": saved_state.tool_call.args.copy(),\n                \"id\": saved_state.tool_call.call_id or \"\",\n            }\n        else:\n            pending_tool_call = None\n    elif user_response_lower in (\"no\", \"n\"):\n        if saved_state.interrupt_id:\n            set_interrupt_approval(saved_state.interrupt_id, approved=False, status=\"rejected\")\n        trajectory[f\"observation_{start_idx}\"] = \"User rejected the operation\"\n        start_idx += 1\n    else:\n        try:\n            modified_args = json.loads(user_response)\n            if isinstance(modified_args, dict):\n                if saved_state.interrupt_id:\n                    set_interrupt_approval(\n                        saved_state.interrupt_id,\n                        approved=True,\n                        data=modified_args,\n                        status=\"edited\",\n                    )\n                if saved_state.tool_call:\n                    pending_tool_call = {\n                        \"name\": saved_state.tool_call.name,\n                        \"args\": modified_args,\n                        \"id\": saved_state.tool_call.call_id or \"\",\n                    }\n                else:\n                    pending_tool_call = None\n            else:\n                if saved_state.interrupt_id:\n                    set_interrupt_approval(\n                        saved_state.interrupt_id, approved=False, status=\"feedback\"\n                    )\n                trajectory[f\"observation_{start_idx}\"] = f\"User feedback: {user_response}\"\n                start_idx += 1\n        except json.JSONDecodeError:\n            if saved_state.interrupt_id:\n                set_interrupt_approval(\n                    saved_state.interrupt_id, approved=False, status=\"feedback\"\n                )\n            trajectory[f\"observation_{start_idx}\"] = f\"User feedback: {user_response}\"\n            start_idx += 1\n\n    for idx in range(start_idx, self.max_iters):\n        try:\n            should_stop = await self._execute_iteration(\n                idx,\n                trajectory,\n                input_args,\n                pending_tool_call=pending_tool_call,\n            )\n            pending_tool_call = None\n\n            if should_stop:\n                break\n\n        except ValueError as e:\n            logger.warning(f\"Agent failed: {e}\")\n            trajectory[f\"observation_{idx}\"] = f\"Error: {e}\"\n            break\n\n    formatted_trajectory = self._format_trajectory(trajectory)\n    extract = await self.extract_module.aforward(\n        **input_args,\n        trajectory=formatted_trajectory,\n    )\n\n    result_dict = {\"trajectory\": trajectory}\n    for field_name in self.signature.get_output_fields():\n        if hasattr(extract, field_name):\n            result_dict[field_name] = getattr(extract, field_name)\n\n    return Prediction(**result_dict)\n</code></pre>"},{"location":"api/react/","title":"ReAct API Reference","text":"<p>API documentation for the ReAct (Reasoning and Acting) module.</p>"},{"location":"api/react/#module-udspymodulereact","title":"Module: <code>udspy.module.react</code>","text":""},{"location":"api/react/#react","title":"<code>ReAct</code>","text":"<pre><code>class ReAct(Module):\n    \"\"\"ReAct (Reasoning and Acting) module for tool-using agents.\"\"\"\n</code></pre> <p>Agent module that iteratively reasons about the current situation and decides whether to call a tool or finish the task.</p>"},{"location":"api/react/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    signature: type[Signature] | str,\n    tools: list[Callable | Tool],\n    *,\n    max_iters: int = 10,\n    enable_ask_to_user: bool = True,\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>signature</code> (<code>type[Signature] | str</code>): Task signature defining inputs and outputs</li> <li>Can be a <code>Signature</code> class or a string like <code>\"input1, input2 -&gt; output1, output2\"</code></li> <li><code>tools</code> (<code>list[Callable | Tool]</code>): List of tool functions or <code>Tool</code> objects</li> <li>Tools can be decorated functions (<code>@tool</code>) or <code>Tool</code> instances</li> <li><code>max_iters</code> (<code>int</code>, default: <code>10</code>): Maximum number of reasoning iterations</li> <li>Agent will stop after this many steps even if not finished</li> <li><code>enable_ask_to_user</code> (<code>bool</code>, default: <code>True</code>): Whether to enable the <code>ask_to_user</code> tool</li> <li>If <code>False</code>, the agent cannot request user clarification</li> </ul> <p>Example:</p> <pre><code>from udspy import ReAct, Signature, InputField, OutputField, tool\nfrom pydantic import Field\n\n@tool(name=\"search\", description=\"Search for information\")\ndef search(query: str = Field(...)) -&gt; str:\n    return f\"Results for: {query}\"\n\nclass QA(Signature):\n    \"\"\"Answer questions using available tools.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\nagent = ReAct(QA, tools=[search], max_iters=10)\n</code></pre>"},{"location":"api/react/#methods","title":"Methods","text":""},{"location":"api/react/#forwardinput_args-prediction","title":"<code>forward(**input_args) -&gt; Prediction</code>","text":"<p>Synchronous forward pass through the ReAct loop.</p> <p>Parameters:</p> <ul> <li><code>**input_args</code>: Input values matching the signature's input fields</li> <li>Keys must match input field names</li> <li>Can include <code>max_iters</code> to override default</li> </ul> <p>Returns:</p> <ul> <li><code>Prediction</code>: Contains output fields and trajectory</li> <li>Trajectory tracks all reasoning steps</li> <li>Output fields match the signature</li> </ul> <p>Raises:</p> <ul> <li><code>HumanInTheLoopRequired</code>: When user input is needed</li> <li>Raised when <code>ask_to_user</code> is called</li> <li>Raised when tool requires confirmation</li> <li>Contains saved state for resumption</li> </ul> <p>Example:</p> <pre><code>result = agent(question=\"What is Python?\")\nprint(result.answer)\nprint(result.trajectory)  # All reasoning steps\n</code></pre>"},{"location":"api/react/#aforwardinput_args-prediction","title":"<code>aforward(**input_args) -&gt; Prediction</code>","text":"<p>Async forward pass through the ReAct loop.</p> <p>Parameters:</p> <ul> <li>Same as <code>forward()</code></li> </ul> <p>Returns:</p> <ul> <li><code>Prediction</code>: Same as <code>forward()</code></li> </ul> <p>Raises:</p> <ul> <li>Same as <code>forward()</code></li> </ul> <p>Example:</p> <pre><code>import asyncio\n\nasync def main():\n    result = await agent.aforward(question=\"What is Python?\")\n    print(result.answer)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/react/#resumeuser_response-saved_state-prediction","title":"<code>resume(user_response, saved_state) -&gt; Prediction</code>","text":"<p>Resume execution after user provides input (synchronous).</p> <p>Parameters:</p> <ul> <li><code>user_response</code> (<code>str</code>): The user's response to the question</li> <li><code>\"yes\"</code> or <code>\"y\"</code>: Approve the pending tool call</li> <li><code>\"no\"</code> or <code>\"n\"</code>: Reject and let agent decide next action</li> <li>Free text: Treated as feedback for the agent</li> <li>JSON with <code>\"edit\"</code> key: Modify tool name/args (e.g., <code>{\"edit\": {\"name\": \"new_tool\", \"args\": {...}}}</code>)</li> <li><code>saved_state</code> (<code>HumanInTheLoopRequired</code>): The exception that was raised</li> <li>Contains <code>context</code> dict with: <code>trajectory</code>, <code>iteration</code>, <code>input_args</code></li> <li>Contains <code>tool_call</code> with pending tool information</li> <li>Contains <code>interrupt_id</code> for tracking</li> </ul> <p>Returns:</p> <ul> <li><code>Prediction</code>: Final result after resuming execution</li> </ul> <p>Example:</p> <pre><code>from udspy import HumanInTheLoopRequired\n\ntry:\n    result = agent(question=\"Delete my files\")\nexcept HumanInTheLoopRequired as e:\n    print(f\"Agent asks: {e.question}\")\n    if e.tool_call:\n        print(f\"Tool: {e.tool_call.name}\")\n        print(f\"Args: {e.tool_call.args}\")\n    response = input(\"Your response (yes/no/feedback): \")\n    result = agent.resume(response, e)\n</code></pre>"},{"location":"api/react/#aresumeuser_response-saved_state-prediction","title":"<code>aresume(user_response, saved_state) -&gt; Prediction</code>","text":"<p>Resume execution after user provides input (async).</p> <p>Parameters:</p> <ul> <li>Same as <code>resume()</code></li> </ul> <p>Returns:</p> <ul> <li>Same as <code>resume()</code></li> </ul> <p>Example:</p> <pre><code>try:\n    result = await agent.aforward(question=\"Delete my files\")\nexcept HumanInTheLoopRequired as e:\n    print(f\"Agent asks: {e.question}\")\n    response = get_user_input(e.question)\n    result = await agent.aresume(response, e)\n</code></pre>"},{"location":"api/react/#properties","title":"Properties","text":""},{"location":"api/react/#signature","title":"<code>signature</code>","text":"<pre><code>signature: type[Signature]\n</code></pre> <p>The task signature used by this agent.</p>"},{"location":"api/react/#tools","title":"<code>tools</code>","text":"<pre><code>tools: dict[str, Tool]\n</code></pre> <p>Dictionary mapping tool names to Tool objects. Includes: - User-provided tools - Built-in <code>finish</code> tool - Built-in <code>ask_to_user</code> tool (if enabled)</p>"},{"location":"api/react/#react_signature","title":"<code>react_signature</code>","text":"<pre><code>react_signature: type[Signature]\n</code></pre> <p>Internal signature for the reasoning loop. With native tool calling: - Outputs <code>reasoning</code>: The agent's reasoning about what to do next - Tools are called natively via OpenAI's tool calling API - The LLM both produces reasoning text and selects tools simultaneously</p>"},{"location":"api/react/#extract_signature","title":"<code>extract_signature</code>","text":"<pre><code>extract_signature: type[Signature]\n</code></pre> <p>Internal signature for extracting the final answer from the trajectory.</p>"},{"location":"api/react/#humaninthelooprequired","title":"<code>HumanInTheLoopRequired</code>","text":"<pre><code>class HumanInTheLoopRequired(Exception):\n    \"\"\"Raised when human input is needed to proceed.\"\"\"\n</code></pre> <p>Note: This exception has been moved to the <code>interrupt</code> module. See the Interrupt API for full documentation.</p> <p>Exception that pauses ReAct execution and saves state for resumption. This exception can be raised by: - The <code>ask_to_user</code> tool when the agent needs clarification - Tools with <code>interruptible=True</code> before execution - Custom tools that need human input</p>"},{"location":"api/react/#constructor_1","title":"Constructor","text":"<pre><code>from udspy.interrupt import HumanInTheLoopRequired, ToolCall\n\ndef __init__(\n    self,\n    question: str,\n    *,\n    interrupt_id: str | None = None,\n    tool_call: ToolCall | None = None,\n    context: dict[str, Any] | None = None,\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>question</code> (<code>str</code>): Question to ask the user</li> <li><code>interrupt_id</code> (<code>str | None</code>): Unique interrupt ID (auto-generated if not provided)</li> <li><code>tool_call</code> (<code>ToolCall | None</code>): Optional tool call information</li> <li>Has attributes: <code>.name</code> (tool name), <code>.args</code> (arguments dict), <code>.call_id</code> (optional ID)</li> <li><code>context</code> (<code>dict[str, Any] | None</code>): Module-specific state dictionary</li> <li>ReAct stores: <code>trajectory</code>, <code>iteration</code>, <code>input_args</code> here</li> </ul>"},{"location":"api/react/#attributes","title":"Attributes","text":""},{"location":"api/react/#question","title":"<code>question</code>","text":"<pre><code>question: str\n</code></pre> <p>The question being asked to the user.</p> <p>Example:</p> <pre><code>try:\n    result = agent(question=\"Delete files\")\nexcept HumanInTheLoopRequired as e:\n    print(e.question)  # \"Confirm execution of delete_file...?\"\n</code></pre>"},{"location":"api/react/#interrupt_id","title":"<code>interrupt_id</code>","text":"<pre><code>interrupt_id: str\n</code></pre> <p>Unique identifier for this interrupt. Used with <code>get_interrupt_status()</code> to check approval status.</p> <p>Example:</p> <pre><code>from udspy import get_interrupt_status\n\ntry:\n    result = agent(question=\"Delete files\")\nexcept HumanInTheLoopRequired as e:\n    print(e.interrupt_id)  # \"abc-123-def-456\"\n    status = get_interrupt_status(e.interrupt_id)\n    print(status)  # \"pending\"\n</code></pre>"},{"location":"api/react/#tool_call","title":"<code>tool_call</code>","text":"<pre><code>tool_call: ToolCall | None\n</code></pre> <p>Information about the tool call that triggered this interrupt (if applicable).</p> <p>Attributes: - <code>name</code> (<code>str</code>): Tool name - <code>args</code> (<code>dict[str, Any]</code>): Tool arguments - <code>call_id</code> (<code>str | None</code>): Optional call ID from OpenAI</p> <p>Example:</p> <pre><code>try:\n    result = agent(question=\"Delete my files\")\nexcept HumanInTheLoopRequired as e:\n    if e.tool_call:\n        print(f\"Tool: {e.tool_call.name}\")  # \"delete_file\"\n        print(f\"Args: {e.tool_call.args}\")  # {\"path\": \"/tmp/test.txt\"}\n        print(f\"Call ID: {e.tool_call.call_id}\")  # \"call_abc123\"\n</code></pre>"},{"location":"api/react/#context","title":"<code>context</code>","text":"<pre><code>context: dict[str, Any]\n</code></pre> <p>Module-specific state dictionary. For ReAct agents, contains: - <code>trajectory</code> (<code>dict[str, Any]</code>): Current execution trajectory with reasoning, tool calls, and observations - <code>iteration</code> (<code>int</code>): Current iteration number (0-indexed) - <code>input_args</code> (<code>dict[str, Any]</code>): Original input arguments</p> <p>Example:</p> <pre><code>try:\n    result = agent(question=\"What is 2+2?\")\nexcept HumanInTheLoopRequired as e:\n    # Access ReAct-specific state\n    trajectory = e.context[\"trajectory\"]\n    print(trajectory)\n    # {\n    #   'reasoning_0': 'I should use the calculator',\n    #   'tool_name_0': 'calculator',\n    #   'tool_args_0': {'expression': '2+2'},\n    #   'observation_0': '4'\n    # }\n\n    iteration = e.context[\"iteration\"]\n    print(f\"Paused at step {iteration + 1}\")\n\n    input_args = e.context[\"input_args\"]\n    print(f\"Original question: {input_args['question']}\")\n</code></pre>"},{"location":"api/react/#built-in-tools","title":"Built-in Tools","text":"<p>Every ReAct agent automatically includes these tools:</p>"},{"location":"api/react/#finish","title":"<code>finish</code>","text":"<p>Tool that signals task completion.</p> <p>Name: <code>finish</code></p> <p>Description: \"Call this when you have all information needed to produce {outputs}\"</p> <p>Arguments: None</p> <p>Usage:</p> <p>The agent automatically selects this tool when it has enough information to answer. You don't call it directly.</p>"},{"location":"api/react/#ask_to_user","title":"<code>ask_to_user</code>","text":"<p>Tool for requesting user clarification (if enabled).</p> <p>Name: <code>ask_to_user</code></p> <p>Description: \"Ask the user for clarification. ONLY use this when: ...\"</p> <p>Arguments: - <code>question</code> (<code>str</code>): The question to ask the user</p> <p>Usage:</p> <p>The agent can call this whenever it needs clarification or more information from the user.</p> <p>Raises <code>HumanInTheLoopRequired</code> exception.</p> <p>Notes: - Can be disabled with <code>enable_ask_to_user=False</code></p>"},{"location":"api/react/#trajectory-format","title":"Trajectory Format","text":"<p>The trajectory is a dictionary with the following keys:</p> <pre><code>{\n    \"reasoning_0\": str,    # Agent's reasoning for step 0\n    \"tool_name_0\": str,    # Tool name selected for step 0\n    \"tool_args_0\": dict,   # Arguments for step 0\n    \"observation_0\": str,  # Tool result for step 0\n\n    \"reasoning_1\": str,    # Agent's reasoning for step 1\n    \"tool_name_1\": str,    # Tool name selected for step 1\n    \"tool_args_1\": dict,   # Arguments for step 1\n    \"observation_1\": str,  # Tool result for step 1\n\n    # ... continues for all iterations\n}\n</code></pre> <p>Example:</p> <pre><code>result = agent(question=\"Calculate 2+2\")\n\n# Access trajectory\nprint(result.trajectory)\n# {\n#     'reasoning_0': 'I need to calculate 2+2',\n#     'tool_name_0': 'calculator',\n#     'tool_args_0': {'expression': '2+2'},\n#     'observation_0': '4',\n#     'reasoning_1': 'I have the answer',\n#     'tool_name_1': 'finish',\n#     'tool_args_1': {},\n#     'observation_1': 'Task completed'\n# }\n\n# Iterate through steps\ni = 0\nwhile f\"observation_{i}\" in result.trajectory:\n    print(f\"Step {i}:\")\n    print(f\"  Reasoning: {result.trajectory.get(f'reasoning_{i}', '')}\")\n    print(f\"  Tool: {result.trajectory[f'tool_name_{i}']}\")\n    print(f\"  Args: {result.trajectory[f'tool_args_{i}']}\")\n    print(f\"  Result: {result.trajectory[f'observation_{i}']}\")\n    i += 1\n</code></pre>"},{"location":"api/react/#string-signature-format","title":"String Signature Format","text":"<p>For quick prototyping, you can use string signatures:</p> <p>Format: <code>\"input1, input2 -&gt; output1, output2\"</code></p> <p>Examples:</p> <pre><code># Single input, single output\nagent = ReAct(\"query -&gt; result\", tools=[search])\n\n# Multiple inputs\nagent = ReAct(\"context, question -&gt; answer\", tools=[search])\n\n# Multiple outputs\nagent = ReAct(\"topic -&gt; summary, sources\", tools=[search])\n</code></pre> <p>The string signature is parsed into: - Input fields: All fields before <code>-&gt;</code> (type: <code>str</code>) - Output fields: All fields after <code>-&gt;</code> (type: <code>str</code>)</p>"},{"location":"api/react/#tool-interruption","title":"Tool Interruption","text":"<p>Tools can require user confirmation before execution using the <code>interruptible</code> parameter:</p> <pre><code>from udspy import tool\nfrom pydantic import Field\n\n@tool(\n    name=\"delete_file\",\n    description=\"Delete a file\",\n    interruptible=True  # Require confirmation before execution\n)\ndef delete_file(path: str = Field(...)) -&gt; str:\n    os.remove(path)\n    return f\"Deleted {path}\"\n</code></pre> <p>When the agent tries to call this tool, it raises <code>HumanInTheLoopRequired</code> with a confirmation question on the first call. After the user approves, the tool executes normally.</p> <p>Confirmation Message Format:</p> <pre><code>\"Confirm execution of {tool_name} with args: {args}? (yes/no)\"\n</code></pre> <p>Response Options: - <code>\"yes\"</code> or <code>\"y\"</code>: Approve and execute the tool - <code>\"no\"</code> or <code>\"n\"</code>: Reject and let agent choose a different action - JSON with <code>\"edit\"</code>: Modify tool arguments before execution</p> <p>Example:</p> <pre><code>try:\n    result = agent(question=\"Delete all temporary files\")\nexcept HumanInTheLoopRequired as e:\n    print(e.question)  # \"Confirm execution of delete_file...\"\n    # User approves\n    result = agent.resume(\"yes\", e)\n</code></pre>"},{"location":"api/react/#error-handling","title":"Error Handling","text":""},{"location":"api/react/#tool-execution-errors","title":"Tool Execution Errors","text":"<p>If a tool raises an exception, the error is captured as an observation:</p> <pre><code>@tool(name=\"api_call\", description=\"Call API\")\ndef api_call(endpoint: str = Field(...)) -&gt; str:\n    if endpoint == \"invalid\":\n        raise ValueError(\"Invalid endpoint\")\n    return \"Success\"\n\n# Agent will see observation:\n# \"Error executing api_call: Invalid endpoint\"\n</code></pre> <p>The agent can then: 1. Try a different tool 2. Retry with different arguments 3. Ask the user for help (using the <code>ask_to_user</code> tool)</p>"},{"location":"api/react/#maximum-iterations","title":"Maximum Iterations","text":"<p>If the agent reaches <code>max_iters</code>, it stops and extracts an answer from the current trajectory:</p> <pre><code>agent = ReAct(signature, tools=tools, max_iters=5)\nresult = agent(question=\"Complex task\")\n# Will stop after 5 iterations even if not finished\n</code></pre>"},{"location":"api/react/#type-annotations","title":"Type Annotations","text":"<pre><code>from typing import Callable, Any\nfrom udspy import ReAct, Signature, Tool, Prediction, HumanInTheLoopRequired\n\n# Constructor types\nsignature: type[Signature] | str\ntools: list[Callable | Tool]\nmax_iters: int\nenable_ask_to_user: bool\n\n# Method types\ndef forward(**input_args: Any) -&gt; Prediction: ...\nasync def aforward(**input_args: Any) -&gt; Prediction: ...\ndef resume(\n    user_response: str,\n    saved_state: HumanInTheLoopRequired\n) -&gt; Prediction: ...\nasync def aresume(\n    user_response: str,\n    saved_state: HumanInTheLoopRequired\n) -&gt; Prediction: ...\n</code></pre>"},{"location":"api/react/#see-also","title":"See Also","text":"<ul> <li>ReAct Examples - Usage guide and examples</li> <li>Interrupt API - Interrupt system and <code>HumanInTheLoopRequired</code> documentation</li> <li>Tool API - Creating and configuring tools</li> <li>Module API - Base module documentation</li> <li>Signature API - Signature documentation</li> </ul>"},{"location":"api/settings/","title":"API Reference: Settings","text":""},{"location":"api/settings/#udspy.settings","title":"<code>udspy.settings</code>","text":"<p>Global settings and configuration.</p>"},{"location":"api/settings/#udspy.settings-classes","title":"Classes","text":""},{"location":"api/settings/#udspy.settings.Settings","title":"<code>Settings</code>","text":"<p>Global settings for udspy.</p> <p>Since udspy is async-first, we only need the async OpenAI client. Sync wrappers (forward(), call()) use asyncio.run() internally, which works fine with the async client.</p> Source code in <code>src/udspy/settings.py</code> <pre><code>class Settings:\n    \"\"\"Global settings for udspy.\n\n    Since udspy is async-first, we only need the async OpenAI client.\n    Sync wrappers (forward(), __call__()) use asyncio.run() internally,\n    which works fine with the async client.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self._aclient: AsyncOpenAI | None = None\n        self._default_model: str | None = None\n        self._default_kwargs: dict[str, Any] = {}\n\n        # Context-specific overrides (thread-safe)\n        self._context_aclient: ContextVar[AsyncOpenAI | None] = ContextVar(\n            \"context_aclient\", default=None\n        )\n        self._context_model: ContextVar[str | None] = ContextVar(\"context_model\", default=None)\n        self._context_kwargs: ContextVar[dict[str, Any] | None] = ContextVar(\n            \"context_kwargs\", default=None\n        )\n\n    def configure(\n        self,\n        api_key: str = \"\",\n        base_url: str | None = None,\n        model: str | None = None,\n        aclient: AsyncOpenAI | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Configure global OpenAI client and defaults.\n\n        Args:\n            api_key: OpenAI API key (creates default async client)\n            base_url: Base URL for OpenAI API\n            model: Default model to use for all predictions\n            aclient: Custom async OpenAI client\n            **kwargs: Default kwargs for all chat completions (temperature, etc.)\n\n        Example:\n            ```python\n            import udspy\n\n            # With API key\n            udspy.settings.configure(api_key=\"sk-...\", model=\"gpt-4o\")\n\n            # With custom client\n            from openai import AsyncOpenAI\n            client = AsyncOpenAI(api_key=\"sk-...\", timeout=30.0)\n            udspy.settings.configure(aclient=client, model=\"gpt-4o\")\n            ```\n        \"\"\"\n        if aclient:\n            self._aclient = aclient\n        else:\n            self._aclient = AsyncOpenAI(api_key=api_key, base_url=base_url)\n\n        if model:\n            self._default_model = model\n\n        self._default_kwargs.update(kwargs)\n\n    @property\n    def aclient(self) -&gt; AsyncOpenAI:\n        \"\"\"Get the async OpenAI client (context-aware).\n\n        This is used by all module operations, both async and sync.\n        Sync wrappers use asyncio.run() internally.\n        \"\"\"\n        # Check context first\n        context_aclient = self._context_aclient.get()\n        if context_aclient is not None:\n            return context_aclient\n\n        # Fall back to global client\n        if self._aclient is None:\n            raise RuntimeError(\n                \"OpenAI client not configured. Call udspy.settings.configure() first.\"\n            )\n        return self._aclient\n\n    @property\n    def default_model(self) -&gt; str:\n        \"\"\"Get the default model name (context-aware).\"\"\"\n        # Check context first\n        context_model = self._context_model.get()\n        if context_model is not None:\n            return context_model\n\n        # Fall back to global model\n        if self._default_model is None:\n            raise ValueError(\n                \"No model configured. Call settings.configure(model='...') or set in context.\"\n            )\n        return self._default_model\n\n    @property\n    def default_kwargs(self) -&gt; dict[str, Any]:\n        \"\"\"Get the default kwargs for chat completions (context-aware).\"\"\"\n        # Start with global defaults\n        result = self._default_kwargs.copy()\n\n        # Override with context-specific kwargs if present\n        context_kwargs = self._context_kwargs.get()\n        if context_kwargs is not None:\n            result.update(context_kwargs)\n\n        return result\n\n    @contextmanager\n    def context(\n        self,\n        api_key: str = \"\",\n        base_url: str | None = None,\n        model: str | None = None,\n        aclient: AsyncOpenAI | None = None,\n        **kwargs: Any,\n    ) -&gt; Iterator[None]:\n        \"\"\"Context manager for temporary settings overrides.\n\n        This is thread-safe and allows you to use different API keys, models,\n        or other settings within a specific context. Useful for multi-tenant\n        applications.\n\n        Args:\n            api_key: Temporary OpenAI API key (creates temporary client)\n            model: Temporary model to use\n            aclient: Temporary async OpenAI client\n            **kwargs: Temporary kwargs for chat completions\n\n        Example:\n            ```python\n            import udspy\n            from udspy import Predict, Signature, InputField, OutputField\n\n            # Global settings\n            udspy.settings.configure(api_key=\"global-key\", model=\"gpt-4o-mini\")\n\n            class QA(Signature):\n                question: str = InputField()\n                answer: str = OutputField()\n\n            predictor = Predict(QA)\n\n            # Temporary override for a specific context (e.g., different tenant)\n            with udspy.settings.context(api_key=\"tenant-key\", model=\"gpt-4\"):\n                result = predictor(question=\"...\")  # Uses \"tenant-key\" and \"gpt-4\"\n\n            # Back to global settings\n            result = predictor(question=\"...\")  # Uses \"global-key\" and \"gpt-4o-mini\"\n            ```\n        \"\"\"\n        # Save current context values\n        prev_aclient = self._context_aclient.get()\n        prev_model = self._context_model.get()\n        prev_kwargs = self._context_kwargs.get()\n\n        try:\n            # Set context-specific values\n            if aclient:\n                self._context_aclient.set(aclient)\n            else:\n                self._context_aclient.set(AsyncOpenAI(api_key=api_key, base_url=base_url))\n\n            if model:\n                self._context_model.set(model)\n\n            if kwargs:\n                # Merge with previous context kwargs if any\n                merged_kwargs = (prev_kwargs or {}).copy()\n                merged_kwargs.update(kwargs)\n                self._context_kwargs.set(merged_kwargs)\n\n            yield\n\n        finally:\n            # Restore previous context values\n            self._context_aclient.set(prev_aclient)\n            self._context_model.set(prev_model)\n            self._context_kwargs.set(prev_kwargs)\n</code></pre>"},{"location":"api/settings/#udspy.settings.Settings-attributes","title":"Attributes","text":""},{"location":"api/settings/#udspy.settings.Settings.aclient","title":"<code>aclient</code>  <code>property</code>","text":"<p>Get the async OpenAI client (context-aware).</p> <p>This is used by all module operations, both async and sync. Sync wrappers use asyncio.run() internally.</p>"},{"location":"api/settings/#udspy.settings.Settings.default_kwargs","title":"<code>default_kwargs</code>  <code>property</code>","text":"<p>Get the default kwargs for chat completions (context-aware).</p>"},{"location":"api/settings/#udspy.settings.Settings.default_model","title":"<code>default_model</code>  <code>property</code>","text":"<p>Get the default model name (context-aware).</p>"},{"location":"api/settings/#udspy.settings.Settings-functions","title":"Functions","text":""},{"location":"api/settings/#udspy.settings.Settings.configure","title":"<code>configure(api_key='', base_url=None, model=None, aclient=None, **kwargs)</code>","text":"<p>Configure global OpenAI client and defaults.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenAI API key (creates default async client)</p> <code>''</code> <code>base_url</code> <code>str | None</code> <p>Base URL for OpenAI API</p> <code>None</code> <code>model</code> <code>str | None</code> <p>Default model to use for all predictions</p> <code>None</code> <code>aclient</code> <code>AsyncOpenAI | None</code> <p>Custom async OpenAI client</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Default kwargs for all chat completions (temperature, etc.)</p> <code>{}</code> Example <pre><code>import udspy\n\n# With API key\nudspy.settings.configure(api_key=\"sk-...\", model=\"gpt-4o\")\n\n# With custom client\nfrom openai import AsyncOpenAI\nclient = AsyncOpenAI(api_key=\"sk-...\", timeout=30.0)\nudspy.settings.configure(aclient=client, model=\"gpt-4o\")\n</code></pre> Source code in <code>src/udspy/settings.py</code> <pre><code>def configure(\n    self,\n    api_key: str = \"\",\n    base_url: str | None = None,\n    model: str | None = None,\n    aclient: AsyncOpenAI | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Configure global OpenAI client and defaults.\n\n    Args:\n        api_key: OpenAI API key (creates default async client)\n        base_url: Base URL for OpenAI API\n        model: Default model to use for all predictions\n        aclient: Custom async OpenAI client\n        **kwargs: Default kwargs for all chat completions (temperature, etc.)\n\n    Example:\n        ```python\n        import udspy\n\n        # With API key\n        udspy.settings.configure(api_key=\"sk-...\", model=\"gpt-4o\")\n\n        # With custom client\n        from openai import AsyncOpenAI\n        client = AsyncOpenAI(api_key=\"sk-...\", timeout=30.0)\n        udspy.settings.configure(aclient=client, model=\"gpt-4o\")\n        ```\n    \"\"\"\n    if aclient:\n        self._aclient = aclient\n    else:\n        self._aclient = AsyncOpenAI(api_key=api_key, base_url=base_url)\n\n    if model:\n        self._default_model = model\n\n    self._default_kwargs.update(kwargs)\n</code></pre>"},{"location":"api/settings/#udspy.settings.Settings.context","title":"<code>context(api_key='', base_url=None, model=None, aclient=None, **kwargs)</code>","text":"<p>Context manager for temporary settings overrides.</p> <p>This is thread-safe and allows you to use different API keys, models, or other settings within a specific context. Useful for multi-tenant applications.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>Temporary OpenAI API key (creates temporary client)</p> <code>''</code> <code>model</code> <code>str | None</code> <p>Temporary model to use</p> <code>None</code> <code>aclient</code> <code>AsyncOpenAI | None</code> <p>Temporary async OpenAI client</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Temporary kwargs for chat completions</p> <code>{}</code> Example <pre><code>import udspy\nfrom udspy import Predict, Signature, InputField, OutputField\n\n# Global settings\nudspy.settings.configure(api_key=\"global-key\", model=\"gpt-4o-mini\")\n\nclass QA(Signature):\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\n\n# Temporary override for a specific context (e.g., different tenant)\nwith udspy.settings.context(api_key=\"tenant-key\", model=\"gpt-4\"):\n    result = predictor(question=\"...\")  # Uses \"tenant-key\" and \"gpt-4\"\n\n# Back to global settings\nresult = predictor(question=\"...\")  # Uses \"global-key\" and \"gpt-4o-mini\"\n</code></pre> Source code in <code>src/udspy/settings.py</code> <pre><code>@contextmanager\ndef context(\n    self,\n    api_key: str = \"\",\n    base_url: str | None = None,\n    model: str | None = None,\n    aclient: AsyncOpenAI | None = None,\n    **kwargs: Any,\n) -&gt; Iterator[None]:\n    \"\"\"Context manager for temporary settings overrides.\n\n    This is thread-safe and allows you to use different API keys, models,\n    or other settings within a specific context. Useful for multi-tenant\n    applications.\n\n    Args:\n        api_key: Temporary OpenAI API key (creates temporary client)\n        model: Temporary model to use\n        aclient: Temporary async OpenAI client\n        **kwargs: Temporary kwargs for chat completions\n\n    Example:\n        ```python\n        import udspy\n        from udspy import Predict, Signature, InputField, OutputField\n\n        # Global settings\n        udspy.settings.configure(api_key=\"global-key\", model=\"gpt-4o-mini\")\n\n        class QA(Signature):\n            question: str = InputField()\n            answer: str = OutputField()\n\n        predictor = Predict(QA)\n\n        # Temporary override for a specific context (e.g., different tenant)\n        with udspy.settings.context(api_key=\"tenant-key\", model=\"gpt-4\"):\n            result = predictor(question=\"...\")  # Uses \"tenant-key\" and \"gpt-4\"\n\n        # Back to global settings\n        result = predictor(question=\"...\")  # Uses \"global-key\" and \"gpt-4o-mini\"\n        ```\n    \"\"\"\n    # Save current context values\n    prev_aclient = self._context_aclient.get()\n    prev_model = self._context_model.get()\n    prev_kwargs = self._context_kwargs.get()\n\n    try:\n        # Set context-specific values\n        if aclient:\n            self._context_aclient.set(aclient)\n        else:\n            self._context_aclient.set(AsyncOpenAI(api_key=api_key, base_url=base_url))\n\n        if model:\n            self._context_model.set(model)\n\n        if kwargs:\n            # Merge with previous context kwargs if any\n            merged_kwargs = (prev_kwargs or {}).copy()\n            merged_kwargs.update(kwargs)\n            self._context_kwargs.set(merged_kwargs)\n\n        yield\n\n    finally:\n        # Restore previous context values\n        self._context_aclient.set(prev_aclient)\n        self._context_model.set(prev_model)\n        self._context_kwargs.set(prev_kwargs)\n</code></pre>"},{"location":"api/signature/","title":"API Reference: Signatures","text":"<p>Signatures define the inputs and outputs for modules. They provide type safety and clear contracts for LLM interactions.</p>"},{"location":"api/signature/#creating-signatures","title":"Creating Signatures","text":""},{"location":"api/signature/#class-based-signatures","title":"Class-based Signatures","text":"<p>The traditional way to define signatures using Python classes:</p> <pre><code>from udspy import Signature, InputField, OutputField\n\nclass QA(Signature):\n    \"\"\"Answer questions concisely.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n</code></pre>"},{"location":"api/signature/#string-signatures-dspy-style","title":"String Signatures (DSPy-style)","text":"<p>For quick prototyping, use the string format <code>\"inputs -&gt; outputs\"</code>:</p> <pre><code>from udspy import Signature\n\n# Simple signature\nQA = Signature.from_string(\"question -&gt; answer\")\n\n# Multiple inputs and outputs\nAnalyze = Signature.from_string(\n    \"context, question -&gt; summary, answer\",\n    \"Analyze text and answer questions\"\n)\n</code></pre> <p>Format: <code>\"input1, input2 -&gt; output1, output2\"</code></p> <ul> <li>Inputs and outputs are comma-separated</li> <li>Whitespace is trimmed automatically</li> <li>All fields default to <code>str</code> type</li> <li>Optional second argument for instructions</li> </ul>"},{"location":"api/signature/#direct-module-usage","title":"Direct Module Usage","text":"<p>All modules automatically recognize string signatures:</p> <pre><code>from udspy import Predict, ChainOfThought\n\n# These are equivalent\npredictor1 = Predict(\"question -&gt; answer\")\npredictor2 = Predict(Signature.from_string(\"question -&gt; answer\"))\n</code></pre>"},{"location":"api/signature/#when-to-use-each-format","title":"When to Use Each Format","text":"<p>Use string signatures (<code>from_string</code>) when: - Prototyping quickly - All fields are strings - You don't need field descriptions - The signature is simple</p> <p>Use class-based signatures when: - You need custom types (int, list, custom Pydantic models) - You want field descriptions for better LLM guidance - The signature is complex - You want IDE autocomplete and type checking</p>"},{"location":"api/signature/#examples","title":"Examples","text":""},{"location":"api/signature/#basic-string-signature","title":"Basic String Signature","text":"<pre><code>from udspy import Predict\n\npredictor = Predict(\"question -&gt; answer\")\nresult = predictor(question=\"What is Python?\")\nprint(result.answer)\n</code></pre>"},{"location":"api/signature/#multiple-fields","title":"Multiple Fields","text":"<pre><code>from udspy import ChainOfThought\n\ncot = ChainOfThought(\"context, question -&gt; summary, answer\")\nresult = cot(\n    context=\"Python is a programming language\",\n    question=\"What is Python?\"\n)\nprint(result.reasoning)\nprint(result.summary)\nprint(result.answer)\n</code></pre>"},{"location":"api/signature/#with-instructions","title":"With Instructions","text":"<pre><code>QA = Signature.from_string(\n    \"question -&gt; answer\",\n    \"Answer questions concisely and accurately\"\n)\npredictor = Predict(QA)\n</code></pre>"},{"location":"api/signature/#comparison","title":"Comparison","text":"<pre><code># String format - quick and simple\nQA_String = Signature.from_string(\"question -&gt; answer\")\n\n# Class format - more control\nclass QA_Class(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField(description=\"Question to answer\")\n    answer: str = OutputField(description=\"Concise answer\")\n</code></pre>"},{"location":"api/signature/#api-reference","title":"API Reference","text":""},{"location":"api/signature/#udspy.signature","title":"<code>udspy.signature</code>","text":"<p>Signature definitions for structured LLM inputs and outputs.</p>"},{"location":"api/signature/#udspy.signature-classes","title":"Classes","text":""},{"location":"api/signature/#udspy.signature.Signature","title":"<code>Signature</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for defining LLM task signatures.</p> <p>A Signature specifies the input and output fields for an LLM task, along with an optional instruction describing the task.</p> Example <pre><code>class QA(Signature):\n    '''Answer questions concisely.'''\n    question: str = InputField(description=\"Question to answer\")\n    answer: str = OutputField(description=\"Concise answer\")\n</code></pre> Source code in <code>src/udspy/signature.py</code> <pre><code>class Signature(BaseModel, metaclass=SignatureMeta):\n    \"\"\"Base class for defining LLM task signatures.\n\n    A Signature specifies the input and output fields for an LLM task,\n    along with an optional instruction describing the task.\n\n    Example:\n        ```python\n        class QA(Signature):\n            '''Answer questions concisely.'''\n            question: str = InputField(description=\"Question to answer\")\n            answer: str = OutputField(description=\"Concise answer\")\n        ```\n    \"\"\"\n\n    @classmethod\n    def get_input_fields(cls) -&gt; dict[str, FieldInfo]:\n        \"\"\"Get all input fields defined in this signature.\"\"\"\n        return {\n            name: field_info\n            for name, field_info in cls.model_fields.items()\n            if (field_info.json_schema_extra or {}).get(\"__udspy_field_type\") == \"input\"  # type: ignore[union-attr]\n        }\n\n    @classmethod\n    def get_output_fields(cls) -&gt; dict[str, FieldInfo]:\n        \"\"\"Get all output fields defined in this signature.\"\"\"\n        return {\n            name: field_info\n            for name, field_info in cls.model_fields.items()\n            if (field_info.json_schema_extra or {}).get(\"__udspy_field_type\") == \"output\"  # type: ignore[union-attr]\n        }\n\n    @classmethod\n    def get_instructions(cls) -&gt; str:\n        \"\"\"Get the task instructions from the docstring.\"\"\"\n        return (cls.__doc__ or \"\").strip()\n\n    @classmethod\n    def from_string(cls, spec: str, instructions: str = \"\") -&gt; type[\"Signature\"]:\n        \"\"\"Create a Signature from DSPy-style string format.\n\n        This is a convenience method for creating simple signatures using\n        the DSPy string format \"input1, input2 -&gt; output1, output2\".\n        All fields default to type `str`.\n\n        For more control over field types, descriptions, and defaults,\n        use the class-based Signature definition or `make_signature()`.\n\n        Args:\n            spec: Signature specification in format \"inputs -&gt; outputs\"\n                  Examples: \"question -&gt; answer\"\n                           \"context, question -&gt; answer\"\n                           \"text -&gt; summary, keywords\"\n            instructions: Optional task instructions (docstring)\n\n        Returns:\n            A new Signature class with all fields as type `str`\n\n        Raises:\n            ValueError: If spec is not in valid format\n\n        Example:\n            ```python\n            # Simple signature\n            QA = Signature.from_string(\"question -&gt; answer\", \"Answer questions\")\n            predictor = Predict(QA)\n\n            # Multiple inputs and outputs\n            Summarize = Signature.from_string(\n                \"document, style -&gt; summary, keywords\",\n                \"Summarize documents in specified style\"\n            )\n            ```\n\n        Note:\n            This is equivalent to DSPy's string-based signature creation.\n            All fields default to `str` type. For custom types, use the\n            class-based approach with InputField() and OutputField().\n        \"\"\"\n        # Split on '-&gt;'\n        if \"-&gt;\" not in spec:\n            raise ValueError(\n                f\"Invalid signature format: '{spec}'. \"\n                \"Must be in format 'inputs -&gt; outputs' (e.g., 'question -&gt; answer')\"\n            )\n\n        parts = spec.split(\"-&gt;\")\n        if len(parts) != 2:\n            raise ValueError(\n                f\"Invalid signature format: '{spec}'. \"\n                \"Must have exactly one '-&gt;' separator\"\n            )\n\n        # Parse inputs (comma-separated, trimmed)\n        input_str = parts[0].strip()\n        if not input_str:\n            raise ValueError(\"Signature must have at least one input field\")\n\n        input_names = [name.strip() for name in input_str.split(\",\")]\n        input_fields: dict[str, type] = {name: str for name in input_names if name}\n\n        # Parse outputs (comma-separated, trimmed)\n        output_str = parts[1].strip()\n        if not output_str:\n            raise ValueError(\"Signature must have at least one output field\")\n\n        output_names = [name.strip() for name in output_str.split(\",\")]\n        output_fields: dict[str, type] = {name: str for name in output_names if name}\n\n        return make_signature(input_fields, output_fields, instructions)\n</code></pre>"},{"location":"api/signature/#udspy.signature.Signature-functions","title":"Functions","text":""},{"location":"api/signature/#udspy.signature.Signature.from_string","title":"<code>from_string(spec, instructions='')</code>  <code>classmethod</code>","text":"<p>Create a Signature from DSPy-style string format.</p> <p>This is a convenience method for creating simple signatures using the DSPy string format \"input1, input2 -&gt; output1, output2\". All fields default to type <code>str</code>.</p> <p>For more control over field types, descriptions, and defaults, use the class-based Signature definition or <code>make_signature()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>str</code> <p>Signature specification in format \"inputs -&gt; outputs\"   Examples: \"question -&gt; answer\"            \"context, question -&gt; answer\"            \"text -&gt; summary, keywords\"</p> required <code>instructions</code> <code>str</code> <p>Optional task instructions (docstring)</p> <code>''</code> <p>Returns:</p> Type Description <code>type[Signature]</code> <p>A new Signature class with all fields as type <code>str</code></p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If spec is not in valid format</p> Example <pre><code># Simple signature\nQA = Signature.from_string(\"question -&gt; answer\", \"Answer questions\")\npredictor = Predict(QA)\n\n# Multiple inputs and outputs\nSummarize = Signature.from_string(\n    \"document, style -&gt; summary, keywords\",\n    \"Summarize documents in specified style\"\n)\n</code></pre> Note <p>This is equivalent to DSPy's string-based signature creation. All fields default to <code>str</code> type. For custom types, use the class-based approach with InputField() and OutputField().</p> Source code in <code>src/udspy/signature.py</code> <pre><code>@classmethod\ndef from_string(cls, spec: str, instructions: str = \"\") -&gt; type[\"Signature\"]:\n    \"\"\"Create a Signature from DSPy-style string format.\n\n    This is a convenience method for creating simple signatures using\n    the DSPy string format \"input1, input2 -&gt; output1, output2\".\n    All fields default to type `str`.\n\n    For more control over field types, descriptions, and defaults,\n    use the class-based Signature definition or `make_signature()`.\n\n    Args:\n        spec: Signature specification in format \"inputs -&gt; outputs\"\n              Examples: \"question -&gt; answer\"\n                       \"context, question -&gt; answer\"\n                       \"text -&gt; summary, keywords\"\n        instructions: Optional task instructions (docstring)\n\n    Returns:\n        A new Signature class with all fields as type `str`\n\n    Raises:\n        ValueError: If spec is not in valid format\n\n    Example:\n        ```python\n        # Simple signature\n        QA = Signature.from_string(\"question -&gt; answer\", \"Answer questions\")\n        predictor = Predict(QA)\n\n        # Multiple inputs and outputs\n        Summarize = Signature.from_string(\n            \"document, style -&gt; summary, keywords\",\n            \"Summarize documents in specified style\"\n        )\n        ```\n\n    Note:\n        This is equivalent to DSPy's string-based signature creation.\n        All fields default to `str` type. For custom types, use the\n        class-based approach with InputField() and OutputField().\n    \"\"\"\n    # Split on '-&gt;'\n    if \"-&gt;\" not in spec:\n        raise ValueError(\n            f\"Invalid signature format: '{spec}'. \"\n            \"Must be in format 'inputs -&gt; outputs' (e.g., 'question -&gt; answer')\"\n        )\n\n    parts = spec.split(\"-&gt;\")\n    if len(parts) != 2:\n        raise ValueError(\n            f\"Invalid signature format: '{spec}'. \"\n            \"Must have exactly one '-&gt;' separator\"\n        )\n\n    # Parse inputs (comma-separated, trimmed)\n    input_str = parts[0].strip()\n    if not input_str:\n        raise ValueError(\"Signature must have at least one input field\")\n\n    input_names = [name.strip() for name in input_str.split(\",\")]\n    input_fields: dict[str, type] = {name: str for name in input_names if name}\n\n    # Parse outputs (comma-separated, trimmed)\n    output_str = parts[1].strip()\n    if not output_str:\n        raise ValueError(\"Signature must have at least one output field\")\n\n    output_names = [name.strip() for name in output_str.split(\",\")]\n    output_fields: dict[str, type] = {name: str for name in output_names if name}\n\n    return make_signature(input_fields, output_fields, instructions)\n</code></pre>"},{"location":"api/signature/#udspy.signature.Signature.get_input_fields","title":"<code>get_input_fields()</code>  <code>classmethod</code>","text":"<p>Get all input fields defined in this signature.</p> Source code in <code>src/udspy/signature.py</code> <pre><code>@classmethod\ndef get_input_fields(cls) -&gt; dict[str, FieldInfo]:\n    \"\"\"Get all input fields defined in this signature.\"\"\"\n    return {\n        name: field_info\n        for name, field_info in cls.model_fields.items()\n        if (field_info.json_schema_extra or {}).get(\"__udspy_field_type\") == \"input\"  # type: ignore[union-attr]\n    }\n</code></pre>"},{"location":"api/signature/#udspy.signature.Signature.get_instructions","title":"<code>get_instructions()</code>  <code>classmethod</code>","text":"<p>Get the task instructions from the docstring.</p> Source code in <code>src/udspy/signature.py</code> <pre><code>@classmethod\ndef get_instructions(cls) -&gt; str:\n    \"\"\"Get the task instructions from the docstring.\"\"\"\n    return (cls.__doc__ or \"\").strip()\n</code></pre>"},{"location":"api/signature/#udspy.signature.Signature.get_output_fields","title":"<code>get_output_fields()</code>  <code>classmethod</code>","text":"<p>Get all output fields defined in this signature.</p> Source code in <code>src/udspy/signature.py</code> <pre><code>@classmethod\ndef get_output_fields(cls) -&gt; dict[str, FieldInfo]:\n    \"\"\"Get all output fields defined in this signature.\"\"\"\n    return {\n        name: field_info\n        for name, field_info in cls.model_fields.items()\n        if (field_info.json_schema_extra or {}).get(\"__udspy_field_type\") == \"output\"  # type: ignore[union-attr]\n    }\n</code></pre>"},{"location":"api/signature/#udspy.signature.SignatureMeta","title":"<code>SignatureMeta</code>","text":"<p>               Bases: <code>type(BaseModel)</code></p> <p>Metaclass for Signature that validates field types.</p> Source code in <code>src/udspy/signature.py</code> <pre><code>class SignatureMeta(type(BaseModel)):  # type: ignore[misc]\n    \"\"\"Metaclass for Signature that validates field types.\"\"\"\n\n    def __new__(\n        mcs,\n        name: str,\n        bases: tuple[type, ...],\n        namespace: dict[str, Any],\n        **kwargs: Any,\n    ) -&gt; type:\n        cls = super().__new__(mcs, name, bases, namespace, **kwargs)\n\n        # Skip validation for the base Signature class\n        if name == \"Signature\":\n            return cls\n\n        # Validate that all fields are marked as input or output\n        for field_name, field_info in cls.model_fields.items():\n            if not isinstance(field_info, FieldInfo):\n                continue\n\n            json_schema_extra = field_info.json_schema_extra or {}\n            field_type = json_schema_extra.get(\"__udspy_field_type\")  # type: ignore[union-attr]\n\n            if field_type not in (\"input\", \"output\"):\n                raise TypeError(\n                    f\"Field '{field_name}' in {name} must be declared with \"\n                    f\"InputField() or OutputField()\"\n                )\n\n        return cls\n</code></pre>"},{"location":"api/signature/#udspy.signature-functions","title":"Functions","text":""},{"location":"api/signature/#udspy.signature.InputField","title":"<code>InputField(default=..., *, description=None, **kwargs)</code>","text":"<p>Define an input field for a Signature.</p> <p>Parameters:</p> Name Type Description Default <code>default</code> <code>Any</code> <p>Default value for the field</p> <code>...</code> <code>description</code> <code>str | None</code> <p>Human-readable description of the field's purpose</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional Pydantic field arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic FieldInfo with input metadata</p> Source code in <code>src/udspy/signature.py</code> <pre><code>def InputField(\n    default: Any = ...,\n    *,\n    description: str | None = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Define an input field for a Signature.\n\n    Args:\n        default: Default value for the field\n        description: Human-readable description of the field's purpose\n        **kwargs: Additional Pydantic field arguments\n\n    Returns:\n        A Pydantic FieldInfo with input metadata\n    \"\"\"\n    json_schema_extra = kwargs.pop(\"json_schema_extra\", {})\n    json_schema_extra[\"__udspy_field_type\"] = \"input\"\n\n    return Field(\n        default=default,\n        description=description,\n        json_schema_extra=json_schema_extra,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/signature/#udspy.signature.OutputField","title":"<code>OutputField(default=..., *, description=None, **kwargs)</code>","text":"<p>Define an output field for a Signature.</p> <p>Parameters:</p> Name Type Description Default <code>default</code> <code>Any</code> <p>Default value for the field</p> <code>...</code> <code>description</code> <code>str | None</code> <p>Human-readable description of the field's purpose</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional Pydantic field arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic FieldInfo with output metadata</p> Source code in <code>src/udspy/signature.py</code> <pre><code>def OutputField(\n    default: Any = ...,\n    *,\n    description: str | None = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Define an output field for a Signature.\n\n    Args:\n        default: Default value for the field\n        description: Human-readable description of the field's purpose\n        **kwargs: Additional Pydantic field arguments\n\n    Returns:\n        A Pydantic FieldInfo with output metadata\n    \"\"\"\n    json_schema_extra = kwargs.pop(\"json_schema_extra\", {})\n    json_schema_extra[\"__udspy_field_type\"] = \"output\"\n\n    return Field(\n        default=default,\n        description=description,\n        json_schema_extra=json_schema_extra,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/signature/#udspy.signature.make_signature","title":"<code>make_signature(input_fields, output_fields, instructions='')</code>","text":"<p>Dynamically create a Signature class.</p> <p>Parameters:</p> Name Type Description Default <code>input_fields</code> <code>dict[str, type]</code> <p>Dictionary mapping field names to types for inputs</p> required <code>output_fields</code> <code>dict[str, type]</code> <p>Dictionary mapping field names to types for outputs</p> required <code>instructions</code> <code>str</code> <p>Task instructions</p> <code>''</code> <p>Returns:</p> Type Description <code>type[Signature]</code> <p>A new Signature class</p> Example <pre><code>QA = make_signature(\n    {\"question\": str},\n    {\"answer\": str},\n    \"Answer questions concisely\"\n)\n</code></pre> Source code in <code>src/udspy/signature.py</code> <pre><code>def make_signature(\n    input_fields: dict[str, type],\n    output_fields: dict[str, type],\n    instructions: str = \"\",\n) -&gt; type[Signature]:\n    \"\"\"Dynamically create a Signature class.\n\n    Args:\n        input_fields: Dictionary mapping field names to types for inputs\n        output_fields: Dictionary mapping field names to types for outputs\n        instructions: Task instructions\n\n    Returns:\n        A new Signature class\n\n    Example:\n        ```python\n        QA = make_signature(\n            {\"question\": str},\n            {\"answer\": str},\n            \"Answer questions concisely\"\n        )\n        ```\n    \"\"\"\n    fields = {}\n\n    for name, type_ in input_fields.items():\n        fields[name] = (type_, InputField())\n\n    for name, type_ in output_fields.items():\n        fields[name] = (type_, OutputField())\n\n    sig = create_model(\n        \"DynamicSignature\",\n        __base__=Signature,\n        **fields,  # type: ignore\n    )\n\n    if instructions:\n        sig.__doc__ = instructions\n\n    return sig\n</code></pre>"},{"location":"api/streaming/","title":"API Reference: Streaming","text":""},{"location":"api/streaming/#udspy.streaming","title":"<code>udspy.streaming</code>","text":"<p>Streaming support with event queue for incremental LLM outputs and tool updates.</p>"},{"location":"api/streaming/#udspy.streaming-classes","title":"Classes","text":""},{"location":"api/streaming/#udspy.streaming.Prediction","title":"<code>Prediction</code>","text":"<p>               Bases: <code>StreamEvent</code>, <code>dict[str, Any]</code></p> <p>Final prediction result with attribute access.</p> <p>This is both a StreamEvent (can be yielded from astream) and a dict (for convenient attribute access to outputs).</p> Example <pre><code>pred = Prediction(answer=\"Paris\", reasoning=\"France's capital\")\nprint(pred.answer)  # \"Paris\"\nprint(pred[\"answer\"])  # \"Paris\"\n</code></pre> Source code in <code>src/udspy/streaming.py</code> <pre><code>class Prediction(StreamEvent, dict[str, Any]):\n    \"\"\"Final prediction result with attribute access.\n\n    This is both a StreamEvent (can be yielded from astream) and a dict\n    (for convenient attribute access to outputs).\n\n    Example:\n        ```python\n        pred = Prediction(answer=\"Paris\", reasoning=\"France's capital\")\n        print(pred.answer)  # \"Paris\"\n        print(pred[\"answer\"])  # \"Paris\"\n        ```\n    \"\"\"\n\n    def __getattr__(self, name: str) -&gt; Any:\n        try:\n            return self[name]\n        except KeyError:\n            raise AttributeError(f\"Prediction has no attribute '{name}'\") from None\n\n    def __setattr__(self, name: str, value: Any) -&gt; None:\n        self[name] = value\n</code></pre>"},{"location":"api/streaming/#udspy.streaming.StreamChunk","title":"<code>StreamChunk</code>","text":"<p>               Bases: <code>StreamEvent</code></p> <p>A chunk of streamed LLM output for a specific field.</p> <p>Attributes:</p> Name Type Description <code>field_name</code> <p>Name of the output field</p> <code>delta</code> <p>Incremental content for this field (new text since last chunk)</p> <code>content</code> <p>Full accumulated content for this field so far</p> <code>is_complete</code> <p>Whether this field is finished streaming</p> Source code in <code>src/udspy/streaming.py</code> <pre><code>class StreamChunk(StreamEvent):\n    \"\"\"A chunk of streamed LLM output for a specific field.\n\n    Attributes:\n        field_name: Name of the output field\n        delta: Incremental content for this field (new text since last chunk)\n        content: Full accumulated content for this field so far\n        is_complete: Whether this field is finished streaming\n    \"\"\"\n\n    def __init__(\n        self, module: \"Module\", field_name: str, delta: str, content: str, is_complete: bool = False\n    ):\n        self.module = module\n        self.field_name = field_name\n        self.delta = delta\n        self.content = content\n        self.is_complete = is_complete\n\n    def __repr__(self) -&gt; str:\n        status = \"complete\" if self.is_complete else \"streaming\"\n        return (\n            f\"StreamChunk(field={self.field_name}, status={status}, \"\n            f\"delta={self.delta!r}, content={self.content!r})\"\n        )\n</code></pre>"},{"location":"api/streaming/#udspy.streaming.StreamEvent","title":"<code>StreamEvent</code>","text":"<p>Base class for all stream events.</p> <p>Users can define custom event types by inheriting from this class. The only built-in events are StreamChunk and Prediction.</p> Example <pre><code>from dataclasses import dataclass\nfrom udspy.streaming import StreamEvent, emit_event\n\n@dataclass\nclass ToolProgress(StreamEvent):\n    tool_name: str\n    message: str\n    progress: float  # 0.0 to 1.0\n\n# In your tool:\nasync def my_tool():\n    await emit_event(ToolProgress(\"search\", \"Searching...\", 0.5))\n</code></pre> Source code in <code>src/udspy/streaming.py</code> <pre><code>class StreamEvent:\n    \"\"\"Base class for all stream events.\n\n    Users can define custom event types by inheriting from this class.\n    The only built-in events are StreamChunk and Prediction.\n\n    Example:\n        ```python\n        from dataclasses import dataclass\n        from udspy.streaming import StreamEvent, emit_event\n\n        @dataclass\n        class ToolProgress(StreamEvent):\n            tool_name: str\n            message: str\n            progress: float  # 0.0 to 1.0\n\n        # In your tool:\n        async def my_tool():\n            await emit_event(ToolProgress(\"search\", \"Searching...\", 0.5))\n        ```\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/streaming/#udspy.streaming-functions","title":"Functions","text":""},{"location":"api/streaming/#udspy.streaming.emit_event","title":"<code>emit_event(event)</code>  <code>async</code>","text":"<p>Emit an event to the active stream.</p> <p>This can be called from anywhere (tools, callbacks, etc.) to inject events into the current streaming context. If no stream is active, this is a no-op (silently ignored).</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>StreamEvent</code> <p>The event to emit (any subclass of StreamEvent)</p> required Example <pre><code>from udspy.streaming import emit_event, StreamEvent\nfrom dataclasses import dataclass\n\n@dataclass\nclass ToolStatus(StreamEvent):\n    message: str\n\nasync def my_tool():\n    await emit_event(ToolStatus(\"Starting search...\"))\n    result = await do_search()\n    await emit_event(ToolStatus(\"Search complete\"))\n    return result\n\n# In the stream consumer:\nasync for event in predictor.astream(question=\"...\"):\n    if isinstance(event, ToolStatus):\n        print(f\"\ud83d\udcca {event.message}\")\n    elif isinstance(event, StreamChunk):\n        print(event.delta, end=\"\", flush=True)\n</code></pre> Source code in <code>src/udspy/streaming.py</code> <pre><code>async def emit_event(event: StreamEvent) -&gt; None:\n    \"\"\"Emit an event to the active stream.\n\n    This can be called from anywhere (tools, callbacks, etc.) to inject\n    events into the current streaming context. If no stream is active,\n    this is a no-op (silently ignored).\n\n    Args:\n        event: The event to emit (any subclass of StreamEvent)\n\n    Example:\n        ```python\n        from udspy.streaming import emit_event, StreamEvent\n        from dataclasses import dataclass\n\n        @dataclass\n        class ToolStatus(StreamEvent):\n            message: str\n\n        async def my_tool():\n            await emit_event(ToolStatus(\"Starting search...\"))\n            result = await do_search()\n            await emit_event(ToolStatus(\"Search complete\"))\n            return result\n\n        # In the stream consumer:\n        async for event in predictor.astream(question=\"...\"):\n            if isinstance(event, ToolStatus):\n                print(f\"\ud83d\udcca {event.message}\")\n            elif isinstance(event, StreamChunk):\n                print(event.delta, end=\"\", flush=True)\n        ```\n    \"\"\"\n    queue = _stream_queue.get()\n    if queue is not None:\n        await queue.put(event)\n</code></pre>"},{"location":"api/tool/","title":"Tool API Reference","text":"<p>API documentation for creating and using tools with native OpenAI function calling.</p>"},{"location":"api/tool/#module-udspytool","title":"Module: <code>udspy.tool</code>","text":""},{"location":"api/tool/#tool-decorator","title":"<code>@tool</code> Decorator","text":"<pre><code>@tool(\n    name: str | None = None,\n    description: str | None = None,\n    *,\n    interruptible: bool = False,\n) -&gt; Callable[[Callable], Tool]\n</code></pre> <p>Decorator to mark a function as a tool for use with <code>Predict</code> and <code>ReAct</code> modules.</p> <p>Parameters:</p> <ul> <li><code>name</code> (<code>str | None</code>, default: <code>None</code>): Tool name visible to the LLM</li> <li>If not provided, uses the function name</li> <li><code>description</code> (<code>str | None</code>, default: <code>None</code>): Tool description visible to the LLM</li> <li>If not provided, uses the function's docstring</li> <li>This helps the LLM decide when to use the tool</li> <li><code>interruptible</code> (<code>bool</code>, default: <code>False</code>): Whether to require user confirmation before execution</li> <li>If <code>True</code>, wraps the tool with <code>@interruptible</code> decorator</li> <li>Raises <code>HumanInTheLoopRequired</code> on first call, executes after approval</li> <li>Useful for destructive or sensitive operations</li> </ul> <p>Returns:</p> <ul> <li><code>Tool</code>: A wrapped tool object with metadata</li> </ul> <p>Example:</p> <pre><code>from pydantic import Field\nfrom udspy import tool\n\n@tool(name=\"calculator\", description=\"Perform arithmetic operations\")\ndef calculator(\n    operation: str = Field(description=\"add, subtract, multiply, or divide\"),\n    a: float = Field(description=\"First number\"),\n    b: float = Field(description=\"Second number\"),\n) -&gt; float:\n    \"\"\"Execute arithmetic operation.\"\"\"\n    ops = {\n        \"add\": a + b,\n        \"subtract\": a - b,\n        \"multiply\": a * b,\n        \"divide\": a / b if b != 0 else float(\"inf\"),\n    }\n    return ops[operation]\n</code></pre>"},{"location":"api/tool/#tool-interruption-example","title":"Tool Interruption Example","text":"<pre><code>import os\nfrom pydantic import Field\nfrom udspy import tool\n\n@tool(\n    name=\"delete_file\",\n    description=\"Delete a file from the filesystem\",\n    interruptible=True  # Requires user confirmation\n)\ndef delete_file(path: str = Field(description=\"File path to delete\")) -&gt; str:\n    \"\"\"Delete a file (requires confirmation).\"\"\"\n    os.remove(path)\n    return f\"Deleted {path}\"\n</code></pre>"},{"location":"api/tool/#tool-class","title":"<code>Tool</code> Class","text":"<pre><code>class Tool:\n    \"\"\"Wrapper for a tool function with metadata.\"\"\"\n</code></pre> <p>The <code>Tool</code> class wraps a function and adds metadata for OpenAI function calling. You typically don't instantiate this directly; use the <code>@tool</code> decorator instead.</p>"},{"location":"api/tool/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    func: Callable,\n    name: str | None = None,\n    description: str | None = None,\n    *,\n    interruptible: bool = False,\n    desc: str | None = None,\n    args: dict[str, str] | None = None,\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>func</code> (<code>Callable</code>): The function to wrap</li> <li><code>name</code> (<code>str | None</code>): Tool name (defaults to function name)</li> <li><code>description</code> (<code>str | None</code>): Tool description (defaults to docstring)</li> <li><code>interruptible</code> (<code>bool</code>, default: <code>False</code>): Whether to require confirmation before execution</li> <li><code>desc</code> (<code>str | None</code>): Alias for <code>description</code> (DSPy compatibility)</li> <li><code>args</code> (<code>dict[str, str] | None</code>): Manual argument specification (DSPy compatibility)</li> </ul> <p>Example:</p> <pre><code>from udspy import Tool\n\ndef my_function(x: int, y: int) -&gt; int:\n    \"\"\"Add two numbers.\"\"\"\n    return x + y\n\ntool = Tool(my_function, name=\"adder\", description=\"Adds numbers\")\n</code></pre> <p>However, using the <code>@tool</code> decorator is preferred:</p> <pre><code>@tool(name=\"adder\", description=\"Adds numbers\")\ndef my_function(x: int, y: int) -&gt; int:\n    \"\"\"Add two numbers.\"\"\"\n    return x + y\n</code></pre>"},{"location":"api/tool/#attributes","title":"Attributes","text":""},{"location":"api/tool/#func","title":"<code>func</code>","text":"<pre><code>func: Callable\n</code></pre> <p>The underlying function that this tool wraps.</p>"},{"location":"api/tool/#name","title":"<code>name</code>","text":"<pre><code>name: str\n</code></pre> <p>The tool's name as seen by the LLM.</p>"},{"location":"api/tool/#description","title":"<code>description</code>","text":"<pre><code>description: str\n</code></pre> <p>The tool's description as seen by the LLM.</p>"},{"location":"api/tool/#interruptible","title":"<code>interruptible</code>","text":"<pre><code>interruptible: bool\n</code></pre> <p>Whether this tool requires user confirmation before execution.</p>"},{"location":"api/tool/#parameters","title":"<code>parameters</code>","text":"<pre><code>parameters: dict[str, dict[str, Any]]\n</code></pre> <p>Dictionary mapping parameter names to their metadata:</p> <pre><code>{\n    \"param_name\": {\n        \"type\": str,  # Python type\n        \"description\": \"Parameter description\",\n        \"required\": True  # Whether parameter is required\n    },\n    # ...\n}\n</code></pre>"},{"location":"api/tool/#desc-dspy-compatibility","title":"<code>desc</code> (DSPy compatibility)","text":"<pre><code>desc: str\n</code></pre> <p>Alias for <code>description</code>. Provided for DSPy compatibility.</p>"},{"location":"api/tool/#args-dspy-compatibility","title":"<code>args</code> (DSPy compatibility)","text":"<pre><code>args: dict[str, str]\n</code></pre> <p>Dictionary mapping parameter names to type + description strings. Provided for DSPy compatibility.</p> <p>Example:</p> <pre><code>{\n    \"operation\": \"str - add, subtract, multiply, or divide\",\n    \"a\": \"float - First number\",\n    \"b\": \"float - Second number\"\n}\n</code></pre>"},{"location":"api/tool/#methods","title":"Methods","text":""},{"location":"api/tool/#__call__args-kwargs-any","title":"<code>__call__(*args, **kwargs) -&gt; Any</code>","text":"<p>Call the wrapped function synchronously.</p> <p>Parameters:</p> <ul> <li><code>*args</code>: Positional arguments</li> <li><code>**kwargs</code>: Keyword arguments</li> </ul> <p>Returns:</p> <ul> <li>Function result</li> </ul> <p>Example:</p> <pre><code>@tool(name=\"add\", description=\"Add numbers\")\ndef add(a: int, b: int) -&gt; int:\n    return a + b\n\nresult = add(2, 3)  # Returns 5\n</code></pre>"},{"location":"api/tool/#acallkwargs-any","title":"<code>acall(**kwargs) -&gt; Any</code>","text":"<p>Call the wrapped function asynchronously.</p> <ul> <li>If the function is async, awaits it directly</li> <li>If the function is sync, runs it in an executor to avoid blocking (unless <code>interruptible=True</code>)</li> <li>If <code>interruptible=True</code>, may raise <code>HumanInTheLoopRequired</code> before execution</li> </ul> <p>Parameters:</p> <ul> <li><code>**kwargs</code>: Keyword arguments</li> </ul> <p>Returns:</p> <ul> <li>Awaitable that resolves to the function result</li> </ul> <p>Raises:</p> <ul> <li><code>HumanInTheLoopRequired</code>: If <code>interruptible=True</code> and not yet approved</li> </ul> <p>Example:</p> <pre><code>import asyncio\n\n@tool(name=\"fetch\", description=\"Fetch data\")\nasync def fetch_data(url: str) -&gt; str:\n    # Async operation\n    return f\"Data from {url}\"\n\n# Call async\nresult = await fetch_data.acall(url=\"https://example.com\")\n</code></pre> <p>Sync function example:</p> <pre><code>@tool(name=\"compute\", description=\"Compute value\")\ndef compute(x: int) -&gt; int:\n    return x * 2\n\n# Still works with acall - runs in executor\nresult = await compute.acall(x=5)\n</code></pre>"},{"location":"api/tool/#to_openai_schema-dict","title":"<code>to_openai_schema() -&gt; dict</code>","text":"<p>Convert the tool to OpenAI function calling schema.</p> <p>Returns:</p> <ul> <li><code>dict</code>: OpenAI-compatible tool schema</li> </ul> <p>Example:</p> <pre><code>@tool(name=\"calculator\", description=\"Do math\")\ndef calculator(\n    operation: str = Field(description=\"Operation type\"),\n    a: float = Field(description=\"First number\"),\n    b: float = Field(description=\"Second number\"),\n) -&gt; float:\n    return eval(f\"{a} {operation} {b}\")\n\nschema = calculator.to_openai_schema()\n# {\n#     \"type\": \"function\",\n#     \"function\": {\n#         \"name\": \"calculator\",\n#         \"description\": \"Do math\",\n#         \"parameters\": {\n#             \"type\": \"object\",\n#             \"properties\": {\n#                 \"operation\": {\n#                     \"type\": \"string\",\n#                     \"description\": \"Operation type\"\n#                 },\n#                 \"a\": {\n#                     \"type\": \"number\",\n#                     \"description\": \"First number\"\n#                 },\n#                 \"b\": {\n#                     \"type\": \"number\",\n#                     \"description\": \"Second number\"\n#                 }\n#             },\n#             \"required\": [\"operation\", \"a\", \"b\"],\n#             \"additionalProperties\": False\n#         }\n#     }\n# }\n</code></pre>"},{"location":"api/tool/#parameter-type-annotations","title":"Parameter Type Annotations","text":"<p>Tools use Python type hints to generate OpenAI schemas. Supported types:</p> Python Type JSON Schema Type <code>str</code> <code>\"string\"</code> <code>int</code> <code>\"integer\"</code> <code>float</code> <code>\"number\"</code> <code>bool</code> <code>\"boolean\"</code> <code>list</code> <code>\"array\"</code> <code>dict</code> <code>\"object\"</code> <code>Optional[T]</code> Type of <code>T</code> (nullable) <p>Example:</p> <pre><code>from typing import Optional\nfrom pydantic import Field\nfrom udspy import tool\n\n@tool(name=\"search\", description=\"Search with filters\")\ndef search(\n    query: str = Field(description=\"Search query\"),\n    max_results: int = Field(description=\"Maximum results\", default=10),\n    include_archived: bool = Field(description=\"Include archived\", default=False),\n    tags: Optional[list] = Field(description=\"Filter by tags\", default=None),\n) -&gt; str:\n    return f\"Searching for: {query}\"\n</code></pre>"},{"location":"api/tool/#using-pydantic-fields","title":"Using Pydantic Fields","text":"<p>Use <code>pydantic.Field()</code> to add parameter descriptions and defaults:</p> <pre><code>from pydantic import Field\n\n@tool(name=\"example\", description=\"Example tool\")\ndef example_tool(\n    # Required parameter with description\n    query: str = Field(description=\"The search query\"),\n\n    # Optional parameter with default\n    limit: int = Field(description=\"Result limit\", default=10),\n\n    # Optional parameter that can be None\n    filter: Optional[str] = Field(description=\"Optional filter\", default=None),\n) -&gt; str:\n    return f\"Query: {query}, Limit: {limit}\"\n</code></pre> <p>Important:</p> <ul> <li>Always provide descriptions for parameters</li> <li>Use <code>Field(...)</code> or <code>Field()</code> for required parameters (no default)</li> <li>Use <code>Field(default=value)</code> for optional parameters</li> <li>Descriptions help the LLM understand when and how to use the tool</li> </ul>"},{"location":"api/tool/#tool-interruption","title":"Tool Interruption","text":"<p>For destructive or sensitive operations, use <code>interruptible=True</code>:</p> <pre><code>import os\nfrom pydantic import Field\nfrom udspy import tool, HumanInTheLoopRequired\n\n@tool(\n    name=\"delete_all_files\",\n    description=\"Delete all files in a directory\",\n    interruptible=True  # Requires confirmation\n)\ndef delete_all_files(\n    directory: str = Field(description=\"Directory path\")\n) -&gt; str:\n    \"\"\"Delete all files in directory (requires confirmation).\"\"\"\n    for file in os.listdir(directory):\n        os.remove(os.path.join(directory, file))\n    return f\"Deleted all files in {directory}\"\n\n# When ReAct tries to call this tool, it raises HumanInTheLoopRequired on first call\n# After user approves, the tool executes normally\n</code></pre> <p>How it works:</p> <ol> <li>LLM decides to call the tool</li> <li>Tool function (wrapped with <code>@interruptible</code>) raises <code>HumanInTheLoopRequired</code> on first call</li> <li>User sees confirmation prompt: <code>\"Confirm execution of delete_all_files with args: {...}? (yes/no)\"</code></li> <li>User responds with <code>\"yes\"</code>, <code>\"no\"</code>, or modified arguments</li> <li>ReAct resumes execution based on user's response</li> <li>If approved, subsequent calls to the same tool with same args execute normally</li> </ol>"},{"location":"api/tool/#usage-with-react","title":"Usage with ReAct","text":"<pre><code>from pydantic import Field\nfrom udspy import InputField, OutputField, ReAct, Signature, tool\n\n# Define tools\n@tool(name=\"search\", description=\"Search for information\")\ndef search(query: str = Field(description=\"Search query\")) -&gt; str:\n    return f\"Results for: {query}\"\n\n@tool(name=\"calculate\", description=\"Perform calculations\")\ndef calculate(expression: str = Field(description=\"Math expression\")) -&gt; str:\n    return str(eval(expression))\n\n# Define signature\nclass QA(Signature):\n    \"\"\"Answer questions using tools.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\n# Create agent with tools\nagent = ReAct(QA, tools=[search, calculate])\n\n# Execute\nresult = agent(question=\"What is the population of Tokyo times 2?\")\n# Agent will:\n# 1. Call search(\"Tokyo population\")\n# 2. Call calculate(\"population * 2\")\n# 3. Synthesize final answer\n</code></pre>"},{"location":"api/tool/#usage-with-predict","title":"Usage with Predict","text":"<pre><code>from udspy import Predict, Signature, InputField, OutputField, tool\nfrom pydantic import Field\n\n@tool(name=\"get_weather\", description=\"Get current weather\")\ndef get_weather(city: str = Field(description=\"City name\")) -&gt; str:\n    return f\"Weather in {city}: Sunny, 72\u00b0F\"\n\nclass WeatherQuery(Signature):\n    \"\"\"Get weather information.\"\"\"\n    city: str = InputField()\n    weather: str = OutputField()\n\npredictor = Predict(WeatherQuery, tools=[get_weather])\nresult = predictor(city=\"San Francisco\")\n</code></pre>"},{"location":"api/tool/#dspy-compatibility","title":"DSPy Compatibility","text":"<p>The <code>Tool</code> class includes DSPy-compatible attributes:</p> <pre><code>@tool(name=\"search\", description=\"Search tool\")\ndef search(query: str = Field(description=\"Search query\")) -&gt; str:\n    return \"results\"\n\n# DSPy-style access\nprint(search.desc)  # Same as search.description: \"Search tool\"\nprint(search.args)  # {\"query\": \"str - Search query\"}\n</code></pre>"},{"location":"api/tool/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Clear descriptions: Write clear, concise tool and parameter descriptions    <pre><code>@tool(\n    name=\"search_papers\",\n    description=\"Search academic papers by keyword, author, or topic\"\n)\n</code></pre></p> </li> <li> <p>Use Field() for all parameters: Always use <code>Field()</code> with descriptions    <pre><code>def search(\n    query: str = Field(description=\"Search query\"),\n    year: Optional[int] = Field(description=\"Publication year\", default=None)\n)\n</code></pre></p> </li> <li> <p>Require confirmation for destructive ops: Use <code>interruptible=True</code> <pre><code>@tool(name=\"delete\", description=\"Delete data\", interruptible=True)\n</code></pre></p> </li> <li> <p>Handle errors gracefully: Return error messages as strings    <pre><code>@tool(name=\"divide\", description=\"Divide numbers\")\ndef divide(a: float = Field(...), b: float = Field(...)) -&gt; str:\n    if b == 0:\n        return \"Error: Cannot divide by zero\"\n    return str(a / b)\n</code></pre></p> </li> <li> <p>Keep tools focused: Each tool should do one thing well    <pre><code># Good: Focused tool\n@tool(name=\"search_users\", description=\"Search for users\")\ndef search_users(query: str = Field(...)) -&gt; str: ...\n\n# Bad: Too many responsibilities\n@tool(name=\"user_management\", description=\"Manage all user operations\")\ndef user_management(action: str, query: str, data: dict) -&gt; str: ...\n</code></pre></p> </li> </ol>"},{"location":"api/tool/#type-annotations","title":"Type Annotations","text":"<pre><code>from typing import Callable, Any\nfrom udspy import Tool\n\n# Decorator signature\ndef tool(\n    name: str | None = None,\n    description: str | None = None,\n    *,\n    interruptible: bool = False,\n) -&gt; Callable[[Callable[..., Any]], Tool]: ...\n\n# Tool class\nclass Tool:\n    func: Callable[..., Any]\n    name: str\n    description: str\n    interruptible: bool\n    parameters: dict[str, dict[str, Any]]\n    desc: str  # Alias for description\n    args: dict[str, str]  # DSPy compatibility\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any: ...\n    async def acall(self, **kwargs: Any) -&gt; Any: ...\n    def to_openai_schema(self) -&gt; dict[str, Any]: ...\n</code></pre>"},{"location":"api/tool/#see-also","title":"See Also","text":"<ul> <li>Interrupt API - Interrupt system and <code>@interruptible</code> decorator</li> <li>ReAct API - Using tools with ReAct agents</li> <li>ReAct Examples - Tool usage examples</li> <li>Module API - Base module documentation</li> </ul>"},{"location":"architecture/adapters/","title":"Adapters","text":"<p>Adapters handle the translation between Signatures and LLM-specific message formats.</p>"},{"location":"architecture/adapters/#overview","title":"Overview","text":"<p>The <code>ChatAdapter</code> is responsible for:</p> <ol> <li>Converting signatures into system prompts</li> <li>Formatting inputs into user messages</li> <li>Parsing LLM completions into structured outputs</li> <li>Converting Pydantic models to tool schemas</li> </ol>"},{"location":"architecture/adapters/#usage","title":"Usage","text":"<pre><code>from udspy import ChatAdapter\n\nadapter = ChatAdapter()\n</code></pre> <p>Adapters are typically used internally by modules, but can be used directly:</p> <pre><code># Format instructions\ninstructions = adapter.format_instructions(signature)\n\n# Format inputs\nformatted = adapter.format_inputs(signature, {\"question\": \"What is AI?\"})\n\n# Parse outputs\noutputs = adapter.parse_outputs(signature, completion_text)\n</code></pre>"},{"location":"architecture/adapters/#custom-adapters","title":"Custom Adapters","text":"<p>You can create custom adapters by subclassing <code>ChatAdapter</code>:</p> <pre><code>class CustomAdapter(ChatAdapter):\n    def format_instructions(self, signature):\n        # Custom instruction formatting\n        return super().format_instructions(signature) + \"\\nBe creative!\"\n</code></pre> <p>See API: Adapters for detailed documentation.</p>"},{"location":"architecture/decisions/","title":"Architectural Decision Records (ADR)","text":"<p>This document tracks major architectural decisions made in udspy, presented chronologically with context, rationale, and consequences.</p>"},{"location":"architecture/decisions/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Initial Project Setup (2025-01-24)</li> <li>Context Manager for Settings (2025-01-24)</li> <li>Chain of Thought Module (2025-01-24)</li> <li>Human-in-the-Loop with Interruptible Decorator (2025-01-25)</li> <li>ReAct Agent Module (2025-01-25)</li> <li>Unified Module Execution Pattern (aexecute) (2025-01-25)</li> </ol>"},{"location":"architecture/decisions/#adr-001-initial-project-setup","title":"ADR-001: Initial Project Setup","text":"<p>Date: 2025-01-24</p> <p>Status: Accepted</p>"},{"location":"architecture/decisions/#context","title":"Context","text":"<p>Created a minimal DSPy-inspired library focused on resource-constrained environments where DSPy's ~200MB memory footprint (due to LiteLLM) is prohibitive.</p>"},{"location":"architecture/decisions/#decision","title":"Decision","text":"<p>Build a lightweight alternative with: - Native OpenAI tool calling instead of custom adapters - Minimal dependencies (~10MB: <code>openai</code> + <code>pydantic</code>) - Streaming support for reasoning and output fields - Modern Python tooling (uv, ruff, justfile)</p>"},{"location":"architecture/decisions/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"architecture/decisions/#1-native-tool-calling","title":"1. Native Tool Calling","text":"<p>Use OpenAI's native function calling API instead of custom adapters and field markers.</p> <p>Rationale: - OpenAI's tool calling is optimized and well-tested - Reduces complexity and leverages provider's optimizations - Better reliability for structured tool invocation - Forward compatible with future improvements</p> <p>Trade-offs: - Couples to OpenAI's API format (but works with any OpenAI-compatible provider) - May need adapters for other providers in future</p>"},{"location":"architecture/decisions/#2-minimal-dependencies","title":"2. Minimal Dependencies","text":"<p>Only <code>openai</code> and <code>pydantic</code> in core dependencies.</p> <p>Rationale: - Keeps the library lightweight and maintainable (~10MB vs ~200MB) - Reduces potential dependency conflicts - Faster installation and lower memory usage - Suitable for serverless, edge, and embedded deployments</p> <p>Trade-offs: - Can't leverage broader ecosystem for advanced features - Users need to install extras for dev tools</p>"},{"location":"architecture/decisions/#3-pydantic-v2","title":"3. Pydantic v2","text":"<p>Use Pydantic v2 for all models and validation.</p> <p>Rationale: - Modern, fast, well-maintained - Excellent JSON schema generation for tools - Built-in validation and type coercion - Better performance than v1 - Great developer experience with IDE support</p> <p>Trade-offs: - Requires Python 3.7+ (we target 3.11+)</p>"},{"location":"architecture/decisions/#4-streaming-architecture","title":"4. Streaming Architecture","text":"<p>Async-first design using Python's async/await.</p> <p>Rationale: - Python's async is the standard for I/O-bound operations - Native support from OpenAI SDK - Better composability with other async code - Easier to reason about than callbacks</p> <p>Trade-offs: - Requires async runtime (asyncio) - Steeper learning curve for beginners</p>"},{"location":"architecture/decisions/#5-module-abstraction","title":"5. Module Abstraction","text":"<p>Modules compose via Python class inheritance.</p> <p>Rationale: - Similar to DSPy but simplified - Familiar Python patterns (no custom DSL) - Good IDE and type checker support - Signatures define I/O contracts using Pydantic models - Predict is the core primitive for LLM calls</p> <p>Trade-offs: - Less \"magical\" than DSPy's meta-programming - Requires more explicit code</p>"},{"location":"architecture/decisions/#consequences","title":"Consequences","text":"<p>Benefits: - 20x smaller memory footprint (~10MB vs ~200MB) - Works in resource-constrained environments - Simple, maintainable codebase - Compatible with any OpenAI-compatible provider</p> <p>Trade-offs: - Less feature-complete than DSPy - Fewer LLM providers supported out-of-the-box - No built-in optimizers or teleprompters</p>"},{"location":"architecture/decisions/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Fork DSPy: Too much baggage and complexity</li> <li>Use LangChain: Even larger footprint, different philosophy</li> <li>Build from scratch: Chose this - learn from DSPy's excellent patterns</li> </ul>"},{"location":"architecture/decisions/#adr-002-context-manager-for-settings","title":"ADR-002: Context Manager for Settings","text":"<p>Date: 2025-01-24</p> <p>Status: Accepted</p>"},{"location":"architecture/decisions/#context_1","title":"Context","text":"<p>Need to support different API keys and models in different contexts (e.g., multi-tenant apps, different users, testing scenarios, concurrent async operations).</p>"},{"location":"architecture/decisions/#decision_1","title":"Decision","text":"<p>Implement thread-safe context manager using Python's <code>contextvars</code> module:</p> <pre><code># Global settings\nudspy.settings.configure(api_key=\"global-key\", model=\"gpt-4o-mini\")\n\n# Temporary override in context\nwith udspy.settings.context(api_key=\"user-key\", model=\"gpt-4\"):\n    result = predictor(question=\"...\")  # Uses user-key and gpt-4\n\n# Back to global settings\nresult = predictor(question=\"...\")  # Uses global-key and gpt-4o-mini\n</code></pre>"},{"location":"architecture/decisions/#implementation-details","title":"Implementation Details","text":"<ul> <li>Added <code>ContextVar</code> fields to <code>Settings</code> class for each configurable attribute</li> <li>Properties now check context first, then fall back to global settings</li> <li>Context manager saves/restores context state using try/finally</li> <li>Proper cleanup ensures no context leakage</li> </ul>"},{"location":"architecture/decisions/#key-features","title":"Key Features","text":"<ol> <li>Thread-Safe: Uses <code>ContextVar</code> for thread-safe context isolation</li> <li>Nestable: Contexts can be nested with proper inheritance</li> <li>Comprehensive: Supports overriding api_key, model, client, async_client, and any kwargs</li> <li>Clean API: Simple context manager interface</li> <li>Backwards Compatible: Existing code continues to work without changes</li> </ol>"},{"location":"architecture/decisions/#use-cases","title":"Use Cases","text":"<ol> <li> <p>Multi-tenant applications: Different API keys per user    <pre><code>with udspy.settings.context(api_key=user.api_key):\n    result = predictor(question=user.question)\n</code></pre></p> </li> <li> <p>Model selection per request: Use different models for different tasks    <pre><code>with udspy.settings.context(model=\"gpt-4\"):\n    result = expensive_predictor(question=complex_question)\n</code></pre></p> </li> <li> <p>Testing: Isolate test settings without affecting global state    <pre><code>with udspy.settings.context(api_key=\"sk-test\", temperature=0.0):\n    assert predictor(question=\"2+2\").answer == \"4\"\n</code></pre></p> </li> <li> <p>Async operations: Safe concurrent operations with different settings    <pre><code>async def handle_user(user):\n    with udspy.settings.context(api_key=user.api_key):\n        async for chunk in streaming_predictor.stream(...):\n            yield chunk\n</code></pre></p> </li> </ol>"},{"location":"architecture/decisions/#consequences_1","title":"Consequences","text":"<p>Benefits: - Clean separation of concerns (global vs context-specific settings) - No need to pass settings through function parameters - Thread-safe and asyncio task-safe for concurrent operations - Flexible and composable</p> <p>Trade-offs: - Slight complexity increase in Settings class - Context variables have a small performance overhead (negligible) - Must remember to use context manager (but gracefully degrades to global settings)</p>"},{"location":"architecture/decisions/#alternatives-considered_1","title":"Alternatives Considered","text":"<ul> <li>Dependency Injection: More verbose, harder to use</li> <li>Environment Variables: Not dynamic enough for multi-tenant use cases</li> <li>Pass settings everywhere: Too cumbersome</li> </ul>"},{"location":"architecture/decisions/#migration-guide","title":"Migration Guide","text":"<p>No migration needed - feature is additive and backwards compatible.</p>"},{"location":"architecture/decisions/#adr-003-chain-of-thought-module","title":"ADR-003: Chain of Thought Module","text":"<p>Date: 2025-01-24</p> <p>Status: Accepted</p>"},{"location":"architecture/decisions/#context_2","title":"Context","text":"<p>Chain of Thought (CoT) is a proven prompting technique that improves LLM reasoning by explicitly requesting step-by-step thinking. Research shows ~25-30% accuracy improvement on math and reasoning tasks (Wei et al., 2022).</p>"},{"location":"architecture/decisions/#decision_2","title":"Decision","text":"<p>Implement <code>ChainOfThought</code> module that automatically adds a reasoning field to any signature:</p> <pre><code>class QA(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\n# Automatically extends to: question -&gt; reasoning, answer\ncot = ChainOfThought(QA)\nresult = cot(question=\"What is 15 * 23?\")\n\nprint(result.reasoning)  # Shows step-by-step calculation\nprint(result.answer)     # \"345\"\n</code></pre>"},{"location":"architecture/decisions/#implementation-approach","title":"Implementation Approach","text":"<p>Unlike DSPy which uses a <code>signature.prepend()</code> method, udspy takes a simpler approach:</p> <ol> <li>Extract fields from original signature</li> <li>Create extended outputs with reasoning prepended: <code>{\"reasoning\": str, **original_outputs}</code></li> <li>Use make_signature to create new signature dynamically</li> <li>Wrap in Predict with the extended signature</li> </ol> <p>This approach: - Doesn't require adding prepend/insert methods to Signature - Leverages existing <code>make_signature</code> utility - Keeps ChainOfThought as a pure Module wrapper - Only ~45 lines of code</p>"},{"location":"architecture/decisions/#key-features_1","title":"Key Features","text":"<ol> <li>Automatic reasoning field: No manual signature modification needed</li> <li>Customizable description: Override reasoning field description</li> <li>Works with any signature: Single or multiple outputs</li> <li>Transparent: Reasoning is always accessible in results</li> <li>Configurable: All Predict parameters (model, temperature, tools) supported</li> </ol>"},{"location":"architecture/decisions/#research-evidence","title":"Research Evidence","text":"<p>Chain of Thought prompting improves performance on: - Math: ~25-30% accuracy improvement (Wei et al., 2022) - Reasoning: Significant gains on logic puzzles - Multi-step: Better at complex multi-hop reasoning - Transparency: Shows reasoning for verification</p>"},{"location":"architecture/decisions/#use-cases_1","title":"Use Cases","text":"<ol> <li> <p>Math and calculation <pre><code>cot = ChainOfThought(QA, temperature=0.0)\nresult = cot(question=\"What is 157 * 234?\")\n</code></pre></p> </li> <li> <p>Analysis and decision-making <pre><code>class Decision(Signature):\n    scenario: str = InputField()\n    decision: str = OutputField()\n    justification: str = OutputField()\n\ndecider = ChainOfThought(Decision)\n</code></pre></p> </li> <li> <p>Educational applications: Show work/reasoning</p> </li> <li>High-stakes decisions: Require explicit justification</li> <li>Debugging: Understand why LLM made specific choices</li> </ol>"},{"location":"architecture/decisions/#consequences_2","title":"Consequences","text":"<p>Benefits: - Improved accuracy on reasoning tasks - Transparent reasoning process - Easy to verify correctness - Simple API (just wrap any signature) - Minimal code overhead</p> <p>Trade-offs: - Increased token usage (~2-3x for simple tasks) - Slightly higher latency - Not always needed for simple factual queries - Reasoning quality depends on model capability</p>"},{"location":"architecture/decisions/#comparison-with-dspy","title":"Comparison with DSPy","text":"Aspect udspy DSPy API <code>ChainOfThought(signature)</code> <code>dspy.ChainOfThought(signature, rationale_field=...)</code> Implementation Dynamic signature creation Signature.prepend() method Customization <code>reasoning_description</code> param Full <code>rationale_field</code> control Complexity ~45 lines ~40 lines Dependencies Uses <code>make_signature</code> Uses signature mutation <p>Both are equally effective; udspy's approach is simpler but less flexible in edge cases.</p>"},{"location":"architecture/decisions/#alternatives-considered_2","title":"Alternatives Considered","text":"<ul> <li>Prompt Engineering: Less reliable than structured reasoning field</li> <li>Tool-based Reasoning: Too heavyweight for simple reasoning</li> <li>Custom Signature per Use: Too much boilerplate</li> </ul>"},{"location":"architecture/decisions/#future-considerations","title":"Future Considerations","text":"<ol> <li>Streaming support: StreamingChainOfThought for incremental reasoning</li> <li>Few-shot examples: Add example reasoning patterns to improve quality</li> <li>Verification: Automatic reasoning quality checks</li> <li>Caching: Built-in caching for repeated queries</li> </ol>"},{"location":"architecture/decisions/#migration-guide_1","title":"Migration Guide","text":"<p>Feature is additive - no migration needed.</p>"},{"location":"architecture/decisions/#adr-004-human-in-the-loop-with-interruptible-decorator","title":"ADR-004: Human-in-the-Loop with Interruptible Decorator","text":"<p>Date: 2025-01-25</p> <p>Status: Accepted</p>"},{"location":"architecture/decisions/#context_3","title":"Context","text":"<p>Many agent applications require human approval for certain actions (e.g., deleting files, sending emails, making purchases). We needed a clean way to suspend execution, ask for user input, and resume where we left off.</p>"},{"location":"architecture/decisions/#decision_3","title":"Decision","text":"<p>Implement an <code>@interruptible</code> decorator that: - Suspends function execution before it runs - Raises <code>HumanInTheLoopRequired</code> exception with context - Allows resumption with user approval/rejection/modifications - Uses thread-safe <code>contextvars</code> for state management - Generates stable interrupt IDs based on function + arguments</p> <pre><code>from udspy import interruptible, HumanInTheLoopRequired, set_interrupt_approval\n\n@interruptible\ndef delete_file(path: str) -&gt; str:\n    os.remove(path)\n    return f\"Deleted {path}\"\n\ntry:\n    delete_file(\"/important.txt\")\nexcept HumanInTheLoopRequired as e:\n    # User approves\n    set_interrupt_approval(e.interrupt_id, approved=True)\n    delete_file(\"/important.txt\")  # Now executes\n</code></pre>"},{"location":"architecture/decisions/#implementation-details_1","title":"Implementation Details","text":"<ol> <li>Stable Interrupt IDs: Generated from <code>function_name:hash(args)</code> to allow same call to resume</li> <li>Context Variables: Thread-safe and async-safe state storage</li> <li>Rejection Support: <code>InterruptRejected</code> exception distinguishes \"user said no\" from \"pending\"</li> <li>Argument Modification: Users can edit arguments before approval</li> <li>Automatic Cleanup: Interrupts are cleared after successful execution</li> </ol>"},{"location":"architecture/decisions/#key-features_2","title":"Key Features","text":"<ol> <li>Decorator-based: Simple to apply to any function</li> <li>Thread-safe: Works with concurrent requests</li> <li>Async-safe: Works with asyncio tasks</li> <li>Resumable: Same function call can be resumed after approval</li> <li>Integrated with Tools: Works seamlessly with <code>@tool</code> decorator</li> </ol>"},{"location":"architecture/decisions/#use-cases_2","title":"Use Cases","text":"<ol> <li>Dangerous Operations: File deletion, system commands</li> <li>User Confirmation: Sending emails, making purchases</li> <li>Clarification: Ask user for additional information</li> <li>Argument Editing: Let user modify parameters before execution</li> </ol>"},{"location":"architecture/decisions/#consequences_3","title":"Consequences","text":"<p>Benefits: - Clean separation of business logic from approval logic - Works naturally with ReAct agent workflows - Thread-safe and async-safe out of the box - Easy to test (deterministic based on interrupt state)</p> <p>Trade-offs: - Requires exception handling (but this is explicit and clear) - Interrupt state needs to be managed (cleared on success) - Not suitable for purely synchronous, single-threaded apps (but works fine there too)</p>"},{"location":"architecture/decisions/#alternatives-considered_3","title":"Alternatives Considered","text":"<ul> <li>Callback-based: More complex, harder to reason about flow</li> <li>Middleware pattern: Too heavyweight for this use case</li> <li>Manual state management: Error-prone, not thread-safe</li> </ul>"},{"location":"architecture/decisions/#migration-guide_2","title":"Migration Guide","text":"<p>Feature is additive - no migration needed.</p>"},{"location":"architecture/decisions/#adr-005-react-agent-module","title":"ADR-005: ReAct Agent Module","text":"<p>Date: 2025-01-25</p> <p>Status: Accepted</p>"},{"location":"architecture/decisions/#context_4","title":"Context","text":"<p>The ReAct (Reasoning + Acting) pattern combines chain-of-thought reasoning with tool usage in an iterative loop. This is essential for building agents that can solve complex tasks by breaking them down and using tools.</p>"},{"location":"architecture/decisions/#decision_4","title":"Decision","text":"<p>Implement a <code>ReAct</code> module that: - Alternates between reasoning and tool execution - Supports human-in-the-loop for clarifications and confirmations - Tracks full trajectory of reasoning and actions - Handles errors gracefully with retries - Works with both streaming and non-streaming modes</p> <pre><code>from udspy import ReAct, InputField, OutputField, Signature, tool\n\n@tool(name=\"search\")\ndef search(query: str) -&gt; str:\n    return search_api(query)\n\nclass ResearchTask(Signature):\n    \"\"\"Research and answer questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\nagent = ReAct(ResearchTask, tools=[search], max_iters=5)\nresult = agent(question=\"What is the population of Tokyo?\")\n</code></pre>"},{"location":"architecture/decisions/#implementation-approach_1","title":"Implementation Approach","text":"<ol> <li>Iterative Loop: Continues until final answer or max iterations</li> <li>Dynamic Signature: Extends signature with reasoning_N, tool_name_N, tool_args_N fields</li> <li>Tool Execution: Automatically executes tools and adds results to context</li> <li>Error Handling: Retries with error feedback if tool execution fails</li> <li>Human Interrupts: Integrates with <code>@interruptible</code> for user input</li> </ol>"},{"location":"architecture/decisions/#key-features_3","title":"Key Features","text":"<ol> <li>Flexible Tool Usage: Agent decides when and which tools to use</li> <li>Self-Correction: Can retry if tool execution fails</li> <li>Trajectory Tracking: Full history of reasoning and actions</li> <li>Streaming Support: Can stream reasoning in real-time</li> <li>Human-in-the-Loop: Built-in support for asking users</li> </ol>"},{"location":"architecture/decisions/#research-evidence_1","title":"Research Evidence","text":"<p>ReAct improves performance on: - Complex Tasks: 15-30% improvement on multi-step reasoning (Yao et al., 2023) - Tool Usage: More accurate tool selection vs. pure CoT - Error Recovery: Better handling of failed tool calls</p>"},{"location":"architecture/decisions/#use-cases_3","title":"Use Cases","text":"<ol> <li>Research Agents: Answer questions using search and APIs</li> <li>Task Automation: Multi-step workflows with tool usage</li> <li>Data Analysis: Fetch data, analyze, and summarize</li> <li>Interactive Assistants: Ask users for clarification when needed</li> </ol>"},{"location":"architecture/decisions/#consequences_4","title":"Consequences","text":"<p>Benefits: - Powerful agent capabilities with minimal code - Transparent reasoning process - Handles complex multi-step tasks - Built-in error handling and retries</p> <p>Trade-offs: - Higher token usage due to multiple iterations - Slower than single-shot predictions - Quality depends on LLM's reasoning ability - Can get stuck in loops if not properly configured</p>"},{"location":"architecture/decisions/#comparison-with-dspy_1","title":"Comparison with DSPy","text":"Aspect udspy DSPy API <code>ReAct(signature, tools=[...])</code> <code>dspy.ReAct(signature, tools=[...])</code> Human-in-Loop Built-in with <code>@interruptible</code> External handling Streaming Supported Limited Tool Execution Automatic with error handling Automatic Max Iterations Configurable with <code>max_iters</code> Configurable"},{"location":"architecture/decisions/#alternatives-considered_4","title":"Alternatives Considered","text":"<ul> <li>Chain-based approach: Too rigid, hard to add dynamic behavior</li> <li>State machine: Overly complex for the use case</li> <li>Pure prompting: Less reliable than structured approach</li> </ul>"},{"location":"architecture/decisions/#future-considerations_1","title":"Future Considerations","text":"<ol> <li>Memory/History: Long-term memory across sessions</li> <li>Tool Chaining: Automatic sequencing of tool calls</li> <li>Parallel Tool Execution: Execute independent tools concurrently</li> <li>Learning: Optimize tool selection based on feedback</li> </ol>"},{"location":"architecture/decisions/#migration-guide_3","title":"Migration Guide","text":"<p>Feature is additive - no migration needed.</p>"},{"location":"architecture/decisions/#adr-006-unified-module-execution-pattern-aexecute","title":"ADR-006: Unified Module Execution Pattern (aexecute)","text":"<p>Date: 2025-01-25</p> <p>Status: Accepted</p>"},{"location":"architecture/decisions/#context_5","title":"Context","text":"<p>Initially, <code>astream()</code> and <code>aforward()</code> had duplicated logic for executing modules. This made maintenance difficult and increased the chance of bugs when updating behavior.</p>"},{"location":"architecture/decisions/#decision_5","title":"Decision","text":"<p>Introduce a single <code>aexecute()</code> method that handles both streaming and non-streaming execution:</p> <pre><code>class Module:\n    async def aexecute(self, *, stream: bool = False, **inputs):\n        \"\"\"Core execution logic - handles both streaming and non-streaming.\"\"\"\n        # Implementation here\n\n    async def astream(self, **inputs):\n        \"\"\"Public streaming API.\"\"\"\n        async for event in self.aexecute(stream=True, **inputs):\n            yield event\n\n    async def aforward(self, **inputs):\n        \"\"\"Public non-streaming API.\"\"\"\n        async for event in self.aexecute(stream=False, **inputs):\n            if isinstance(event, Prediction):\n                return event\n</code></pre>"},{"location":"architecture/decisions/#implementation-details_2","title":"Implementation Details","text":"<ol> <li>Single Source of Truth: All execution logic in <code>aexecute()</code></li> <li>Stream Parameter: Boolean flag controls behavior</li> <li>Generator Pattern: Always yields events, even in non-streaming mode</li> <li>Clean Separation: Public methods are thin wrappers</li> </ol>"},{"location":"architecture/decisions/#key-benefits","title":"Key Benefits","text":"<ol> <li>No Duplication: Write logic once, use in both modes</li> <li>Easier Testing: Test one method instead of two</li> <li>Consistent Behavior: Streaming and non-streaming guaranteed to behave identically</li> <li>Maintainable: Changes only need to be made in one place</li> <li>Extensible: Easy to add new execution modes</li> </ol>"},{"location":"architecture/decisions/#consequences_5","title":"Consequences","text":"<p>Benefits: - Reduced code duplication (~40% less code in modules) - Easier to maintain and debug - Consistent behavior across modes - Simpler to understand (one execution path)</p> <p>Trade-offs: - Slightly more complex to implement initially - Need to handle both streaming and non-streaming cases in same method - Generator pattern requires understanding of async generators</p>"},{"location":"architecture/decisions/#before-and-after","title":"Before and After","text":"<p>Before: <pre><code>async def astream(self, **inputs):\n    # 100 lines of logic\n    ...\n\nasync def aforward(self, **inputs):\n    # 100 lines of DUPLICATED logic with minor differences\n    ...\n</code></pre></p> <p>After: <pre><code>async def aexecute(self, *, stream: bool, **inputs):\n    # 100 lines of logic (used by both)\n    ...\n\nasync def astream(self, **inputs):\n    async for event in self.aexecute(stream=True, **inputs):\n        yield event\n\nasync def aforward(self, **inputs):\n    async for event in self.aexecute(stream=False, **inputs):\n        if isinstance(event, Prediction):\n            return event\n</code></pre></p>"},{"location":"architecture/decisions/#naming-rationale","title":"Naming Rationale","text":"<p>We chose <code>aexecute()</code> (without underscore prefix) because: - Public Method: This is the main extension point for subclasses - Clear Intent: \"Execute\" is explicit about what it does - Python Conventions: No underscore = public API, expected to be overridden - Not Abbreviated: Full word avoids ambiguity (vs <code>aexec</code> or <code>acall</code>)</p>"},{"location":"architecture/decisions/#migration-guide_4","title":"Migration Guide","text":"<p>For Users: No changes needed - public API remains the same</p> <p>For Module Authors: When creating custom modules, implement <code>aexecute()</code> instead of both <code>astream()</code> and <code>aforward()</code>.</p>"},{"location":"architecture/decisions/#additional-design-decisions","title":"Additional Design Decisions","text":""},{"location":"architecture/decisions/#field-markers-for-parsing","title":"Field Markers for Parsing","text":"<p>Decision: Use <code>[[ ## field_name ## ]]</code> markers to delineate fields in completions.</p> <p>Rationale: - Simple, regex-parseable format - Clear visual separation - Consistent with DSPy's approach (proven) - Fallback when native tools aren't available</p> <p>Trade-offs: - Requires careful prompt engineering - LLM might not always respect markers - Uses extra tokens</p>"},{"location":"architecture/decisions/#see-also","title":"See Also","text":"<ul> <li>CLAUDE.md - Chronological architectural changes (development log)</li> <li>Architecture Overview - Component relationships</li> <li>Contributing Guide - How to propose new decisions</li> </ul>"},{"location":"architecture/decisions/#template-for-future-adrs","title":"Template for Future ADRs","text":"<p>When adding new architectural decisions, use this template:</p>"},{"location":"architecture/decisions/#adr-xxx-decision-title","title":"ADR-XXX: Decision Title","text":"<p>Date: YYYY-MM-DD</p> <p>Status: Proposed | Accepted | Deprecated | Superseded</p>"},{"location":"architecture/decisions/#context_6","title":"Context","text":"<p>Why was this change needed? What problem does it solve?</p>"},{"location":"architecture/decisions/#decision_6","title":"Decision","text":"<p>What was decided and implemented? Include code examples if relevant.</p>"},{"location":"architecture/decisions/#implementation-details_3","title":"Implementation Details","text":"<p>How is this implemented? Key technical details.</p>"},{"location":"architecture/decisions/#consequences_6","title":"Consequences","text":"<p>Benefits: - What are the advantages?</p> <p>Trade-offs: - What are the disadvantages or limitations?</p>"},{"location":"architecture/decisions/#alternatives-considered_5","title":"Alternatives Considered","text":"<ul> <li>What other approaches were considered?</li> <li>Why were they rejected?</li> </ul>"},{"location":"architecture/decisions/#migration-guide-if-applicable","title":"Migration Guide (if applicable)","text":"<p>How should users update their code?</p>"},{"location":"architecture/modules/","title":"Modules","text":"<p>Modules are composable units that encapsulate LLM calls. They provide a standard interface for building complex LLM-powered applications.</p>"},{"location":"architecture/modules/#overview","title":"Overview","text":"<p>All modules inherit from the base <code>Module</code> class and implement a unified execution pattern. This allows modules to be composed, nested, and combined to create sophisticated behaviors.</p>"},{"location":"architecture/modules/#core-concepts","title":"Core Concepts","text":""},{"location":"architecture/modules/#unified-interface","title":"Unified Interface","text":"<p>Every module implements: - <code>aexecute(*, stream: bool, **inputs)</code> - Core async execution - <code>aforward(**inputs)</code> - Async convenience (no streaming) - <code>__call__(**inputs)</code> - Synchronous wrapper</p>"},{"location":"architecture/modules/#composition","title":"Composition","text":"<p>Modules can contain other modules:</p> <pre><code>class Pipeline(Module):\n    def __init__(self):\n        self.analyze = Predict(\"text -&gt; analysis\")\n        self.summarize = ChainOfThought(\"text, analysis -&gt; summary\")\n\n    async def aexecute(self, *, stream: bool = False, **inputs):\n        analysis = await self.analyze.aforward(text=inputs[\"text\"])\n        result = self.summarize.aexecute(\n            stream=stream,\n            text=inputs[\"text\"],\n            analysis=analysis.analysis\n        )\n        async for event in result:\n            yield event\n</code></pre>"},{"location":"architecture/modules/#streaming-support","title":"Streaming Support","text":"<p>All modules support streaming for real-time output:</p> <pre><code>async for event in module.aexecute(stream=True, **inputs):\n    if isinstance(event, StreamChunk):\n        print(event.delta, end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        print(f\"\\nFinal: {event}\")\n</code></pre>"},{"location":"architecture/modules/#built-in-modules","title":"Built-in Modules","text":"<p>udspy provides three core modules:</p>"},{"location":"architecture/modules/#base-module","title":"Base Module","text":"<p>The foundation for all modules. Provides: - Unified execution interface - Streaming infrastructure - Async-first design - Composition support</p> <p>When to use: When creating custom modules</p>"},{"location":"architecture/modules/#predict","title":"Predict","text":"<p>The core module for LLM predictions. Features: - Maps signature inputs to outputs - Native tool calling support - Conversation history management - Streaming and async execution</p> <p>When to use: For basic LLM calls, tool usage, and as a building block for other modules</p> <pre><code>predictor = Predict(\"question -&gt; answer\")\nresult = predictor(question=\"What is AI?\")\n</code></pre>"},{"location":"architecture/modules/#chainofthought","title":"ChainOfThought","text":"<p>Adds step-by-step reasoning before outputs. Features: - Automatic reasoning field injection - Improves answer quality on complex tasks - Transparent reasoning process - Works with any signature</p> <p>When to use: For tasks requiring reasoning (math, analysis, decision-making)</p> <pre><code>cot = ChainOfThought(\"question -&gt; answer\")\nresult = cot(question=\"What is 157 * 234?\")\nprint(result.reasoning)  # Shows step-by-step work\nprint(result.answer)     # \"36738\"\n</code></pre>"},{"location":"architecture/modules/#react","title":"ReAct","text":"<p>Agent that reasons and acts with tools. Features: - Iterative reasoning and tool usage - Human-in-the-loop support - Built-in ask_to_user and finish tools - Full trajectory tracking</p> <p>When to use: For tasks requiring multiple steps, tool usage, or agent-like behavior</p> <pre><code>@tool(name=\"search\")\ndef search(query: str = Field(...)) -&gt; str:\n    return search_web(query)\n\nagent = ReAct(\"question -&gt; answer\", tools=[search])\nresult = agent(question=\"What's the weather in Tokyo?\")\n</code></pre>"},{"location":"architecture/modules/#module-comparison","title":"Module Comparison","text":"Feature Predict ChainOfThought ReAct Basic LLM calls \u2705 \u2705 \u2705 Step-by-step reasoning \u274c \u2705 \u2705 Tool usage \u2705 \u2705 \u2705 Multi-step iteration \u274c \u274c \u2705 Human-in-the-loop \u274c \u274c \u2705 Trajectory tracking \u274c \u274c \u2705 Complexity Low Low Medium Token usage Low Medium High Latency Low Medium High"},{"location":"architecture/modules/#creating-custom-modules","title":"Creating Custom Modules","text":"<p>To create a custom module:</p> <ol> <li>Subclass <code>Module</code></li> <li>Implement <code>aexecute()</code> method</li> <li>Yield <code>StreamEvent</code> objects during execution</li> <li>Yield final <code>Prediction</code> at the end</li> </ol> <pre><code>from udspy import Module, Prediction, StreamChunk\n\nclass CustomModule(Module):\n    def __init__(self, signature):\n        self.predictor = Predict(signature)\n\n    async def aexecute(self, *, stream: bool = False, **inputs):\n        # Custom logic before prediction\n        processed_inputs = preprocess(inputs)\n\n        # Optionally yield streaming events\n        if stream:\n            yield StreamChunk(field=\"status\", delta=\"Processing...\")\n\n        # Run prediction\n        result = await self.predictor.aforward(**processed_inputs)\n\n        # Custom logic after prediction\n        final_result = postprocess(result)\n\n        # Always yield final prediction\n        yield Prediction(**final_result)\n</code></pre> <p>See Base Module for detailed guidance.</p>"},{"location":"architecture/modules/#best-practices","title":"Best Practices","text":""},{"location":"architecture/modules/#choose-the-right-module","title":"Choose the Right Module","text":"<ul> <li>Simple tasks: Use <code>Predict</code></li> <li>Need reasoning: Use <code>ChainOfThought</code></li> <li>Need tools/agents: Use <code>ReAct</code></li> <li>Custom logic: Create custom module</li> </ul>"},{"location":"architecture/modules/#composition-over-inheritance","title":"Composition over Inheritance","text":"<p>Build complex behaviors by composing modules rather than deep inheritance:</p> <pre><code># Good: Composition\nclass Pipeline(Module):\n    def __init__(self):\n        self.step1 = Predict(sig1)\n        self.step2 = ChainOfThought(sig2)\n\n# Avoid: Deep inheritance\nclass MyComplexModule(ChainOfThought):\n    # Complex overrides\n</code></pre>"},{"location":"architecture/modules/#async-best-practices","title":"Async Best Practices","text":"<ul> <li>Use <code>aforward()</code> when you don't need streaming</li> <li>Use <code>aexecute(stream=True)</code> for real-time output</li> <li>Use <code>__call__()</code> only in sync contexts</li> <li>Always <code>await</code> async operations</li> </ul>"},{"location":"architecture/modules/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    result = await module.aforward(**inputs)\nexcept HumanInTheLoopRequired as e:\n    # Handle interrupts\n    result = await module.aresume(user_input, e)\nexcept Exception as e:\n    # Handle other errors\n    logger.error(f\"Module failed: {e}\")\n</code></pre>"},{"location":"architecture/modules/#see-also","title":"See Also","text":"<ul> <li>Base Module - Module foundation</li> <li>Predict Module - Core prediction</li> <li>ChainOfThought Module - Step-by-step reasoning</li> <li>ReAct Module - Agent with tools</li> <li>Signatures - Define inputs/outputs</li> <li>Streaming - Real-time output</li> <li>API: Modules - Full API reference</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>udspy consists of four main components that work together to provide a clean abstraction for LLM interactions.</p>"},{"location":"architecture/overview/#core-components","title":"Core Components","text":""},{"location":"architecture/overview/#1-signatures","title":"1. Signatures","text":"<p>Signatures define the input/output contract for an LLM task using Pydantic models.</p> <pre><code>class QA(Signature):\n    \"\"\"Answer questions concisely.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n</code></pre> <p>Key responsibilities: - Define input and output fields with type information - Provide field descriptions for prompt construction - Validate data at runtime using Pydantic</p> <p>Learn more about Signatures \u2192</p>"},{"location":"architecture/overview/#2-adapters","title":"2. Adapters","text":"<p>Adapters handle the formatting of signatures into LLM-specific message formats and parsing responses back.</p> <pre><code>adapter = ChatAdapter()\ninstructions = adapter.format_instructions(signature)\nformatted_input = adapter.format_inputs(signature, inputs)\noutputs = adapter.parse_outputs(signature, completion)\n</code></pre> <p>Key responsibilities: - Convert signature definitions to system prompts - Format input values into user messages - Parse LLM completions into structured outputs - Convert Pydantic models to OpenAI tool schemas</p> <p>Learn more about Adapters \u2192</p>"},{"location":"architecture/overview/#3-modules","title":"3. Modules","text":"<p>Modules are composable units that encapsulate LLM calls. The core module is <code>Predict</code>.</p> <pre><code>predictor = Predict(signature, model=\"gpt-4o-mini\")\nresult = predictor(question=\"What is AI?\")\n</code></pre> <p>Key responsibilities: - Manage signature, model, and configuration - Orchestrate adapter formatting and API calls - Return structured <code>Prediction</code> objects - Support composition and reuse</p> <p>Learn more about Modules \u2192</p>"},{"location":"architecture/overview/#4-streaming","title":"4. Streaming","text":"<p>Streaming support allows incremental output processing for better UX.</p> <pre><code>predictor = StreamingPredict(signature)\nasync for chunk in predictor.stream(question=\"Explain AI\"):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <p>Key responsibilities: - Process streaming API responses incrementally - Detect field boundaries in streams - Emit field-specific chunks - Provide final parsed prediction</p> <p>Learn more about Streaming \u2192</p>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":"<pre><code>User Input \u2192 Signature \u2192 Adapter \u2192 OpenAI API \u2192 Adapter \u2192 Prediction \u2192 User\n              \u2193           \u2193                        \u2191\n           Validate    Format                   Parse\n</code></pre> <ol> <li>User provides input matching signature's input fields</li> <li>Signature validates input types</li> <li>Adapter formats signature + inputs into messages</li> <li>OpenAI API generates completion</li> <li>Adapter parses completion into structured outputs</li> <li>User receives <code>Prediction</code> with typed outputs</li> </ol>"},{"location":"architecture/overview/#design-principles","title":"Design Principles","text":""},{"location":"architecture/overview/#native-tool-calling","title":"Native Tool Calling","text":"<p>Unlike DSPy which uses custom field markers in prompts, udspy uses OpenAI's native function calling:</p> <pre><code>class Calculator(BaseModel):\n    \"\"\"Perform arithmetic.\"\"\"\n    operation: str\n    a: float\n    b: float\n\npredictor = Predict(signature, tools=[Calculator])\n</code></pre> <p>This provides: - Better performance (optimized by OpenAI) - More reliable parsing - Cleaner prompts - Forward compatibility</p>"},{"location":"architecture/overview/#minimal-abstractions","title":"Minimal Abstractions","text":"<p>Every component has a clear, focused responsibility:</p> <ul> <li>Signatures: Define I/O contracts</li> <li>Adapters: Handle format translation</li> <li>Modules: Encapsulate LLM calls</li> <li>Streaming: Process incremental outputs</li> </ul> <p>No hidden magic, no over-engineering.</p>"},{"location":"architecture/overview/#type-safety","title":"Type Safety","text":"<p>Pydantic provides runtime type checking throughout:</p> <pre><code>class QA(Signature):\n    question: str = InputField()\n    answer: int = OutputField()  # Will validate output is int\n\npredictor = Predict(QA)\nresult = predictor(question=\"What is 2+2?\")\nassert isinstance(result.answer, int)\n</code></pre>"},{"location":"architecture/overview/#configuration","title":"Configuration","text":"<p>Global configuration via <code>settings</code>:</p> <pre><code>import udspy\n\nudspy.settings.configure(\n    api_key=\"sk-...\",\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n)\n</code></pre> <p>Per-module overrides:</p> <pre><code>predictor = Predict(\n    signature,\n    model=\"gpt-4\",\n    temperature=0.0,\n)\n</code></pre>"},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Deep dive into Signatures</li> <li>Understand Adapters</li> <li>Explore Modules</li> <li>Learn about Streaming</li> </ul>"},{"location":"architecture/signatures/","title":"Signatures","text":"<p>Signatures define the input/output contract for LLM tasks using Pydantic models.</p>"},{"location":"architecture/signatures/#creating-signatures","title":"Creating Signatures","text":"<p>There are three ways to create signatures in udspy:</p>"},{"location":"architecture/signatures/#1-string-signatures-quick-simple","title":"1. String Signatures (Quick &amp; Simple)","text":"<p>For rapid prototyping, use the DSPy-style string format:</p> <pre><code>from udspy import Signature\n\n# Simple signature\nQA = Signature.from_string(\"question -&gt; answer\")\n\n# Multiple inputs and outputs\nAnalyze = Signature.from_string(\n    \"context, question -&gt; summary, answer\",\n    \"Analyze text and answer questions\"\n)\n</code></pre> <p>Format: <code>\"input1, input2 -&gt; output1, output2\"</code></p> <ul> <li>All fields default to <code>str</code> type</li> <li>Optional second argument for instructions</li> <li>Great for quick prototyping</li> </ul>"},{"location":"architecture/signatures/#2-class-based-signatures-full-control","title":"2. Class-based Signatures (Full Control)","text":"<p>For production code, use class-based signatures:</p> <pre><code>from udspy import Signature, InputField, OutputField\n\nclass QA(Signature):\n    \"\"\"Answer questions concisely and accurately.\"\"\"\n    question: str = InputField(description=\"Question to answer\")\n    answer: str = OutputField(description=\"Concise answer\")\n</code></pre> <p>Benefits: - Custom field types - Field descriptions for better prompts - IDE autocomplete and type checking - Better for complex signatures</p>"},{"location":"architecture/signatures/#3-dynamic-signatures-programmatic","title":"3. Dynamic Signatures (Programmatic)","text":"<p>For runtime signature creation:</p> <pre><code>from udspy import make_signature\n\nQA = make_signature(\n    input_fields={\"question\": str},\n    output_fields={\"answer\": str},\n    instructions=\"Answer questions concisely\",\n)\n</code></pre>"},{"location":"architecture/signatures/#components","title":"Components","text":""},{"location":"architecture/signatures/#docstring","title":"Docstring","text":"<p>The class docstring becomes the task instruction in the system prompt:</p> <pre><code>class Summarize(Signature):\n    \"\"\"Summarize the given text in 2-3 sentences.\"\"\"\n    text: str = InputField()\n    summary: str = OutputField()\n</code></pre>"},{"location":"architecture/signatures/#inputfield","title":"InputField","text":"<p>Marks a field as an input:</p> <pre><code>question: str = InputField(\n    description=\"Question to answer\",  # Used in prompt\n    default=\"\",  # Optional default value\n)\n</code></pre>"},{"location":"architecture/signatures/#outputfield","title":"OutputField","text":"<p>Marks a field as an output:</p> <pre><code>answer: str = OutputField(\n    description=\"Concise answer\",\n)\n</code></pre>"},{"location":"architecture/signatures/#field-types","title":"Field Types","text":"<p>Signatures support various field types:</p>"},{"location":"architecture/signatures/#primitives","title":"Primitives","text":"<pre><code>class Example(Signature):\n    \"\"\"Example signature.\"\"\"\n    text: str = InputField()\n    count: int = InputField()\n    score: float = InputField()\n    enabled: bool = InputField()\n</code></pre>"},{"location":"architecture/signatures/#collections","title":"Collections","text":"<pre><code>class Example(Signature):\n    \"\"\"Example signature.\"\"\"\n    tags: list[str] = InputField()\n    metadata: dict[str, Any] = InputField()\n</code></pre>"},{"location":"architecture/signatures/#pydantic-models","title":"Pydantic Models","text":"<pre><code>from pydantic import BaseModel\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\nclass Example(Signature):\n    \"\"\"Example signature.\"\"\"\n    person: Person = InputField()\n    related: list[Person] = OutputField()\n</code></pre>"},{"location":"architecture/signatures/#validation","title":"Validation","text":"<p>Signatures use Pydantic for validation:</p> <pre><code>class Sentiment(Signature):\n    \"\"\"Analyze sentiment.\"\"\"\n    text: str = InputField()\n    sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = OutputField()\n\n# Output will be validated to match literal values\n</code></pre>"},{"location":"architecture/signatures/#multi-output-signatures","title":"Multi-Output Signatures","text":"<p>Signatures can have multiple outputs:</p> <pre><code>class ReasonedQA(Signature):\n    \"\"\"Answer with step-by-step reasoning.\"\"\"\n    question: str = InputField()\n    reasoning: str = OutputField(description=\"Reasoning process\")\n    answer: str = OutputField(description=\"Final answer\")\n</code></pre>"},{"location":"architecture/signatures/#best-practices","title":"Best Practices","text":""},{"location":"architecture/signatures/#1-clear-descriptions","title":"1. Clear Descriptions","text":"<pre><code># Good\nquestion: str = InputField(description=\"User's question about the product\")\n\n# Bad\nquestion: str = InputField()\n</code></pre>"},{"location":"architecture/signatures/#2-specific-instructions","title":"2. Specific Instructions","text":"<pre><code># Good\nclass Summarize(Signature):\n    \"\"\"Summarize in exactly 3 bullet points, each under 20 words.\"\"\"\n\n# Bad\nclass Summarize(Signature):\n    \"\"\"Summarize.\"\"\"\n</code></pre>"},{"location":"architecture/signatures/#3-structured-outputs","title":"3. Structured Outputs","text":"<pre><code># Good - use Pydantic models for complex outputs\nclass Analysis(BaseModel):\n    sentiment: str\n    confidence: float\n    keywords: list[str]\n\nclass Analyze(Signature):\n    \"\"\"Analyze text.\"\"\"\n    text: str = InputField()\n    analysis: Analysis = OutputField()\n\n# Bad - use many separate fields\nclass Analyze(Signature):\n    \"\"\"Analyze text.\"\"\"\n    text: str = InputField()\n    sentiment: str = OutputField()\n    confidence: float = OutputField()\n    keywords: list[str] = OutputField()\n</code></pre>"},{"location":"architecture/signatures/#choosing-the-right-approach","title":"Choosing the Right Approach","text":""},{"location":"architecture/signatures/#string-signatures","title":"String Signatures","text":"<p>Use when: - Prototyping quickly - All fields are strings - Signature is simple - You don't need field descriptions</p> <pre><code># Perfect for quick tests\npredictor = Predict(\"question -&gt; answer\")\n</code></pre>"},{"location":"architecture/signatures/#class-based-signatures","title":"Class-based Signatures","text":"<p>Use when: - Building production code - You need custom types - You want field descriptions - Signature is complex</p> <pre><code>class QA(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField(description=\"User's question\")\n    answer: str = OutputField(description=\"Concise answer\")\n</code></pre>"},{"location":"architecture/signatures/#dynamic-signatures","title":"Dynamic Signatures","text":"<p>Use when: - Creating signatures at runtime - Building signature builders/factories - Signature structure depends on config</p> <pre><code># Build signature based on config\nfields = load_field_config()\nMySignature = make_signature(fields[\"inputs\"], fields[\"outputs\"])\n</code></pre>"},{"location":"architecture/signatures/#modules-accept-all-formats","title":"Modules Accept All Formats","text":"<p>All modules automatically recognize string signatures:</p> <pre><code>from udspy import Predict, ChainOfThought, ReAct\n\n# All of these work:\npredictor1 = Predict(\"question -&gt; answer\")\npredictor2 = Predict(QA)  # Class-based\npredictor3 = Predict(make_signature(...))  # Dynamic\n\ncot = ChainOfThought(\"question -&gt; answer\")\nagent = ReAct(\"question -&gt; answer\", tools=[...])\n</code></pre>"},{"location":"architecture/signatures/#api-reference","title":"API Reference","text":"<p>See API: Signatures for detailed API documentation including the full <code>Signature.from_string()</code> reference.</p>"},{"location":"architecture/streaming/","title":"Streaming","text":"<p>Streaming support allows incremental processing of LLM outputs, providing real-time feedback to users.</p>"},{"location":"architecture/streaming/#overview","title":"Overview","text":"<p>All modules support streaming out of the box through the <code>astream()</code> method:</p> <pre><code>from udspy import Predict, StreamChunk, Prediction\n\npredictor = Predict(\"question -&gt; answer\")\n\nasync for event in predictor.astream(question=\"Explain AI\"):\n    if isinstance(event, StreamChunk):\n        print(event.delta, end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        print(f\"\\n\\nFinal result: {event.answer}\")\n</code></pre>"},{"location":"architecture/streaming/#stream-events","title":"Stream Events","text":"<p>The streaming API yields two types of events:</p>"},{"location":"architecture/streaming/#streamchunk","title":"StreamChunk","text":"<p>Incremental text updates for a specific field:</p> <pre><code>class StreamChunk(StreamEvent):\n    module: Module          # Module that generated this chunk\n    field_name: str        # Which output field (e.g., \"answer\")\n    delta: str             # New text since last chunk\n    content: str           # Full accumulated text so far\n    is_complete: bool      # Whether field is done streaming\n</code></pre> <p>Example: <pre><code>async for event in predictor.astream(question=\"...\"):\n    if isinstance(event, StreamChunk):\n        print(f\"[{event.field_name}] {event.delta}\", end=\"\", flush=True)\n        if event.is_complete:\n            print(f\"\\n--- {event.field_name} complete ---\")\n</code></pre></p>"},{"location":"architecture/streaming/#prediction","title":"Prediction","text":"<p>Final result with all output fields:</p> <pre><code>class Prediction(StreamEvent, dict):\n    # Dict with all output fields\n    # Supports both dict and attribute access\n</code></pre> <p>Example: <pre><code>async for event in predictor.astream(question=\"...\"):\n    if isinstance(event, Prediction):\n        print(f\"Answer: {event.answer}\")\n        print(f\"Same: {event['answer']}\")\n</code></pre></p>"},{"location":"architecture/streaming/#field-specific-streaming","title":"Field-Specific Streaming","text":"<p>Streaming automatically handles multiple output fields:</p> <pre><code>from udspy import ChainOfThought\n\ncot = ChainOfThought(\"question -&gt; answer\")\n\nasync for event in cot.astream(question=\"What is 157 * 234?\"):\n    if isinstance(event, StreamChunk):\n        if event.field_name == \"reasoning\":\n            print(f\"\ud83d\udcad {event.delta}\", end=\"\", flush=True)\n        elif event.field_name == \"answer\":\n            print(f\"\\n\u2713 {event.delta}\", end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        print(f\"\\n\\nComplete!\")\n</code></pre>"},{"location":"architecture/streaming/#custom-stream-events","title":"Custom Stream Events","text":"<p>You can emit custom events from tools or callbacks:</p> <pre><code>from dataclasses import dataclass\nfrom udspy.streaming import StreamEvent, emit_event\n\n@dataclass\nclass ToolProgress(StreamEvent):\n    tool_name: str\n    message: str\n    progress: float  # 0.0 to 1.0\n\n# In your tool:\nfrom udspy import tool\nfrom pydantic import Field\n\n@tool(name=\"search\")\nasync def search(query: str = Field(...)) -&gt; str:\n    await emit_event(ToolProgress(\"search\", \"Starting search...\", 0.0))\n\n    results = await search_api(query)\n\n    await emit_event(ToolProgress(\"search\", \"Processing results...\", 0.5))\n\n    processed = process_results(results)\n\n    await emit_event(ToolProgress(\"search\", \"Complete!\", 1.0))\n\n    return processed\n\n# In the stream consumer:\nasync for event in predictor.astream(question=\"...\"):\n    if isinstance(event, ToolProgress):\n        print(f\"[{event.tool_name}] {event.message} ({event.progress*100:.0f}%)\")\n    elif isinstance(event, StreamChunk):\n        print(event.delta, end=\"\", flush=True)\n</code></pre>"},{"location":"architecture/streaming/#module-support","title":"Module Support","text":"<p>All built-in modules support streaming:</p>"},{"location":"architecture/streaming/#predict","title":"Predict","text":"<pre><code>predictor = Predict(\"question -&gt; answer\")\nasync for event in predictor.astream(question=\"...\"):\n    ...\n</code></pre>"},{"location":"architecture/streaming/#chainofthought","title":"ChainOfThought","text":"<p>Streams both reasoning and answer:</p> <pre><code>cot = ChainOfThought(\"question -&gt; answer\")\nasync for event in cot.astream(question=\"...\"):\n    if isinstance(event, StreamChunk):\n        if event.field_name == \"reasoning\":\n            # Reasoning streams first\n            ...\n        elif event.field_name == \"answer\":\n            # Answer streams second\n            ...\n</code></pre>"},{"location":"architecture/streaming/#react","title":"ReAct","text":"<p>Streams reasoning and tool interactions:</p> <pre><code>from udspy import ReAct\n\nagent = ReAct(\"question -&gt; answer\", tools=[search])\nasync for event in agent.astream(question=\"...\"):\n    if isinstance(event, StreamChunk):\n        if event.field_name == \"reasoning\":\n            print(f\"\ud83d\udcad {event.delta}\", end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        print(f\"\\n\\n\u2713 {event.answer}\")\n</code></pre> <p>See <code>examples/react_streaming.py</code> for a complete example.</p>"},{"location":"architecture/streaming/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/streaming/#context-variables","title":"Context Variables","text":"<p>Streaming uses Python's <code>contextvars</code> for thread-safe event queuing:</p> <pre><code>from udspy.streaming import _stream_queue, emit_event\n\n# Internal: stream queue is set when aexecute(stream=True) is called\n# emit_event() checks if a queue exists and puts events there\n</code></pre> <p>This allows tools and nested modules to emit events without explicit queue passing.</p>"},{"location":"architecture/streaming/#event-flow","title":"Event Flow","text":"<ol> <li>Module's <code>astream(**inputs)</code> is called</li> <li>Queue is created and set in context</li> <li>Internal <code>aexecute(stream=True)</code> is called</li> <li>Module yields <code>StreamChunk</code> events as text arrives</li> <li>Tools can call <code>emit_event()</code> to inject custom events</li> <li>Module yields final <code>Prediction</code> when complete</li> <li>Queue is cleaned up</li> </ol>"},{"location":"architecture/streaming/#non-streaming-mode","title":"Non-Streaming Mode","text":"<p>For non-streaming execution, use <code>aforward()</code> instead of <code>astream()</code>:</p> <pre><code># Streaming: iterate over events\nasync for event in predictor.astream(question=\"...\"):\n    if isinstance(event, Prediction):\n        result = event\n\n# Non-streaming: get final result directly\nresult = await predictor.aforward(question=\"...\")\n</code></pre>"},{"location":"architecture/streaming/#best-practices","title":"Best Practices","text":""},{"location":"architecture/streaming/#1-always-handle-both-event-types","title":"1. Always Handle Both Event Types","text":"<pre><code>async for event in module.astream(**inputs):\n    match event:\n        case StreamChunk():\n            # Handle streaming text\n            print(event.delta, end=\"\", flush=True)\n        case Prediction():\n            # Handle final result\n            final_result = event\n</code></pre>"},{"location":"architecture/streaming/#2-check-field-names-for-multi-field-outputs","title":"2. Check Field Names for Multi-field Outputs","text":"<pre><code>async for event in module.astream(**inputs):\n    if isinstance(event, StreamChunk):\n        if event.field_name == \"reasoning\":\n            # Different formatting for reasoning\n            print(f\"\ud83d\udcad {event.delta}\", end=\"\")\n        elif event.field_name == \"answer\":\n            # Different formatting for answer\n            print(f\"\u2713 {event.delta}\", end=\"\")\n</code></pre>"},{"location":"architecture/streaming/#3-use-custom-events-for-progress","title":"3. Use Custom Events for Progress","text":"<pre><code>@dataclass\nclass Progress(StreamEvent):\n    step: str\n    percent: float\n\nasync def long_running_tool():\n    await emit_event(Progress(\"Loading data\", 0.3))\n    data = load_data()\n\n    await emit_event(Progress(\"Processing\", 0.6))\n    result = process(data)\n\n    await emit_event(Progress(\"Complete\", 1.0))\n    return result\n</code></pre>"},{"location":"architecture/streaming/#4-accumulate-chunks-for-display","title":"4. Accumulate Chunks for Display","text":"<pre><code>accumulated = {}\n\nasync for event in module.astream(**inputs):\n    if isinstance(event, StreamChunk):\n        field = event.field_name\n        if field not in accumulated:\n            accumulated[field] = \"\"\n        accumulated[field] += event.delta\n\n        # Update UI with accumulated[field]\n        update_display(field, accumulated[field])\n</code></pre>"},{"location":"architecture/streaming/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/streaming/#latency","title":"Latency","text":"<p>Streaming reduces perceived latency by showing results immediately:</p> <ul> <li>Non-streaming: Wait for full response (~5s), then show all text</li> <li>Streaming: Start showing text after ~500ms, continue as it arrives</li> </ul>"},{"location":"architecture/streaming/#token-usage","title":"Token Usage","text":"<p>Streaming doesn't affect token usage - same number of tokens are generated.</p>"},{"location":"architecture/streaming/#error-handling","title":"Error Handling","text":"<p>Errors can occur mid-stream:</p> <pre><code>try:\n    async for event in module.astream(**inputs):\n        if isinstance(event, StreamChunk):\n            print(event.delta, end=\"\", flush=True)\nexcept Exception as e:\n    print(f\"\\n\\nError during streaming: {e}\")\n</code></pre>"},{"location":"architecture/streaming/#see-also","title":"See Also","text":"<ul> <li>API: Streaming - Full API reference</li> <li>Examples: Streaming - Complete examples</li> <li>Examples: ReAct Streaming - Agent streaming</li> <li>Base Module - Module execution patterns</li> </ul>"},{"location":"architecture/modules/base/","title":"Base Module","text":"<p>The <code>Module</code> class is the foundation for all udspy modules. It provides a standard interface for composable LLM components.</p>"},{"location":"architecture/modules/base/#purpose","title":"Purpose","text":"<p>The base module serves several key purposes:</p> <ol> <li>Unified Interface: All modules implement the same execution methods (<code>aexecute</code>, <code>aforward</code>, <code>__call__</code>)</li> <li>Composition: Modules can be nested and composed to build complex behaviors</li> <li>Streaming Support: Built-in streaming infrastructure for real-time outputs</li> <li>Async-First: Native async/await support for efficient I/O operations</li> </ol>"},{"location":"architecture/modules/base/#core-methods","title":"Core Methods","text":""},{"location":"architecture/modules/base/#aexecute-stream-bool-false-inputs","title":"<code>aexecute(*, stream: bool = False, **inputs)</code>","text":"<p>The core execution method that all modules must implement. This is the public API for module execution.</p> <ul> <li>stream: If <code>True</code>, enables streaming mode for real-time output</li> <li>inputs: Keyword arguments matching the module's signature input fields</li> <li>Returns: <code>AsyncGenerator[StreamEvent, None]</code> that yields events and ends with a <code>Prediction</code></li> </ul> <pre><code>class CustomModule(Module):\n    async def aexecute(self, *, stream: bool = False, **inputs):\n        # Implementation here\n        ...\n        yield Prediction(result=final_result)\n</code></pre>"},{"location":"architecture/modules/base/#aforwardinputs","title":"<code>aforward(**inputs)</code>","text":"<p>Convenience method that calls <code>aexecute(stream=False)</code> and returns just the final <code>Prediction</code>.</p> <pre><code>result = await module.aforward(question=\"What is Python?\")\nprint(result.answer)\n</code></pre>"},{"location":"architecture/modules/base/#__call__inputs","title":"<code>__call__(**inputs)</code>","text":"<p>Synchronous wrapper that runs <code>aforward</code> and returns the result. This is the most convenient way to use modules in synchronous code.</p> <pre><code>result = module(question=\"What is Python?\")\nprint(result.answer)\n</code></pre>"},{"location":"architecture/modules/base/#streaming-architecture","title":"Streaming Architecture","text":"<p>Modules support streaming through an async generator pattern:</p> <pre><code>async for event in module.aexecute(stream=True, question=\"Explain AI\"):\n    if isinstance(event, StreamChunk):\n        print(event.delta, end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        print(f\"\\nFinal: {event.answer}\")\n</code></pre> <p>The streaming system yields: - <code>StreamChunk</code> events during generation (with <code>field</code> and <code>delta</code>) - A final <code>Prediction</code> object with complete results</p>"},{"location":"architecture/modules/base/#module-composition","title":"Module Composition","text":"<p>Modules can contain other modules, creating powerful compositions:</p> <pre><code>from udspy import Module, Predict, ChainOfThought\n\nclass Pipeline(Module):\n    def __init__(self):\n        self.analyzer = Predict(\"text -&gt; analysis\")\n        self.summarizer = ChainOfThought(\"text, analysis -&gt; summary\")\n\n    async def aexecute(self, *, stream: bool = False, **inputs):\n        # First module\n        analysis = await self.analyzer.aforward(text=inputs[\"text\"])\n\n        # Second module uses first module's output\n        result = await self.summarizer.aexecute(\n            stream=stream,\n            text=inputs[\"text\"],\n            analysis=analysis.analysis\n        )\n\n        # Yield the final result\n        async for event in result:\n            yield event\n</code></pre>"},{"location":"architecture/modules/base/#design-rationale","title":"Design Rationale","text":""},{"location":"architecture/modules/base/#why-aexecute-instead-of-_aexecute","title":"Why <code>aexecute</code> instead of <code>_aexecute</code>?","text":"<p>The method is named <code>aexecute</code> (public) rather than <code>_aexecute</code> (private) because:</p> <ol> <li>It's the public API: Modules are meant to be executed via this method</li> <li>Subclasses override it: Marking it private would be confusing since it's meant to be overridden</li> <li>Consistency: Follows Python conventions where overridable methods are public</li> </ol> <p>See ADR-006 for detailed rationale.</p>"},{"location":"architecture/modules/base/#async-first-design","title":"Async-First Design","text":"<p>All modules are async-first because:</p> <ol> <li>I/O Bound: LLM calls are network I/O operations</li> <li>Concurrent Operations: Multiple LLM calls can run in parallel</li> <li>Streaming: Async generators are ideal for streaming responses</li> <li>Modern Python: Async/await is the standard for I/O-bound operations</li> </ol> <p>The synchronous <code>__call__</code> wrapper provides convenience but internally uses async operations.</p>"},{"location":"architecture/modules/base/#built-in-modules","title":"Built-in Modules","text":"<p>udspy provides several built-in modules:</p> <ul> <li>Predict: Core module for LLM predictions</li> <li>ChainOfThought: Adds reasoning before outputs</li> <li>ReAct: Reasoning and acting with tools</li> </ul>"},{"location":"architecture/modules/base/#creating-custom-modules","title":"Creating Custom Modules","text":"<p>To create a custom module:</p> <ol> <li>Subclass <code>Module</code></li> <li>Implement <code>aexecute()</code> method</li> <li>Yield <code>StreamEvent</code> objects during execution</li> <li>Yield final <code>Prediction</code> at the end</li> </ol> <pre><code>from udspy import Module, Prediction\n\nclass CustomModule(Module):\n    async def aexecute(self, *, stream: bool = False, **inputs):\n        # Your logic here\n        result = process_inputs(inputs)\n\n        # Return final prediction\n        yield Prediction(output=result)\n</code></pre>"},{"location":"architecture/modules/base/#see-also","title":"See Also","text":"<ul> <li>Predict Module - The core prediction module</li> <li>ChainOfThought Module - Step-by-step reasoning</li> <li>ReAct Module - Agent with tool usage</li> <li>ADR-006: Unified Execution Pattern</li> </ul>"},{"location":"architecture/modules/chain_of_thought/","title":"Chain of Thought","text":"<p>Chain of Thought (CoT) is a prompting technique that improves reasoning by explicitly requesting step-by-step thinking.</p>"},{"location":"architecture/modules/chain_of_thought/#overview","title":"Overview","text":"<p>The <code>ChainOfThought</code> module is a wrapper around <code>Predict</code> that automatically adds a \"reasoning\" field to any signature:</p> <pre><code>class QA(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\n# ChainOfThought extends the signature:\n# question -&gt; reasoning, answer\ncot = ChainOfThought(QA)\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#how-it-works","title":"How It Works","text":""},{"location":"architecture/modules/chain_of_thought/#1-signature-extension","title":"1. Signature Extension","text":"<p>ChainOfThought takes the original signature and creates an extended version with a reasoning field:</p> <pre><code># Original signature\nquestion: str -&gt; answer: str\n\n# Extended signature (automatically)\nquestion: str -&gt; reasoning: str, answer: str\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#2-implementation","title":"2. Implementation","text":"<pre><code>class ChainOfThought(Module):\n    def __init__(self, signature, **kwargs):\n        # Extract input and output fields\n        input_fields = signature.get_input_fields()\n        output_fields = signature.get_output_fields()\n\n        # Prepend reasoning to outputs\n        extended_outputs = {\"reasoning\": str, **output_fields}\n\n        # Create new signature\n        extended_signature = make_signature(\n            input_fields,\n            extended_outputs,\n            signature.get_instructions()\n        )\n\n        # Use Predict with extended signature\n        self.predict = Predict(extended_signature, **kwargs)\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#3-prompt-engineering","title":"3. Prompt Engineering","text":"<p>The reasoning field encourages step-by-step thinking through:</p> <ol> <li>Field description: \"Step-by-step reasoning process\" (customizable)</li> <li>Field ordering: Reasoning comes before the answer</li> <li>Output format: Uses field markers to structure the response</li> </ol> <p>Example prompt structure: <pre><code>[System]\nAnswer questions with clear reasoning.\n\nRequired Outputs:\n- reasoning: Step-by-step reasoning process\n- answer: Final answer\n\n[User]\n[[ ## question ## ]]\nWhat is 15 * 23?\n\n[Assistant]\n[[ ## reasoning ## ]]\nLet me calculate: 15 * 23 = 15 * 20 + 15 * 3 = 300 + 45 = 345\n\n[[ ## answer ## ]]\n345\n</code></pre></p>"},{"location":"architecture/modules/chain_of_thought/#benefits","title":"Benefits","text":""},{"location":"architecture/modules/chain_of_thought/#improved-accuracy","title":"Improved Accuracy","text":"<p>Chain of Thought improves accuracy on:</p> <ul> <li>Math problems: 67% \u2192 92% accuracy (typical improvement)</li> <li>Logic puzzles: Forces explicit reasoning steps</li> <li>Multi-step tasks: Prevents skipping intermediate steps</li> <li>Complex analysis: Organizes thinking</li> </ul>"},{"location":"architecture/modules/chain_of_thought/#transparency","title":"Transparency","text":"<p>Shows the reasoning process:</p> <pre><code>result = cot(question=\"Is 17 prime?\")\n\nprint(result.reasoning)\n# \"To check if 17 is prime, I need to test divisibility\n#  by all primes up to \u221a17 \u2248 4.12.\n#  Testing: 17 \u00f7 2 = 8.5 (not divisible)\n#          17 \u00f7 3 = 5.67 (not divisible)\n#  No divisors found, so 17 is prime.\"\n\nprint(result.answer)\n# \"Yes, 17 is prime\"\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#debugging","title":"Debugging","text":"<p>Easier to identify issues:</p> <pre><code>result = cot(question=\"What is 2^10?\")\n\nif \"1024\" not in result.answer:\n    # Check reasoning to see where it went wrong\n    print(\"Error in reasoning:\", result.reasoning)\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#customization","title":"Customization","text":""},{"location":"architecture/modules/chain_of_thought/#custom-reasoning-description","title":"Custom Reasoning Description","text":"<pre><code>cot = ChainOfThought(\n    signature,\n    reasoning_description=\"Detailed mathematical proof with all steps\"\n)\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#model-parameters","title":"Model Parameters","text":"<pre><code># Deterministic reasoning for math\ncot = ChainOfThought(QA, temperature=0.0)\n\n# Creative reasoning for analysis\ncot = ChainOfThought(Analysis, temperature=0.7)\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#multiple-outputs","title":"Multiple Outputs","text":"<p>Works seamlessly with multiple output fields:</p> <pre><code>class ComplexTask(Signature):\n    \"\"\"Complex task.\"\"\"\n    input: str = InputField()\n    analysis: str = OutputField()\n    recommendation: str = OutputField()\n    confidence: float = OutputField()\n\ncot = ChainOfThought(ComplexTask)\nresult = cot(input=\"...\")\n\n# All outputs available\nresult.reasoning       # Added automatically\nresult.analysis        # Original output\nresult.recommendation  # Original output\nresult.confidence      # Original output\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#comparison-with-dspy","title":"Comparison with DSPy","text":""},{"location":"architecture/modules/chain_of_thought/#similarities","title":"Similarities","text":"<ul> <li>Same concept: adds reasoning field to signature</li> <li>Improves accuracy through explicit reasoning</li> <li>Transparent reasoning process</li> </ul>"},{"location":"architecture/modules/chain_of_thought/#differences","title":"Differences","text":"Feature udspy DSPy Implementation Signature extension Signature prepend method Customization reasoning_description rationale_field parameter Complexity ~45 lines ~40 lines Dependencies make_signature signature.prepend()"},{"location":"architecture/modules/chain_of_thought/#udspy-approach","title":"udspy Approach","text":"<pre><code># Simpler API\ncot = ChainOfThought(QA)\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#dspy-approach","title":"DSPy Approach","text":"<pre><code># More flexible but complex\ncot = dspy.ChainOfThought(\n    \"question -&gt; answer\",\n    rationale_field=dspy.OutputField(\n        prefix=\"Reasoning: Let's think step by step\",\n        desc=\"${reasoning}\"\n    )\n)\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#when-to-use","title":"When to Use","text":""},{"location":"architecture/modules/chain_of_thought/#good-use-cases","title":"Good Use Cases \u2713","text":"<ul> <li>Math problems requiring calculation</li> <li>Logic puzzles and reasoning tasks</li> <li>Multi-step analysis or planning</li> <li>Tasks where you want to verify reasoning</li> <li>Educational applications (show work)</li> <li>High-stakes decisions requiring justification</li> </ul>"},{"location":"architecture/modules/chain_of_thought/#less-useful","title":"Less Useful \u2717","text":"<ul> <li>Simple factual recall (\"What is the capital of France?\")</li> <li>Binary classification without reasoning</li> <li>Very short outputs where reasoning overhead is large</li> <li>Real-time systems with strict latency requirements</li> </ul>"},{"location":"architecture/modules/chain_of_thought/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/modules/chain_of_thought/#token-usage","title":"Token Usage","text":"<p>Chain of Thought uses more tokens:</p> <pre><code># Without CoT: ~50 tokens\nresult = predict(question=\"What is 2+2?\")\n# answer: \"4\"\n\n# With CoT: ~150 tokens\nresult = cot(question=\"What is 2+2?\")\n# reasoning: \"This is basic arithmetic. 2+2 = 4\"\n# answer: \"4\"\n</code></pre> <p>Trade-off: Higher cost/latency for better accuracy and transparency.</p>"},{"location":"architecture/modules/chain_of_thought/#optimization","title":"Optimization","text":"<p>For cost-sensitive applications:</p> <pre><code># Use CoT only for complex queries\nif is_complex(question):\n    result = cot(question=question)\nelse:\n    result = simple_predict(question=question)\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#best-practices","title":"Best Practices","text":"<ol> <li>Use appropriate temperature</li> <li><code>0.0</code> for math/logic (deterministic)</li> <li> <p><code>0.3-0.7</code> for analysis/planning</p> </li> <li> <p>Customize reasoning description for domain-specific tasks    <pre><code>ChainOfThought(\n    MedicalDiagnosis,\n    reasoning_description=\"Clinical reasoning with differential diagnosis\"\n)\n</code></pre></p> </li> <li> <p>Validate reasoning quality in production    <pre><code>if len(result.reasoning) &lt; 100:\n    logger.warning(\"Reasoning too brief\")\n</code></pre></p> </li> <li> <p>Cache for repeated queries to save costs    <pre><code>@lru_cache(maxsize=100)\ndef cached_cot(question):\n    return cot(question=question)\n</code></pre></p> </li> </ol> <p>See Examples for more details.</p>"},{"location":"architecture/modules/predict/","title":"Predict Module","text":"<p>The <code>Predict</code> module is the foundational building block of udspy. It takes a signature and generates outputs from inputs using an LLM.</p>"},{"location":"architecture/modules/predict/#overview","title":"Overview","text":"<p><code>Predict</code> is the simplest and most essential module in udspy. It:</p> <ul> <li>Maps signature inputs to outputs via an LLM</li> <li>Supports native tool calling for function execution</li> <li>Handles streaming for real-time output generation</li> <li>Manages conversation history and message formatting</li> </ul>"},{"location":"architecture/modules/predict/#basic-usage","title":"Basic Usage","text":"<pre><code>from udspy import Predict, Signature, InputField, OutputField\n\nclass QA(Signature):\n    \"\"\"Answer questions concisely.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\nresult = predictor(question=\"What is Python?\")\nprint(result.answer)\n</code></pre>"},{"location":"architecture/modules/predict/#string-signatures","title":"String Signatures","text":"<p>For quick prototyping, use string signatures:</p> <pre><code>predictor = Predict(\"question -&gt; answer\")\nresult = predictor(question=\"What is Python?\")\n</code></pre> <p>See Signatures for more details.</p>"},{"location":"architecture/modules/predict/#configuration","title":"Configuration","text":""},{"location":"architecture/modules/predict/#model-selection","title":"Model Selection","text":"<pre><code># Global default\nimport udspy\nudspy.settings.configure(model=\"gpt-4o-mini\")\n\n# Per-module override\npredictor = Predict(QA, model=\"gpt-4o\")\n</code></pre>"},{"location":"architecture/modules/predict/#temperature-and-sampling","title":"Temperature and Sampling","text":"<pre><code>predictor = Predict(\n    QA,\n    temperature=0.7,\n    max_tokens=1000,\n    top_p=0.9,\n)\n</code></pre>"},{"location":"architecture/modules/predict/#custom-adapter","title":"Custom Adapter","text":"<pre><code>from udspy import ChatAdapter\n\nadapter = ChatAdapter()\npredictor = Predict(QA, adapter=adapter)\n</code></pre>"},{"location":"architecture/modules/predict/#tool-calling","title":"Tool Calling","text":"<p>Predict supports native OpenAI tool calling:</p> <pre><code>from udspy import tool\nfrom pydantic import Field\n\n@tool(name=\"search\", description=\"Search for information\")\ndef search(query: str = Field(...)) -&gt; str:\n    return f\"Results for: {query}\"\n\npredictor = Predict(QA, tools=[search])\nresult = predictor(question=\"What is the weather in Tokyo?\")\n\n# Access tool calls\nfor call in result.tool_calls:\n    print(f\"Called {call['name']} with {call['arguments']}\")\n</code></pre>"},{"location":"architecture/modules/predict/#auto-execution-vs-manual","title":"Auto-execution vs Manual","text":"<p>By default, tools are NOT auto-executed. You control execution:</p> <pre><code># Auto-execute tools\nresult = await predictor.aexecute(\n    question=\"Search for Python\",\n    auto_execute_tools=True\n)\n\n# Manual execution\nresult = await predictor.aexecute(\n    question=\"Search for Python\",\n    auto_execute_tools=False\n)\n# Tools are returned in result.tool_calls but not executed\n</code></pre> <p>See Tool Calling for more details.</p>"},{"location":"architecture/modules/predict/#streaming","title":"Streaming","text":"<p>Stream outputs in real-time:</p> <pre><code>async for event in predictor.aexecute(\n    stream=True,\n    question=\"Explain quantum computing\"\n):\n    if isinstance(event, StreamChunk):\n        print(event.delta, end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        print(f\"\\n\\nFinal: {event.answer}\")\n</code></pre>"},{"location":"architecture/modules/predict/#stream-events","title":"Stream Events","text":"<p>The streaming API yields: - <code>StreamChunk(field: str, delta: str)</code> - Incremental text for a field - <code>Prediction(**outputs)</code> - Final result with all fields</p> <pre><code>from udspy import StreamChunk, Prediction\n\nasync for event in predictor.aexecute(stream=True, **inputs):\n    match event:\n        case StreamChunk(field=field, delta=delta):\n            print(f\"[{field}] {delta}\", end=\"\")\n        case Prediction() as pred:\n            print(f\"\\n\\nComplete: {pred}\")\n</code></pre>"},{"location":"architecture/modules/predict/#execution-methods","title":"Execution Methods","text":""},{"location":"architecture/modules/predict/#async-execution","title":"Async Execution","text":"<pre><code># With streaming\nasync for event in predictor.aexecute(stream=True, question=\"...\"):\n    ...\n\n# Without streaming\nresult = await predictor.aforward(question=\"...\")\n</code></pre>"},{"location":"architecture/modules/predict/#synchronous-execution","title":"Synchronous Execution","text":"<pre><code># Synchronous wrapper\nresult = predictor(question=\"...\")\n</code></pre> <p>Internally, the synchronous call runs async code using <code>asyncio.run()</code> or the current event loop.</p>"},{"location":"architecture/modules/predict/#history-management","title":"History Management","text":"<p>Predict automatically manages conversation history:</p> <pre><code>predictor = Predict(QA)\n\n# First call\nresult1 = predictor(question=\"What is Python?\")\n\n# Second call - includes history\nresult2 = predictor(question=\"What are its key features?\")\n\n# Access history\nfor msg in predictor.history:\n    print(f\"{msg.role}: {msg.content}\")\n\n# Clear history\npredictor.history.clear()\n</code></pre> <p>See History API for more details.</p>"},{"location":"architecture/modules/predict/#under-the-hood","title":"Under the Hood","text":""},{"location":"architecture/modules/predict/#message-flow","title":"Message Flow","text":"<ol> <li>Signature \u2192 Messages: Adapter converts signature to system prompt</li> <li>Inputs \u2192 User Message: Input fields become user message</li> <li>LLM Call: OpenAI API generates response</li> <li>Response \u2192 Outputs: Parse response into output fields</li> <li>History Update: Add messages to history</li> </ol>"},{"location":"architecture/modules/predict/#field-parsing","title":"Field Parsing","text":"<p>Outputs are parsed from the LLM response:</p> <ul> <li>JSON Mode: If response is valid JSON, parse it</li> <li>Field Markers: Look for field boundaries like <code>answer: ...</code></li> <li>Fallback: Extract text content</li> </ul> <p>The <code>Prediction</code> object makes all output fields accessible as attributes:</p> <pre><code>result = predictor(question=\"...\")\nprint(result.answer)  # Access via attribute\nprint(result[\"answer\"])  # Access via dict key\n</code></pre>"},{"location":"architecture/modules/predict/#design-rationale","title":"Design Rationale","text":""},{"location":"architecture/modules/predict/#why-not-auto-execute-tools-by-default","title":"Why Not Auto-execute Tools by Default?","text":"<p>Tools are not auto-executed by default because:</p> <ol> <li>Safety: User should control when external functions run</li> <li>Flexibility: Sometimes you want to inspect/modify tool calls</li> <li>Explicit is better: Makes behavior clear and predictable</li> </ol>"},{"location":"architecture/modules/predict/#why-native-tool-calling","title":"Why Native Tool Calling?","text":"<p>Unlike DSPy which uses custom adapters, udspy uses OpenAI's native function calling:</p> <ol> <li>Simplicity: Less code to maintain</li> <li>Performance: Optimized by OpenAI</li> <li>Reliability: Well-tested and production-ready</li> <li>Features: Access to latest tool calling improvements</li> </ol> <p>See ADR-001: Native Tool Calling (if available) for full rationale.</p>"},{"location":"architecture/modules/predict/#common-patterns","title":"Common Patterns","text":""},{"location":"architecture/modules/predict/#simple-qa","title":"Simple Q&amp;A","text":"<pre><code>qa = Predict(\"question -&gt; answer\")\nresult = qa(question=\"What is AI?\")\n</code></pre>"},{"location":"architecture/modules/predict/#multi-field-output","title":"Multi-field Output","text":"<pre><code>analyzer = Predict(\"text -&gt; summary, sentiment, keywords\")\nresult = analyzer(text=\"I love this product!\")\n</code></pre>"},{"location":"architecture/modules/predict/#with-context","title":"With Context","text":"<pre><code>contextual = Predict(\"context, question -&gt; answer\")\nresult = contextual(\n    context=\"Python is a programming language\",\n    question=\"What is it used for?\"\n)\n</code></pre>"},{"location":"architecture/modules/predict/#with-tools","title":"With Tools","text":"<pre><code>@tool(name=\"calculator\")\ndef calc(expression: str = Field(...)) -&gt; str:\n    return str(eval(expression))\n\nmath_solver = Predict(\"problem -&gt; solution\", tools=[calc])\nresult = await math_solver.aexecute(\n    problem=\"What is 157 * 234?\",\n    auto_execute_tools=True\n)\n</code></pre>"},{"location":"architecture/modules/predict/#see-also","title":"See Also","text":"<ul> <li>Base Module - Module foundation</li> <li>ChainOfThought - Add reasoning</li> <li>ReAct - Agent with tool usage</li> <li>Signatures - Define inputs/outputs</li> <li>Tool Calling - Function calling API</li> <li>Streaming - Real-time output</li> </ul>"},{"location":"architecture/modules/react/","title":"ReAct Module","text":"<p>The <code>ReAct</code> (Reasoning and Acting) module implements an agent that iteratively reasons about tasks and uses tools to accomplish goals.</p>"},{"location":"architecture/modules/react/#overview","title":"Overview","text":"<p>ReAct combines:</p> <ul> <li>Reasoning: Step-by-step thinking about what to do next</li> <li>Acting: Calling tools to perform actions</li> <li>Iteration: Repeating until the task is complete</li> </ul> <p>This creates an agent that can break down complex tasks, use available tools, and ask for help when needed.</p>"},{"location":"architecture/modules/react/#basic-usage","title":"Basic Usage","text":"<pre><code>from udspy import ReAct, Signature, InputField, OutputField, tool\nfrom pydantic import Field\n\n@tool(name=\"search\", description=\"Search for information\")\ndef search(query: str = Field(...)) -&gt; str:\n    return f\"Results for: {query}\"\n\nclass QA(Signature):\n    \"\"\"Answer questions using available tools.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\nagent = ReAct(QA, tools=[search])\nresult = agent(question=\"What is the weather in Tokyo?\")\n\nprint(result.answer)\nprint(result.trajectory)  # Full reasoning history\n</code></pre>"},{"location":"architecture/modules/react/#string-signatures","title":"String Signatures","text":"<p>For quick prototyping:</p> <pre><code>agent = ReAct(\"question -&gt; answer\", tools=[search])\nresult = agent(question=\"What is Python?\")\n</code></pre>"},{"location":"architecture/modules/react/#how-react-works","title":"How ReAct Works","text":""},{"location":"architecture/modules/react/#iteration-loop","title":"Iteration Loop","text":"<ol> <li>Reason: Agent thinks about current situation</li> <li>Act: Agent calls a tool (or finish)</li> <li>Observe: Agent sees tool result</li> <li>Repeat: Until agent calls <code>finish</code> tool or max iterations</li> </ol>"},{"location":"architecture/modules/react/#built-in-tools","title":"Built-in Tools","text":"<p>ReAct automatically provides:</p> <ul> <li>finish: Call when task is complete</li> <li>ask_to_user: Ask user for clarification (if enabled)</li> </ul> <pre><code># The agent automatically has these tools available:\n# - finish(answer: str) - Complete the task\n# - ask_to_user(question: str) - Ask user for help\n</code></pre>"},{"location":"architecture/modules/react/#trajectory","title":"Trajectory","text":"<p>The trajectory records every step:</p> <pre><code>result = agent(question=\"Calculate 15 * 23\")\n\n# Access trajectory\nprint(result.trajectory)\n# {\n#   \"reasoning_0\": \"I need to calculate 15 * 23\",\n#   \"tool_name_0\": \"calculator\",\n#   \"tool_args_0\": {\"expression\": \"15 * 23\"},\n#   \"observation_0\": \"345\",\n#   \"reasoning_1\": \"I have the answer\",\n#   \"tool_name_1\": \"finish\",\n#   ...\n# }\n</code></pre>"},{"location":"architecture/modules/react/#configuration","title":"Configuration","text":""},{"location":"architecture/modules/react/#maximum-iterations","title":"Maximum Iterations","text":"<pre><code>agent = ReAct(QA, tools=[search], max_iters=10)\nresult = agent(question=\"...\", max_iters=5)  # Override per call\n</code></pre>"},{"location":"architecture/modules/react/#disable-ask-to-user","title":"Disable Ask-to-User","text":"<pre><code>agent = ReAct(QA, tools=[search], enable_ask_to_user=False)\n</code></pre>"},{"location":"architecture/modules/react/#human-in-the-loop","title":"Human-in-the-Loop","text":"<p>ReAct supports interruptible tools that require human confirmation:</p> <pre><code>from udspy import HumanInTheLoopRequired, tool\n\n@tool(name=\"delete_file\", interruptible=True)\ndef delete_file(path: str = Field(...)) -&gt; str:\n    return f\"Deleted {path}\"\n\nagent = ReAct(QA, tools=[delete_file])\n\ntry:\n    result = await agent.aforward(question=\"Delete /tmp/test.txt\")\nexcept HumanInTheLoopRequired as e:\n    print(f\"Confirm: {e.question}\")\n    print(f\"Tool: {e.tool_call.name}\")\n    print(f\"Args: {e.tool_call.args}\")\n\n    # User approves\n    result = await agent.aresume(\"yes\", e)\n\n    # Or user rejects\n    result = await agent.aresume(\"no\", e)\n\n    # Or user modifies arguments\n    result = await agent.aresume('{\"path\": \"/tmp/other.txt\"}', e)\n</code></pre>"},{"location":"architecture/modules/react/#resumption-flow","title":"Resumption Flow","text":"<p>When an interrupt occurs:</p> <ol> <li>Agent pauses and raises <code>HumanInTheLoopRequired</code></li> <li>Exception contains saved state and pending tool call</li> <li>User reviews and responds</li> <li>Call <code>aresume(response, saved_state)</code> to continue</li> </ol> <p>See Interrupt API for details.</p>"},{"location":"architecture/modules/react/#streaming","title":"Streaming","text":"<p>Stream the agent's reasoning in real-time:</p> <pre><code>async for event in agent.aexecute(\n    stream=True,\n    question=\"What is quantum computing?\"\n):\n    if isinstance(event, StreamChunk):\n        if event.field == \"reasoning\":\n            print(f\"Thinking: {event.delta}\", end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        print(f\"\\n\\nAnswer: {event.answer}\")\n</code></pre> <p>See <code>examples/react_streaming.py</code> for a complete example.</p>"},{"location":"architecture/modules/react/#architecture","title":"Architecture","text":""},{"location":"architecture/modules/react/#internal-signatures","title":"Internal Signatures","text":"<p>ReAct uses two internal signatures:</p> <ol> <li>react_signature: For reasoning and tool selection</li> <li>Inputs: Original inputs + trajectory</li> <li>Outputs: reasoning</li> <li> <p>Tools: All provided tools + finish</p> </li> <li> <p>extract_signature: For extracting final answer</p> </li> <li>Inputs: Original inputs + trajectory</li> <li>Outputs: Original outputs</li> <li>Uses ChainOfThought for extraction</li> </ol>"},{"location":"architecture/modules/react/#modules","title":"Modules","text":"<p>ReAct composes two modules:</p> <ul> <li><code>react_module</code>: Predict with tools for reasoning/acting</li> <li><code>extract_module</code>: ChainOfThought for final answer extraction</li> </ul>"},{"location":"architecture/modules/react/#example-flow","title":"Example Flow","text":"<pre><code>User: \"What is the capital of France?\"\n\nIteration 0:\n  Reasoning: \"I need to search for France's capital\"\n  Tool: search\n  Args: {\"query\": \"capital of France\"}\n  Observation: \"Paris is the capital of France\"\n\nIteration 1:\n  Reasoning: \"I have the answer, I can finish\"\n  Tool: finish\n  Args: {}\n  Observation: \"Task completed\"\n\nExtract:\n  Reasoning: \"Based on the search, Paris is the capital\"\n  Answer: \"Paris\"\n</code></pre>"},{"location":"architecture/modules/react/#advanced-usage","title":"Advanced Usage","text":""},{"location":"architecture/modules/react/#custom-tools","title":"Custom Tools","text":"<pre><code>from pydantic import Field\n\n@tool(\n    name=\"calculator\",\n    description=\"Evaluate mathematical expressions\"\n)\ndef calc(expression: str = Field(description=\"Math expression\")) -&gt; str:\n    return str(eval(expression))\n\n@tool(\n    name=\"web_search\",\n    description=\"Search the web for information\"\n)\nasync def web_search(query: str = Field(...)) -&gt; str:\n    # Async tools are supported\n    return await search_api(query)\n\nagent = ReAct(QA, tools=[calc, web_search])\n</code></pre>"},{"location":"architecture/modules/react/#multiple-outputs","title":"Multiple Outputs","text":"<pre><code>class Research(Signature):\n    \"\"\"Research a topic thoroughly.\"\"\"\n    topic: str = InputField()\n    summary: str = OutputField()\n    sources: str = OutputField()\n    confidence: str = OutputField()\n\nagent = ReAct(Research, tools=[search])\nresult = agent(topic=\"Quantum Computing\")\n\nprint(result.summary)\nprint(result.sources)\nprint(result.confidence)\n</code></pre>"},{"location":"architecture/modules/react/#tool-error-handling","title":"Tool Error Handling","text":"<p>Tools can raise exceptions - they're caught and added to observations:</p> <pre><code>@tool(name=\"divide\")\ndef divide(a: int = Field(...), b: int = Field(...)) -&gt; str:\n    return str(a / b)\n\nagent = ReAct(QA, tools=[divide])\nresult = agent(question=\"What is 10 divided by 0?\")\n\n# Agent sees: \"Error executing divide: division by zero\"\n# Agent can reason about the error and try alternative approaches\n</code></pre>"},{"location":"architecture/modules/react/#design-rationale","title":"Design Rationale","text":""},{"location":"architecture/modules/react/#why-two-phases-react-extract","title":"Why Two Phases (React + Extract)?","text":"<ol> <li>react_module: Focuses on tool usage and reasoning</li> <li>extract_module: Focuses on clean output formatting</li> </ol> <p>This separation ensures: - Tool-using prompts stay focused on actions - Final outputs are well-formatted - Trajectory doesn't pollute final answer</p>"},{"location":"architecture/modules/react/#why-ask_to_user-tool","title":"Why ask_to_user Tool?","text":"<p>The built-in <code>ask_to_user</code> tool allows agents to: - Request clarification when ambiguous - Ask for additional information - Interact naturally with users</p> <p>It's implemented as an interruptible tool, so users can provide responses that the agent incorporates into its reasoning.</p>"},{"location":"architecture/modules/react/#why-finish-tool","title":"Why finish Tool?","text":"<p>The <code>finish</code> tool signals task completion: - Explicit end condition (vs implicit max iterations) - Agent decides when it has enough information - More natural than counting iterations</p>"},{"location":"architecture/modules/react/#common-patterns","title":"Common Patterns","text":""},{"location":"architecture/modules/react/#research-agent","title":"Research Agent","text":"<pre><code>@tool(name=\"search\")\ndef search(query: str = Field(...)) -&gt; str:\n    return search_web(query)\n\n@tool(name=\"summarize\")\ndef summarize(text: str = Field(...)) -&gt; str:\n    return llm_summarize(text)\n\nresearcher = ReAct(\n    \"topic -&gt; summary, sources\",\n    tools=[search, summarize]\n)\nresult = researcher(topic=\"AI Safety\")\n</code></pre>"},{"location":"architecture/modules/react/#task-automation","title":"Task Automation","text":"<pre><code>@tool(name=\"read_file\")\ndef read_file(path: str = Field(...)) -&gt; str:\n    return open(path).read()\n\n@tool(name=\"write_file\", interruptible=True)\ndef write_file(path: str = Field(...), content: str = Field(...)) -&gt; str:\n    with open(path, 'w') as f:\n        f.write(content)\n    return f\"Wrote to {path}\"\n\nassistant = ReAct(\n    \"task -&gt; result\",\n    tools=[read_file, write_file]\n)\n</code></pre>"},{"location":"architecture/modules/react/#multi-tool-problem-solving","title":"Multi-tool Problem Solving","text":"<pre><code>@tool(name=\"calculator\")\ndef calc(expr: str = Field(...)) -&gt; str:\n    return str(eval(expr))\n\n@tool(name=\"unit_converter\")\ndef convert(value: float = Field(...), from_unit: str = Field(...), to_unit: str = Field(...)) -&gt; str:\n    # Conversion logic\n    return f\"{result} {to_unit}\"\n\nsolver = ReAct(\n    \"problem -&gt; solution\",\n    tools=[calc, convert]\n)\nresult = solver(problem=\"Convert 100 fahrenheit to celsius and add 10\")\n</code></pre>"},{"location":"architecture/modules/react/#limitations","title":"Limitations","text":"<ol> <li>Token Usage: Each iteration adds to token count</li> <li>Latency: Multiple LLM calls increase response time</li> <li>Reliability: Agent may not always pick the right tool</li> <li>Max Iterations: Tasks may not complete within iteration limit</li> </ol>"},{"location":"architecture/modules/react/#see-also","title":"See Also","text":"<ul> <li>Base Module - Module foundation</li> <li>Predict Module - Core prediction</li> <li>Tool API - Creating tools</li> <li>Interrupt API - Human-in-the-loop</li> <li>ADR-005: ReAct Module</li> <li>ADR-004: Interruptible Decorator</li> </ul>"},{"location":"examples/advanced/","title":"Advanced Examples","text":"<p>Advanced patterns and techniques.</p>"},{"location":"examples/advanced/#module-composition","title":"Module Composition","text":"<p>Build complex modules from simpler ones:</p> <pre><code>from udspy import Module, Predict, Prediction\n\nclass ChainOfThought(Module):\n    \"\"\"Answer questions with explicit reasoning.\"\"\"\n\n    def __init__(self, signature):\n        # Create intermediate signature for reasoning\n        self.reason = Predict(make_signature(\n            signature.get_input_fields(),\n            {\"reasoning\": str},\n            \"Think step-by-step about this problem\",\n        ))\n\n        # Final answer with reasoning context\n        self.answer = Predict(signature)\n\n    def forward(self, **inputs):\n        # Generate reasoning\n        thought = self.reason(**inputs)\n\n        # Generate answer (could inject reasoning into prompt)\n        result = self.answer(**inputs)\n        result[\"reasoning\"] = thought.reasoning\n\n        return Prediction(**result)\n</code></pre>"},{"location":"examples/advanced/#retry-logic","title":"Retry Logic","text":"<p>Implement retry with validation:</p> <pre><code>from pydantic import ValidationError\n\nclass ValidatedPredict(Module):\n    def __init__(self, signature, max_retries=3):\n        self.predictor = Predict(signature)\n        self.signature = signature\n        self.max_retries = max_retries\n\n    def forward(self, **inputs):\n        for attempt in range(self.max_retries):\n            try:\n                result = self.predictor(**inputs)\n                # Validate result matches expected schema\n                return result\n            except ValidationError as e:\n                if attempt == self.max_retries - 1:\n                    raise\n                # Could inject error message to help LLM correct\n                continue\n</code></pre>"},{"location":"examples/advanced/#prompt-caching","title":"Prompt Caching","text":"<p>Cache prompts for repeated queries:</p> <pre><code>from functools import lru_cache\n\nclass CachedPredict(Module):\n    def __init__(self, signature):\n        self.predictor = Predict(signature)\n        self._cached_predict = lru_cache(maxsize=128)(self._predict)\n\n    def _predict(self, **inputs):\n        # Convert inputs to hashable tuple\n        key = tuple(sorted(inputs.items()))\n        return self.predictor(**dict(key))\n\n    def forward(self, **inputs):\n        return self._cached_predict(**inputs)\n</code></pre>"},{"location":"examples/advanced/#custom-adapters","title":"Custom Adapters","text":"<p>Create custom formatting:</p> <pre><code>from udspy import ChatAdapter\n\nclass VerboseAdapter(ChatAdapter):\n    def format_instructions(self, signature):\n        base = super().format_instructions(signature)\n        return f\"{base}\\n\\nIMPORTANT: Be extremely detailed in your response.\"\n\n    def format_inputs(self, signature, inputs):\n        base = super().format_inputs(signature, inputs)\n        return f\"Input data:\\n{base}\\n\\nAnalyze thoroughly:\"\n\npredictor = Predict(signature, adapter=VerboseAdapter())\n</code></pre>"},{"location":"examples/advanced/#testing-helpers","title":"Testing Helpers","text":"<p>Utilities for testing:</p> <pre><code>from unittest.mock import MagicMock\nfrom openai.types.chat import ChatCompletion, ChatCompletionMessage, Choice\n\ndef mock_openai_response(content: str) -&gt; ChatCompletion:\n    \"\"\"Create a mock OpenAI response.\"\"\"\n    return ChatCompletion(\n        id=\"test\",\n        model=\"gpt-4o-mini\",\n        object=\"chat.completion\",\n        created=1234567890,\n        choices=[\n            Choice(\n                index=0,\n                message=ChatCompletionMessage(\n                    role=\"assistant\",\n                    content=content,\n                ),\n                finish_reason=\"stop\",\n            )\n        ],\n    )\n\ndef test_with_mock():\n    mock_client = MagicMock()\n    mock_client.chat.completions.create.return_value = mock_openai_response(\n        \"[[ ## answer ## ]]\\nTest answer\"\n    )\n\n    udspy.settings.configure(client=mock_client)\n\n    predictor = Predict(QA)\n    result = predictor(question=\"Test?\")\n    assert result.answer == \"Test answer\"\n</code></pre>"},{"location":"examples/basic_usage/","title":"Basic Usage","text":"<p>This guide covers the fundamentals of using udspy.</p>"},{"location":"examples/basic_usage/#setup","title":"Setup","text":"<p>First, configure the OpenAI client:</p> <pre><code>import udspy\n\nudspy.settings.configure(api_key=\"sk-...\")\n</code></pre> <p>Or use environment variables:</p> <pre><code>import os\nimport udspy\n\nudspy.settings.configure(api_key=os.getenv(\"OPENAI_API_KEY\"))\n</code></pre>"},{"location":"examples/basic_usage/#simple-question-answering","title":"Simple Question Answering","text":"<pre><code>from udspy import Signature, InputField, OutputField, Predict\n\nclass QA(Signature):\n    \"\"\"Answer questions concisely.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\nresult = predictor(question=\"What is the capital of France?\")\nprint(result.answer)  # \"Paris\"\n</code></pre>"},{"location":"examples/basic_usage/#with-reasoning","title":"With Reasoning","text":"<pre><code>class ReasonedQA(Signature):\n    \"\"\"Answer questions with reasoning.\"\"\"\n    question: str = InputField()\n    reasoning: str = OutputField(description=\"Step-by-step reasoning\")\n    answer: str = OutputField(description=\"Final answer\")\n\npredictor = Predict(ReasonedQA)\nresult = predictor(question=\"What is 15 * 23?\")\nprint(f\"Reasoning: {result.reasoning}\")\nprint(f\"Answer: {result.answer}\")\n</code></pre>"},{"location":"examples/basic_usage/#custom-model-parameters","title":"Custom Model Parameters","text":"<pre><code>predictor = Predict(\n    signature=QA,\n    model=\"gpt-4\",\n    temperature=0.7,\n    max_tokens=100,\n)\n</code></pre>"},{"location":"examples/basic_usage/#global-defaults","title":"Global Defaults","text":"<pre><code>udspy.settings.configure(\n    api_key=\"sk-...\",\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n)\n\n# All predictors use these defaults unless overridden\npredictor = Predict(QA)\n</code></pre>"},{"location":"examples/basic_usage/#error-handling","title":"Error Handling","text":"<pre><code>from pydantic import ValidationError\n\ntry:\n    result = predictor(question=\"What is AI?\")\nexcept ValidationError as e:\n    print(f\"Output validation failed: {e}\")\nexcept Exception as e:\n    print(f\"API error: {e}\")\n</code></pre>"},{"location":"examples/basic_usage/#testing","title":"Testing","text":"<p>Mock the OpenAI client for testing:</p> <pre><code>from unittest.mock import MagicMock\nfrom openai.types.chat import ChatCompletion, ChatCompletionMessage, Choice\n\ndef test_qa():\n    # Mock response\n    mock_response = ChatCompletion(\n        id=\"test\",\n        model=\"gpt-4o-mini\",\n        object=\"chat.completion\",\n        created=1234567890,\n        choices=[\n            Choice(\n                index=0,\n                message=ChatCompletionMessage(\n                    role=\"assistant\",\n                    content=\"[[ ## answer ## ]]\\nParis\",\n                ),\n                finish_reason=\"stop\",\n            )\n        ],\n    )\n\n    # Configure mock client\n    mock_client = MagicMock()\n    mock_client.chat.completions.create.return_value = mock_response\n\n    udspy.settings.configure(client=mock_client)\n\n    # Test\n    predictor = Predict(QA)\n    result = predictor(question=\"What is the capital of France?\")\n    assert result.answer == \"Paris\"\n</code></pre>"},{"location":"examples/basic_usage/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Streaming</li> <li>Explore Tool Calling</li> <li>See Advanced Examples</li> </ul>"},{"location":"examples/chain_of_thought/","title":"Chain of Thought Examples","text":"<p>Chain of Thought (CoT) is a prompting technique that improves LLM reasoning by explicitly requesting step-by-step thinking before producing the final answer.</p>"},{"location":"examples/chain_of_thought/#overview","title":"Overview","text":"<p>The <code>ChainOfThought</code> module automatically adds a \"reasoning\" field to any signature, encouraging the LLM to show its work:</p> <pre><code>from udspy import ChainOfThought, Signature, InputField, OutputField\n\nclass QA(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\n# Automatically adds reasoning step\ncot = ChainOfThought(QA)\nresult = cot(question=\"What is 15 * 23?\")\n\nprint(result.reasoning)  # Step-by-step calculation\nprint(result.answer)     # \"345\"\n</code></pre>"},{"location":"examples/chain_of_thought/#basic-usage","title":"Basic Usage","text":""},{"location":"examples/chain_of_thought/#simple-question-answering","title":"Simple Question Answering","text":"<pre><code>import udspy\n\nudspy.settings.configure(api_key=\"your-key\")\n\nclass QA(Signature):\n    \"\"\"Answer questions clearly.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = ChainOfThought(QA)\nresult = predictor(question=\"What is the capital of France?\")\n\nprint(\"Reasoning:\", result.reasoning)\n# \"Let me recall the capital cities of European countries.\n#  France is a major European nation, and its capital is Paris.\"\n\nprint(\"Answer:\", result.answer)\n# \"Paris\"\n</code></pre>"},{"location":"examples/chain_of_thought/#math-problems","title":"Math Problems","text":"<p>Chain of Thought excels at mathematical reasoning:</p> <pre><code>result = predictor(question=\"What is 17 * 24?\")\n\nprint(\"Reasoning:\", result.reasoning)\n# \"I'll break this down: 17 * 24 = 17 * 20 + 17 * 4 = 340 + 68 = 408\"\n\nprint(\"Answer:\", result.answer)\n# \"408\"\n</code></pre>"},{"location":"examples/chain_of_thought/#advanced-usage","title":"Advanced Usage","text":""},{"location":"examples/chain_of_thought/#custom-reasoning-description","title":"Custom Reasoning Description","text":"<p>Customize how the reasoning field is described:</p> <pre><code>cot = ChainOfThought(\n    QA,\n    reasoning_description=\"Detailed mathematical proof with all intermediate steps\"\n)\n\nresult = cot(question=\"Prove that the sum of angles in a triangle is 180 degrees\")\n</code></pre>"},{"location":"examples/chain_of_thought/#multiple-output-fields","title":"Multiple Output Fields","text":"<p>Chain of Thought works with signatures that have multiple outputs:</p> <pre><code>class Analysis(Signature):\n    \"\"\"Analyze text comprehensively.\"\"\"\n    text: str = InputField()\n    summary: str = OutputField(description=\"Brief summary\")\n    sentiment: str = OutputField(description=\"Sentiment analysis\")\n    keywords: list[str] = OutputField(description=\"Key terms\")\n\nanalyzer = ChainOfThought(Analysis)\nresult = analyzer(text=\"Long article text here...\")\n\n# Access all outputs plus reasoning\nprint(result.reasoning)   # Analysis process\nprint(result.summary)     # Summary\nprint(result.sentiment)   # Sentiment\nprint(result.keywords)    # Keywords\n</code></pre>"},{"location":"examples/chain_of_thought/#with-custom-model-parameters","title":"With Custom Model Parameters","text":"<pre><code># Use with specific model and temperature\ncot = ChainOfThought(\n    QA,\n    model=\"gpt-4\",\n    temperature=0.0,  # Deterministic for math\n)\n\nresult = cot(question=\"What is the square root of 144?\")\n</code></pre>"},{"location":"examples/chain_of_thought/#comparison-with-vs-without-cot","title":"Comparison: With vs Without CoT","text":""},{"location":"examples/chain_of_thought/#without-chain-of-thought","title":"Without Chain of Thought","text":"<pre><code>from udspy import Predict\n\npredictor = Predict(QA)\nresult = predictor(question=\"Why is the sky blue?\")\n\nprint(result.answer)\n# \"The sky is blue due to Rayleigh scattering.\"\n</code></pre>"},{"location":"examples/chain_of_thought/#with-chain-of-thought","title":"With Chain of Thought","text":"<pre><code>cot_predictor = ChainOfThought(QA)\nresult = cot_predictor(question=\"Why is the sky blue?\")\n\nprint(result.reasoning)\n# \"Let me explain the physics: Sunlight contains all colors. As it enters\n#  the atmosphere, it interacts with air molecules. Blue light has shorter\n#  wavelengths and scatters more than other colors (Rayleigh scattering).\n#  This scattered blue light reaches our eyes from all directions.\"\n\nprint(result.answer)\n# \"The sky appears blue because blue light scatters more in the atmosphere\n#  due to its shorter wavelength (Rayleigh scattering).\"\n</code></pre> <p>Benefits: - More detailed and accurate answers - Shows the reasoning process - Better for complex or multi-step problems - Easier to verify correctness</p>"},{"location":"examples/chain_of_thought/#best-practices","title":"Best Practices","text":""},{"location":"examples/chain_of_thought/#1-use-for-complex-tasks","title":"1. Use for Complex Tasks","text":"<p>Chain of Thought shines for tasks requiring reasoning:</p> <pre><code># Good use cases\n- Math problems\n- Logic puzzles\n- Multi-step analysis\n- Proof generation\n- Planning tasks\n\n# Less useful for\n- Simple factual recall (\"What is 2+2?\")\n- Classification without reasoning\n- Direct information retrieval\n</code></pre>"},{"location":"examples/chain_of_thought/#2-adjust-temperature","title":"2. Adjust Temperature","text":"<pre><code># For deterministic tasks (math, logic)\ncot = ChainOfThought(QA, temperature=0.0)\n\n# For creative reasoning\ncot = ChainOfThought(QA, temperature=0.7)\n</code></pre>"},{"location":"examples/chain_of_thought/#3-review-reasoning-quality","title":"3. Review Reasoning Quality","text":"<p>Always check if reasoning makes sense:</p> <pre><code>result = cot(question=\"Complex problem\")\n\nif \"step\" in result.reasoning.lower():\n    print(\"\u2713 Good reasoning structure\")\n\nif len(result.reasoning) &lt; 50:\n    print(\"\u26a0 Reasoning might be too brief\")\n</code></pre>"},{"location":"examples/chain_of_thought/#real-world-examples","title":"Real-World Examples","text":""},{"location":"examples/chain_of_thought/#code-review-reasoning","title":"Code Review Reasoning","text":"<pre><code>class CodeReview(Signature):\n    \"\"\"Review code for issues.\"\"\"\n    code: str = InputField()\n    issues: list[str] = OutputField()\n    severity: str = OutputField()\n\nreviewer = ChainOfThought(CodeReview)\nresult = reviewer(code=\"\"\"\ndef divide(a, b):\n    return a / b\n\"\"\")\n\nprint(result.reasoning)\n# \"Let me analyze this code:\n#  1. No error handling for division by zero\n#  2. No type checking\n#  3. No documentation\n#  These are significant issues.\"\n\nprint(result.issues)\n# [\"Division by zero not handled\", \"Missing type hints\", \"No docstring\"]\n\nprint(result.severity)\n# \"High - can cause runtime errors\"\n</code></pre>"},{"location":"examples/chain_of_thought/#decision-making","title":"Decision Making","text":"<pre><code>class Decision(Signature):\n    \"\"\"Make informed decisions.\"\"\"\n    situation: str = InputField()\n    options: list[str] = InputField()\n    decision: str = OutputField()\n    justification: str = OutputField()\n\ndecider = ChainOfThought(Decision)\nresult = decider(\n    situation=\"Need to scale database, budget is tight\",\n    options=[\"Vertical scaling\", \"Horizontal scaling\", \"Managed service\"]\n)\n\nprint(result.reasoning)\n# \"Let me evaluate each option:\n#  - Vertical: Quick but limited and expensive long-term\n#  - Horizontal: Complex but scalable\n#  - Managed: Higher cost but less maintenance\n#  Given budget constraints...\"\n\nprint(result.decision)\nprint(result.justification)\n</code></pre> <p>See the full example in the repository.</p>"},{"location":"examples/context_settings/","title":"Context-Specific Settings","text":"<p>Learn how to use different API keys, models, and settings in different contexts.</p>"},{"location":"examples/context_settings/#overview","title":"Overview","text":"<p>The <code>settings.context()</code> context manager allows you to temporarily override global settings for specific operations. This is useful for:</p> <ul> <li>Multi-tenant applications with different API keys per user</li> <li>Testing with different models</li> <li>Varying temperature or other parameters per request</li> <li>Isolating settings in async operations</li> </ul> <p>The context manager is thread-safe using Python's <code>contextvars</code>, making it safe for concurrent operations.</p>"},{"location":"examples/context_settings/#basic-usage","title":"Basic Usage","text":""},{"location":"examples/context_settings/#override-model","title":"Override Model","text":"<pre><code>import udspy\n\n# Configure global settings\nudspy.settings.configure(api_key=\"sk-global\", model=\"gpt-4o-mini\")\n\n# Temporarily use a different model\nwith udspy.settings.context(model=\"gpt-4\"):\n    predictor = Predict(QA)\n    result = predictor(question=\"What is AI?\")\n    # Uses gpt-4\n\n# Back to global settings (gpt-4o-mini)\nresult = predictor(question=\"What is ML?\")\n</code></pre>"},{"location":"examples/context_settings/#override-api-key","title":"Override API Key","text":"<pre><code># Use a different API key for specific requests\nwith udspy.settings.context(api_key=\"sk-user-specific\"):\n    result = predictor(question=\"User-specific query\")\n    # Uses the user-specific API key\n</code></pre>"},{"location":"examples/context_settings/#override-multiple-settings","title":"Override Multiple Settings","text":"<pre><code>with udspy.settings.context(\n    model=\"gpt-4\",\n    temperature=0.0,\n    max_tokens=500\n):\n    result = predictor(question=\"Deterministic response needed\")\n</code></pre>"},{"location":"examples/context_settings/#multi-tenant-applications","title":"Multi-Tenant Applications","text":"<p>Handle different users with different API keys:</p> <pre><code>def handle_user_request(user_id: str, question: str):\n    \"\"\"Handle a request from a specific user.\"\"\"\n    # Get user-specific API key from database\n    user_api_key = get_user_api_key(user_id)\n\n    # Use user's API key for this request\n    with udspy.settings.context(api_key=user_api_key):\n        predictor = Predict(QA)\n        result = predictor(question=question)\n\n    return result.answer\n\n# Each user's request uses their own API key\nanswer1 = handle_user_request(\"user1\", \"What is Python?\")\nanswer2 = handle_user_request(\"user2\", \"What is Rust?\")\n</code></pre>"},{"location":"examples/context_settings/#nested-contexts","title":"Nested Contexts","text":"<p>Contexts can be nested, with inner contexts overriding outer ones:</p> <pre><code>udspy.settings.configure(model=\"gpt-4o-mini\", temperature=0.7)\n\nwith udspy.settings.context(model=\"gpt-4\", temperature=0.5):\n    # Uses gpt-4, temp=0.5\n\n    with udspy.settings.context(temperature=0.0):\n        # Uses gpt-4 (inherited), temp=0.0 (overridden)\n        pass\n\n    # Back to gpt-4, temp=0.5\n\n# Back to gpt-4o-mini, temp=0.7\n</code></pre>"},{"location":"examples/context_settings/#async-support","title":"Async Support","text":"<p>Context managers work seamlessly with async code:</p> <pre><code>import asyncio\n\nasync def generate_response(question: str, user_api_key: str):\n    with udspy.settings.context(api_key=user_api_key):\n        predictor = StreamingPredict(QA)\n        async for chunk in predictor.stream(question=question):\n            yield chunk\n\n# Handle multiple users concurrently\nasync def main():\n    tasks = [\n        generate_response(\"Question 1\", \"sk-user1\"),\n        generate_response(\"Question 2\", \"sk-user2\"),\n    ]\n    await asyncio.gather(*tasks)\n</code></pre>"},{"location":"examples/context_settings/#testing","title":"Testing","text":"<p>Use contexts to isolate test settings:</p> <pre><code>def test_with_specific_model():\n    \"\"\"Test behavior with a specific model.\"\"\"\n    with udspy.settings.context(\n        api_key=\"sk-test\",\n        model=\"gpt-4\",\n        temperature=0.0,  # Deterministic for testing\n    ):\n        predictor = Predict(QA)\n        result = predictor(question=\"2+2\")\n        assert \"4\" in result.answer\n</code></pre>"},{"location":"examples/context_settings/#custom-clients","title":"Custom Clients","text":"<p>You can also provide custom OpenAI clients:</p> <pre><code>from openai import OpenAI, AsyncOpenAI\n\ncustom_client = OpenAI(\n    api_key=\"sk-custom\",\n    base_url=\"https://custom-endpoint.example.com\",\n)\n\nwith udspy.settings.context(client=custom_client):\n    # Uses custom client with custom endpoint\n    result = predictor(question=\"...\")\n</code></pre>"},{"location":"examples/context_settings/#complete-example","title":"Complete Example","text":"<pre><code>import udspy\nfrom udspy import Signature, InputField, OutputField, Predict\n\n# Global configuration\nudspy.settings.configure(\n    api_key=\"sk-default\",\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n)\n\nclass QA(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\n\n# Scenario 1: Default settings\nresult = predictor(question=\"What is AI?\")\n\n# Scenario 2: High-quality request (use GPT-4)\nwith udspy.settings.context(model=\"gpt-4\"):\n    result = predictor(question=\"Explain quantum computing\")\n\n# Scenario 3: Deterministic response\nwith udspy.settings.context(temperature=0.0):\n    result = predictor(question=\"What is 2+2?\")\n\n# Scenario 4: User-specific API key\nwith udspy.settings.context(api_key=user.api_key):\n    result = predictor(question=user.question)\n</code></pre> <p>See the full example in the repository.</p>"},{"location":"examples/history/","title":"Conversation History","text":"<p>The <code>History</code> class manages conversation history for multi-turn interactions. When passed to <code>Predict</code>, it automatically maintains context across multiple calls.</p>"},{"location":"examples/history/#basic-usage","title":"Basic Usage","text":"<pre><code>from udspy import History, Predict, Signature, InputField, OutputField\n\nclass QA(Signature):\n    '''Answer questions.'''\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\nhistory = History()\n\n# First turn\nresult = predictor(question=\"What is Python?\", history=history)\nprint(result.answer)\n\n# Second turn - context is maintained\nresult = predictor(question=\"What are its main features?\", history=history)\nprint(result.answer)  # Assistant knows we're still talking about Python\n</code></pre>"},{"location":"examples/history/#how-it-works","title":"How It Works","text":"<p><code>History</code> stores messages in OpenAI format and automatically: - Adds user messages when you call the predictor - Adds assistant responses after generation - Maintains tool calls and results (when using tool calling) - Preserves conversation context across turns</p>"},{"location":"examples/history/#api","title":"API","text":""},{"location":"examples/history/#creating-history","title":"Creating History","text":"<pre><code># Empty history\nhistory = History()\n\n# With initial messages\nhistory = History(messages=[\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"}\n])\n</code></pre>"},{"location":"examples/history/#adding-messages","title":"Adding Messages","text":"<pre><code># Add user message\nhistory.add_user_message(\"What is AI?\")\n\n# Add assistant message\nhistory.add_assistant_message(\"AI stands for Artificial Intelligence...\")\n\n# Add system message\nhistory.add_system_message(\"You are a helpful tutor\")\n\n# Add tool result\nhistory.add_tool_result(tool_call_id=\"call_123\", content=\"Result: 42\")\n\n# Add generic message\nhistory.add_message(\"user\", \"Custom message\")\n</code></pre>"},{"location":"examples/history/#managing-history","title":"Managing History","text":"<pre><code># Get number of messages\nprint(len(history))  # e.g., 5\n\n# Clear all messages\nhistory.clear()\n\n# Copy history (for branching conversations)\nbranch = history.copy()\n\n# Access messages directly\nfor msg in history.messages:\n    print(f\"{msg['role']}: {msg['content']}\")\n\n# String representation\nprint(history)  # Shows formatted conversation\n</code></pre>"},{"location":"examples/history/#use-cases","title":"Use Cases","text":""},{"location":"examples/history/#multi-turn-conversations","title":"Multi-Turn Conversations","text":"<pre><code>predictor = Predict(QA)\nhistory = History()\n\n# Each call maintains context\npredictor(question=\"What is machine learning?\", history=history)\npredictor(question=\"How does it differ from traditional programming?\", history=history)\npredictor(question=\"Can you give me an example?\", history=history)\n</code></pre>"},{"location":"examples/history/#pre-populating-context","title":"Pre-Populating Context","text":"<pre><code>history = History()\n\n# Set up initial context\nhistory.add_system_message(\"You are a Python expert. Keep answers concise.\")\nhistory.add_user_message(\"I'm learning Python\")\nhistory.add_assistant_message(\"Great! I'm here to help.\")\n\n# Now ask questions with this context\nresult = predictor(question=\"How do I use list comprehensions?\", history=history)\n</code></pre>"},{"location":"examples/history/#branching-conversations","title":"Branching Conversations","text":"<pre><code>main_history = History()\n\n# Start main conversation\npredictor(question=\"Tell me about programming languages\", history=main_history)\n\n# Branch 1: Explore Python\npython_branch = main_history.copy()\npredictor(question=\"Tell me more about Python\", history=python_branch)\n\n# Branch 2: Explore JavaScript\njs_branch = main_history.copy()\npredictor(question=\"Tell me more about JavaScript\", history=js_branch)\n\n# Each branch maintains independent context\n</code></pre>"},{"location":"examples/history/#conversation-reset","title":"Conversation Reset","text":"<pre><code>history = History()\n\n# First conversation\npredictor(question=\"What is Python?\", history=history)\n\n# Reset for new topic\nhistory.clear()\n\n# New conversation with no context\npredictor(question=\"What is JavaScript?\", history=history)\n</code></pre>"},{"location":"examples/history/#history-with-tool-calling","title":"History with Tool Calling","text":"<pre><code>from udspy import tool\nfrom pydantic import Field\n\n@tool(name=\"Calculator\", description=\"Perform calculations\")\ndef calculator(operation: str = Field(...), a: float = Field(...), b: float = Field(...)) -&gt; float:\n    ops = {\"add\": a + b, \"subtract\": a - b, \"multiply\": a * b, \"divide\": a / b}\n    return ops[operation]\n\npredictor = Predict(QA, tools=[calculator])\nhistory = History()\n\n# Tool calls are automatically recorded in history\nresult = predictor(question=\"What is 15 times 23?\", history=history)\n# History now contains: user message, assistant tool call, tool result, final assistant answer\n\n# Next turn has full context including tool usage\nresult = predictor(question=\"Now add 100 to that\", history=history)\n</code></pre>"},{"location":"examples/history/#best-practices","title":"Best Practices","text":"<ol> <li>One History per Conversation Thread: Create a new <code>History</code> instance for each independent conversation</li> <li>Use <code>copy()</code> for Branching: When you want to explore different paths from the same starting point</li> <li>Clear When Changing Topics: Use <code>history.clear()</code> when starting a completely new conversation</li> <li>Pre-populate for Context: Add system messages or previous conversation history to set context</li> <li>Inspect Messages: Access <code>history.messages</code> directly when you need to debug or log conversations</li> </ol>"},{"location":"examples/history/#async-support","title":"Async Support","text":"<p>History works seamlessly with all async patterns:</p> <pre><code># Async streaming\nasync for event in predictor.astream(question=\"...\", history=history):\n    if isinstance(event, StreamChunk):\n        print(event.delta, end=\"\", flush=True)\n\n# Async non-streaming\nresult = await predictor.aforward(question=\"...\", history=history)\n\n# Sync (uses asyncio.run internally)\nresult = predictor(question=\"...\", history=history)\n</code></pre>"},{"location":"examples/history/#examples","title":"Examples","text":"<p>See history.py for complete working examples.</p>"},{"location":"examples/react/","title":"ReAct (Reasoning and Acting)","text":"<p>ReAct is a powerful pattern for building LLM agents that can reason through multi-step problems and use tools to accomplish tasks. The name comes from combining Reasoning and Acting.</p>"},{"location":"examples/react/#overview","title":"Overview","text":"<p>The ReAct module enables you to build agents that:</p> <ul> <li>Reason iteratively: Think through problems step-by-step</li> <li>Use multiple tools: Call different tools to gather information or perform actions</li> <li>Handle ambiguity: Ask users for clarification when needed</li> <li>Require confirmation: Request user approval for destructive operations</li> <li>Save and restore state: Pause execution for user input and resume seamlessly</li> </ul>"},{"location":"examples/react/#how-react-works","title":"How ReAct Works","text":"<p>ReAct follows a thought \u2192 action \u2192 observation loop:</p> <ol> <li>Thought: The agent reasons about what to do next</li> <li>Action: The agent selects a tool and specifies arguments</li> <li>Observation: The tool returns a result</li> <li>Repeat: Continue until the task is complete</li> </ol> <p>All reasoning steps are tracked in a trajectory that provides context for subsequent decisions.</p>"},{"location":"examples/react/#basic-usage","title":"Basic Usage","text":"<pre><code>from pydantic import Field\nfrom udspy import InputField, OutputField, ReAct, Signature, tool\n\n# Define tools\n@tool(name=\"search\", description=\"Search for information\")\ndef search(query: str = Field(description=\"Search query\")) -&gt; str:\n    # Call search API\n    return f\"Search results for: {query}\"\n\n@tool(name=\"calculator\", description=\"Perform calculations\")\ndef calculator(expression: str = Field(description=\"Math expression\")) -&gt; str:\n    return str(eval(expression))\n\n# Define task signature\nclass ResearchTask(Signature):\n    \"\"\"Research a topic and provide a comprehensive answer.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\n# Create ReAct agent\nagent = ReAct(\n    ResearchTask,\n    tools=[search, calculator],\n    max_iters=10\n)\n\n# Execute\nresult = agent(question=\"What is Python and how many letters are in 'Python'?\")\nprint(result.answer)\n# The agent will:\n# 1. Search for \"Python\"\n# 2. Calculate len(\"Python\") = 6\n# 3. Synthesize an answer combining both results\n</code></pre>"},{"location":"examples/react/#string-signatures","title":"String Signatures","text":"<p>For quick prototyping, you can use string signatures:</p> <pre><code>agent = ReAct(\n    \"task -&gt; result\",  # Simple format: inputs -&gt; outputs\n    tools=[search, calculator]\n)\n\nresult = agent(task=\"Find information about React\")\nprint(result.result)\n</code></pre>"},{"location":"examples/react/#user-clarification-with-ask_to_user","title":"User Clarification with <code>ask_to_user</code>","text":"<p>When the user's request is ambiguous, the agent can ask for clarification:</p> <pre><code>from udspy import HumanInTheLoopRequired\n\nagent = ReAct(\n    ResearchTask,\n    tools=[search],\n    enable_ask_to_user=True  # Enable clarification requests\n)\n\ntry:\n    result = agent(question=\"Tell me about it\")\nexcept HumanInTheLoopRequired as e:\n    # Agent needs clarification\n    print(f\"Agent asks: {e.question}\")\n    # \"What topic would you like to know about?\"\n\n    # User provides clarification\n    user_response = \"The Python programming language\"\n\n    # Resume execution\n    result = agent.resume(user_response, e)\n    print(result.answer)\n</code></pre>"},{"location":"examples/react/#configuring-ask_to_user","title":"Configuring <code>ask_to_user</code>","text":"<p>The <code>ask_to_user</code> tool is enabled by default and can be called by the agent whenever clarification is needed:</p> <pre><code>agent = ReAct(\n    ResearchTask,\n    tools=[search],\n    enable_ask_to_user=True,  # Default: enabled\n)\n</code></pre> <p>To disable <code>ask_to_user</code> entirely:</p> <pre><code>agent = ReAct(\n    ResearchTask,\n    tools=[search],\n    enable_ask_to_user=False  # No clarification requests\n)\n</code></pre>"},{"location":"examples/react/#tool-confirmation","title":"Tool Confirmation","text":"<p>For destructive or sensitive operations, you can require user confirmation:</p> <pre><code>@tool(\n    name=\"delete_file\",\n    description=\"Delete a file\",\n    interruptible=True  # Require confirmation\n)\ndef delete_file(path: str = Field(description=\"File path\")) -&gt; str:\n    os.remove(path)\n    return f\"Deleted {path}\"\n\nagent = ReAct(\n    ResearchTask,\n    tools=[delete_file]\n)\n\ntry:\n    result = agent(question=\"Delete /tmp/old_data.txt\")\nexcept HumanInTheLoopRequired as e:\n    # Agent asks for confirmation\n    print(f\"Confirm: {e.question}\")\n    # \"Confirm execution of delete_file with args: {'path': '/tmp/old_data.txt'}? (yes/no)\"\n\n    # Check tool call info\n    if e.tool_call:\n        print(f\"Tool: {e.tool_call.name}\")\n        print(f\"Args: {e.tool_call.args}\")\n\n    # User confirms\n    result = agent.resume(\"yes\", e)\n</code></pre>"},{"location":"examples/react/#accessing-the-trajectory","title":"Accessing the Trajectory","text":"<p>The trajectory contains all reasoning steps and tool calls:</p> <pre><code>result = agent(question=\"What is 2 + 2?\")\n\n# Access trajectory\nfor i in range(10):  # Max iterations\n    observation_key = f\"observation_{i}\"\n    if observation_key not in result.trajectory:\n        break\n\n    print(f\"Step {i + 1}:\")\n    print(f\"  Reasoning: {result.trajectory.get(f'reasoning_{i}', '')}\")\n    print(f\"  Tool: {result.trajectory[f'tool_name_{i}']}\")\n    print(f\"  Args: {result.trajectory[f'tool_args_{i}']}\")\n    print(f\"  Observation: {result.trajectory[observation_key]}\")\n</code></pre> <p>Example trajectory: <pre><code>Step 1:\n  Reasoning: I need to calculate 2 + 2\n  Tool: calculator\n  Args: {'expression': '2 + 2'}\n  Observation: 4\n\nStep 2:\n  Reasoning: I have the answer\n  Tool: finish\n  Args: {}\n  Observation: Task completed\n</code></pre></p>"},{"location":"examples/react/#configuration-options","title":"Configuration Options","text":"<pre><code>agent = ReAct(\n    signature=ResearchTask,       # Task signature\n    tools=[search, calculator],   # Available tools\n    max_iters=10,                 # Maximum reasoning steps (default: 10)\n    enable_ask_to_user=True       # Enable user clarification (default: True)\n)\n</code></pre>"},{"location":"examples/react/#parameters","title":"Parameters","text":"<ul> <li><code>signature</code>: Signature class or string format (<code>\"input -&gt; output\"</code>)</li> <li><code>tools</code>: List of tool functions (decorated with <code>@tool</code>) or <code>Tool</code> objects</li> <li><code>max_iters</code>: Maximum number of reasoning iterations before stopping</li> <li><code>enable_ask_to_user</code>: Whether to enable the <code>ask_to_user</code> tool</li> </ul>"},{"location":"examples/react/#async-support","title":"Async Support","text":"<p>ReAct fully supports async execution:</p> <pre><code>import asyncio\n\nasync def main():\n    agent = ReAct(ResearchTask, tools=[search])\n\n    # Async forward\n    result = await agent.aforward(question=\"What is Python?\")\n    print(result.answer)\n\n    # Or use the async resume method\n    try:\n        result = await agent.aforward(question=\"Tell me about it\")\n    except HumanInTheLoopRequired as e:\n        result = await agent.aresume(\"Python\", e)\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/react/#built-in-tools","title":"Built-in Tools","text":"<p>Every ReAct agent automatically includes these tools:</p>"},{"location":"examples/react/#finish","title":"<code>finish</code>","text":"<p>Signals that the agent has collected enough information to answer:</p> <pre><code># Agent internally calls:\n# Tool: finish\n# Args: {}\n</code></pre> <p>This is automatically selected by the LLM when it has sufficient information.</p>"},{"location":"examples/react/#ask_to_user-if-enabled","title":"<code>ask_to_user</code> (if enabled)","text":"<p>Requests clarification from the user:</p> <pre><code># Agent internally calls:\n# Tool: ask_to_user\n# Args: {\"question\": \"What topic would you like to know about?\"}\n</code></pre>"},{"location":"examples/react/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"examples/react/#multi-tool-research","title":"Multi-Tool Research","text":"<pre><code>@tool(name=\"search_papers\", description=\"Search academic papers\")\ndef search_papers(query: str = Field(...)) -&gt; str:\n    return f\"Papers about: {query}\"\n\n@tool(name=\"summarize\", description=\"Summarize text\")\ndef summarize(text: str = Field(...)) -&gt; str:\n    return f\"Summary of: {text[:100]}...\"\n\nclass DeepResearch(Signature):\n    \"\"\"Conduct deep research on a scientific topic.\"\"\"\n    topic: str = InputField()\n    summary: str = OutputField()\n\nagent = ReAct(\n    DeepResearch,\n    tools=[search_papers, summarize],\n    max_iters=15  # More steps for complex research\n)\n\nresult = agent(topic=\"quantum computing\")\n</code></pre>"},{"location":"examples/react/#error-recovery","title":"Error Recovery","text":"<p>The agent automatically handles tool errors and can recover:</p> <pre><code>@tool(name=\"api_call\", description=\"Call external API\")\ndef api_call(endpoint: str = Field(...)) -&gt; str:\n    try:\n        # Simulated API call that might fail\n        if endpoint == \"invalid\":\n            raise ValueError(\"Invalid endpoint\")\n        return \"API response\"\n    except Exception as e:\n        # Error is returned as observation\n        raise\n\n# Agent will see error in observation and can:\n# 1. Try a different tool\n# 2. Retry with different args\n# 3. Ask user for help (using ask_to_user tool)\n</code></pre>"},{"location":"examples/react/#state-management","title":"State Management","text":"<p>Save and restore execution state:</p> <pre><code>try:\n    result = agent(question=\"Delete important files\")\nexcept HumanInTheLoopRequired as e:\n    # Save state\n    saved_state = e\n    saved_question = e.question\n    # Access ReAct-specific state from context\n    saved_trajectory = e.context[\"trajectory\"]\n    saved_iteration = e.context[\"iteration\"]\n    saved_input_args = e.context[\"input_args\"]\n\n    # Later, restore and continue\n    user_response = input(f\"{saved_question} \")\n    result = agent.resume(user_response, saved_state)\n</code></pre>"},{"location":"examples/react/#dspy-compatibility","title":"DSPy Compatibility","text":"<p>The <code>Tool</code> class includes DSPy-compatible aliases:</p> <pre><code>from udspy import Tool\n\n@tool(name=\"search\", description=\"Search tool\")\ndef search(query: str = Field(...)) -&gt; str:\n    return \"results\"\n\n# DSPy-style access\nprint(search.desc)   # Same as search.description\nprint(search.args)   # Dict of argument specs\n</code></pre>"},{"location":"examples/react/#best-practices","title":"Best Practices","text":"<ol> <li>Provide clear tool descriptions: The LLM uses descriptions to select tools</li> <li>Use Field() for parameters: Provide descriptions for all tool parameters</li> <li>Limit max_iters: Prevent infinite loops with reasonable iteration limits</li> <li>Enable confirmation for destructive ops: Use <code>interruptible=True</code></li> <li>Handle HumanInTheLoopRequired: Always catch and handle clarification requests</li> <li>Use specific signatures: Clear input/output fields help the agent understand the task</li> <li>Test with mock tools: Use simple mock tools to validate agent logic</li> </ol>"},{"location":"examples/react/#common-patterns","title":"Common Patterns","text":""},{"location":"examples/react/#research-and-summarize","title":"Research and Summarize","text":"<pre><code>agent = ReAct(\n    \"query -&gt; summary\",\n    tools=[search, summarize]\n)\nresult = agent(query=\"Latest AI developments\")\n</code></pre>"},{"location":"examples/react/#data-analysis","title":"Data Analysis","text":"<pre><code>@tool(name=\"load_data\", description=\"Load dataset\")\ndef load_data(path: str = Field(...)) -&gt; str:\n    return \"data loaded\"\n\n@tool(name=\"analyze\", description=\"Analyze data\")\ndef analyze(metric: str = Field(...)) -&gt; str:\n    return \"analysis results\"\n\nagent = ReAct(\n    \"dataset, question -&gt; insights\",\n    tools=[load_data, analyze]\n)\n</code></pre>"},{"location":"examples/react/#multi-step-workflows","title":"Multi-Step Workflows","text":"<pre><code>@tool(name=\"step1\", description=\"First step\")\ndef step1() -&gt; str: return \"step1 done\"\n\n@tool(name=\"step2\", description=\"Second step\")\ndef step2(input: str = Field(...)) -&gt; str: return \"step2 done\"\n\n@tool(name=\"step3\", description=\"Third step\")\ndef step3(input: str = Field(...)) -&gt; str: return \"step3 done\"\n\nagent = ReAct(\n    \"task -&gt; result\",\n    tools=[step1, step2, step3],\n    max_iters=20\n)\n</code></pre>"},{"location":"examples/react/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/react/#agent-doesnt-finish","title":"Agent doesn't finish","text":"<p>Increase <code>max_iters</code> or simplify the task:</p> <pre><code>agent = ReAct(signature, tools=tools, max_iters=20)\n</code></pre>"},{"location":"examples/react/#too-many-tool-calls","title":"Too many tool calls","text":"<p>Reduce <code>max_iters</code> or improve tool descriptions:</p> <pre><code>@tool(\n    name=\"search\",\n    description=\"Search ONLY when you need external information. Use for factual queries.\"\n)\n</code></pre>"},{"location":"examples/react/#agent-asks-for-clarification-too-often","title":"Agent asks for clarification too often","text":"<p>Disable or restrict <code>ask_to_user</code>:</p> <pre><code>agent = ReAct(\n    signature,\n    tools=tools,\n    enable_ask_to_user=False  # Disable entirely\n)\n</code></pre> <p>Or disable it completely if the agent should never ask for clarification:</p> <pre><code>agent = ReAct(\n    signature,\n    tools=tools,\n    enable_ask_to_user=False  # Disable user clarification\n)\n</code></pre>"},{"location":"examples/react/#see-also","title":"See Also","text":"<ul> <li>Tool Calling Guide - Creating custom tools</li> <li>Chain of Thought - Simpler reasoning module</li> <li>ReAct API Reference - Full API documentation</li> </ul>"},{"location":"examples/streaming/","title":"Streaming Examples","text":"<p>Learn how to use streaming for better user experience.</p>"},{"location":"examples/streaming/#basic-streaming","title":"Basic Streaming","text":"<pre><code>import asyncio\nfrom udspy import StreamingPredict, Signature, InputField, OutputField\n\nclass QA(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\nasync def main():\n    predictor = StreamingPredict(QA)\n\n    async for chunk in predictor.stream(question=\"What is AI?\"):\n        if isinstance(chunk, StreamChunk):\n            print(chunk.delta, end=\"\", flush=True)\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/streaming/#multi-field-streaming","title":"Multi-Field Streaming","text":"<p>Stream reasoning and answer separately:</p> <pre><code>class ReasonedQA(Signature):\n    \"\"\"Answer with reasoning.\"\"\"\n    question: str = InputField()\n    reasoning: str = OutputField()\n    answer: str = OutputField()\n\nasync def main():\n    predictor = StreamingPredict(ReasonedQA)\n\n    print(\"Question: What is the sum of first 10 primes?\\n\")\n\n    async for item in predictor.stream(\n        question=\"What is the sum of first 10 primes?\"\n    ):\n        if isinstance(item, StreamChunk):\n            if item.field_name == \"reasoning\":\n                print(f\"\ud83d\udcad {item.delta}\", end=\"\", flush=True)\n            elif item.field_name == \"answer\":\n                print(f\"\\n\u2713 {item.delta}\", end=\"\", flush=True)\n\n            if item.is_complete:\n                print()  # Newline after field completes\n\n        elif isinstance(item, Prediction):\n            print(f\"\\n\\nFinal: {item.answer}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/streaming/#web-application-integration","title":"Web Application Integration","text":"<p>Use streaming in a web application:</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\n\napp = FastAPI()\n\n@app.get(\"/ask\")\nasync def ask_question(question: str):\n    async def generate():\n        predictor = StreamingPredict(QA)\n        async for chunk in predictor.stream(question=question):\n            if isinstance(chunk, StreamChunk) and not chunk.is_complete:\n                # chunk.delta contains the new incremental text\n                # chunk.content contains the full accumulated text so far\n                yield f\"data: {chunk.delta}\\n\\n\"\n\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n</code></pre> <p>See the full example in the repository.</p>"},{"location":"examples/tool_calling/","title":"Tool Calling","text":"<p>Learn how to use OpenAI's native tool calling with udspy.</p>"},{"location":"examples/tool_calling/#two-ways-to-use-tools","title":"Two Ways to Use Tools","text":"<p>udspy supports two approaches to tool calling:</p> <ol> <li>Automatic Execution with <code>@tool</code> decorator (Recommended) - Tools are automatically executed</li> <li>Manual Execution with Pydantic models - You handle tool execution yourself</li> </ol>"},{"location":"examples/tool_calling/#automatic-tool-execution-recommended","title":"Automatic Tool Execution (Recommended)","text":"<p>Use the <code>@tool</code> decorator to mark functions as executable tools. udspy will automatically execute them and handle multi-turn conversations:</p> <pre><code>from pydantic import Field\nfrom udspy import tool, Predict, Signature, InputField, OutputField\n\n@tool(name=\"Calculator\", description=\"Perform arithmetic operations\")\ndef calculator(\n    operation: str = Field(description=\"add, subtract, multiply, or divide\"),\n    a: float = Field(description=\"First number\"),\n    b: float = Field(description=\"Second number\"),\n) -&gt; float:\n    \"\"\"Execute calculator operation.\"\"\"\n    ops = {\"add\": a + b, \"subtract\": a - b, \"multiply\": a * b, \"divide\": a / b}\n    return ops[operation]\n\nclass MathQuery(Signature):\n    \"\"\"Answer math questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\n# Tools decorated with @tool are automatically executed\npredictor = Predict(MathQuery, tools=[calculator])\nresult = predictor(question=\"What is 157 times 234?\")\nprint(result.answer)  # \"The answer is 36738\"\n</code></pre> <p>The predictor automatically: 1. Detects when the LLM wants to call a tool 2. Executes the tool function 3. Sends the result back to the LLM 4. Returns the final answer</p>"},{"location":"examples/tool_calling/#optional-tool-execution","title":"Optional Tool Execution","text":"<p>You can control whether tools are automatically executed:</p> <pre><code># Default: auto_execute_tools=True\nresult = predictor(question=\"What is 5 + 3?\")\nprint(result.answer)  # \"The answer is 8\"\n\n# Get tool calls without execution\nresult = predictor(question=\"What is 5 + 3?\", auto_execute_tools=False)\nif \"tool_calls\" in result:\n    print(f\"LLM wants to call: {result.tool_calls[0]['name']}\")\n    print(f\"With arguments: {result.tool_calls[0]['arguments']}\")\n    # Now you can execute manually or log/analyze the tool calls\n</code></pre> <p>This is useful for: - Requiring user approval before executing tools - Logging or analyzing tool usage patterns - Implementing custom execution logic - Rate limiting or caching tool results</p>"},{"location":"examples/tool_calling/#manual-tool-execution","title":"Manual Tool Execution","text":"<p>Define tools as Pydantic models when you want full control:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass Calculator(BaseModel):\n    \"\"\"Perform arithmetic operations.\"\"\"\n    operation: str = Field(description=\"add, subtract, multiply, or divide\")\n    a: float = Field(description=\"First number\")\n    b: float = Field(description=\"Second number\")\n\n# Pydantic models are schema-only (not automatically executed)\npredictor = Predict(MathQuery, tools=[Calculator])\nresult = predictor(question=\"What is 157 times 234?\")\n\n# You must check for and execute tool calls yourself\nif \"tool_calls\" in result:\n    for tool_call in result.tool_calls:\n        print(f\"Called: {tool_call['name']}\")\n        print(f\"Arguments: {tool_call['arguments']}\")\n        # Execute manually and construct follow-up messages\n</code></pre>"},{"location":"examples/tool_calling/#multiple-tools","title":"Multiple Tools","text":"<p>Provide multiple tools for different operations:</p> <pre><code>class Calculator(BaseModel):\n    \"\"\"Perform arithmetic operations.\"\"\"\n    operation: str\n    a: float\n    b: float\n\nclass WebSearch(BaseModel):\n    \"\"\"Search the web.\"\"\"\n    query: str = Field(description=\"Search query\")\n\nclass DateInfo(BaseModel):\n    \"\"\"Get date information.\"\"\"\n    timezone: str = Field(description=\"Timezone name\")\n\npredictor = Predict(\n    signature,\n    tools=[Calculator, WebSearch, DateInfo],\n)\n</code></pre>"},{"location":"examples/tool_calling/#tool-execution","title":"Tool Execution","text":"<p>Execute tool calls and continue the conversation:</p> <pre><code>def execute_calculator(operation: str, a: float, b: float) -&gt; float:\n    \"\"\"Execute calculator tool.\"\"\"\n    ops = {\n        \"add\": lambda x, y: x + y,\n        \"subtract\": lambda x, y: x - y,\n        \"multiply\": lambda x, y: x * y,\n        \"divide\": lambda x, y: x / y,\n    }\n    return ops[operation](a, b)\n\n# Get initial response with tool calls\nresult = predictor(question=\"What is 15 * 23?\")\n\nif \"tool_calls\" in result:\n    # Execute tool calls\n    tool_results = []\n    for tool_call in result.tool_calls:\n        if tool_call[\"name\"] == \"Calculator\":\n            args = json.loads(tool_call[\"arguments\"])\n            result_value = execute_calculator(**args)\n            tool_results.append({\n                \"id\": tool_call[\"id\"],\n                \"result\": result_value,\n            })\n\n    # Continue conversation with tool results\n    # (requires manual message construction - see advanced examples)\n</code></pre> <p>See the full example in the repository.</p>"}]}