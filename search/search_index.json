{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"udspy","text":"<p>A minimal DSPy-inspired library with native OpenAI tool calling.</p>"},{"location":"#overview","title":"Overview","text":"<p>udspy provides a clean, minimal abstraction for building LLM-powered applications with structured inputs and outputs. Inspired by DSPy, it focuses on simplicity and leverages OpenAI's native tool calling capabilities.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Pydantic-based Signatures: Define clear input/output contracts using Pydantic models</li> <li>Native Tool Calling: First-class support for OpenAI's function calling API</li> <li>Module Abstraction: Compose LLM calls into reusable, testable modules</li> <li>Streaming Support: Stream reasoning and outputs incrementally for better UX</li> <li>Minimal Dependencies: Only requires <code>openai</code> and <code>pydantic</code></li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install udspy\n</code></pre> <p>Or with uv:</p> <pre><code>uv add udspy\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import udspy\nfrom udspy import Signature, InputField, OutputField, Predict\n\n# Configure OpenAI client\nudspy.settings.configure(api_key=\"your-api-key\")\n\n# Define a signature\nclass QA(Signature):\n    \"\"\"Answer questions concisely.\"\"\"\n    question: str = InputField(description=\"Question to answer\")\n    answer: str = OutputField(description=\"Concise answer\")\n\n# Create and use a predictor\npredictor = Predict(QA)\nresult = predictor(question=\"What is the capital of France?\")\nprint(result.answer)  # \"Paris\"\n</code></pre>"},{"location":"#philosophy","title":"Philosophy","text":"<p>udspy is designed with these principles:</p> <ol> <li>Simplicity First: Start minimal, iterate based on real needs</li> <li>Type Safety: Leverage Pydantic for runtime validation</li> <li>Native Integration: Use platform features (like OpenAI tools) instead of reinventing</li> <li>Testability: Make it easy to test LLM-powered code</li> <li>Composability: Build complex behavior from simple, reusable modules</li> </ol>"},{"location":"#comparison-with-dspy","title":"Comparison with DSPy","text":"Feature udspy DSPy Input/Output Definition Pydantic models Custom signatures Tool Calling Native OpenAI tools Custom adapter layer Streaming Built-in async support Complex callback system Dependencies 2 (openai, pydantic) Many Focus Minimal, opinionated Full-featured framework"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Read the Architecture Overview</li> <li>Check out Examples</li> <li>Browse the API Reference</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE file for details.</p>"},{"location":"api/adapter/","title":"API Reference: Adapters","text":""},{"location":"api/adapter/#udspy.adapter","title":"<code>udspy.adapter</code>","text":"<p>Adapter for formatting LLM inputs/outputs with Pydantic models.</p>"},{"location":"api/adapter/#udspy.adapter-classes","title":"Classes","text":""},{"location":"api/adapter/#udspy.adapter.ChatAdapter","title":"<code>ChatAdapter</code>","text":"<p>Adapter for formatting signatures into OpenAI chat messages.</p> <p>This adapter converts Signature inputs into properly formatted chat messages and parses LLM responses back into structured outputs.</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>class ChatAdapter:\n    \"\"\"Adapter for formatting signatures into OpenAI chat messages.\n\n    This adapter converts Signature inputs into properly formatted\n    chat messages and parses LLM responses back into structured outputs.\n    \"\"\"\n\n    def format_instructions(self, signature: type[Signature]) -&gt; str:\n        \"\"\"Format signature instructions and field descriptions.\n\n        Args:\n            signature: The signature to format\n\n        Returns:\n            Formatted instruction string\n        \"\"\"\n        parts = []\n\n        # Add main instructions\n        instructions = signature.get_instructions()\n        if instructions:\n            parts.append(instructions)\n\n        # Add input field descriptions\n        input_fields = signature.get_input_fields()\n        if input_fields:\n            parts.append(\"\\n**Inputs:**\")\n            for name, field_info in input_fields.items():\n                desc = field_info.description or \"\"\n                parts.append(f\"- `{name}`: {desc}\")\n\n        # Add output field descriptions\n        output_fields = signature.get_output_fields()\n        if output_fields:\n            parts.append(\"\\n**Required Outputs:**\")\n            for name, field_info in output_fields.items():\n                desc = field_info.description or \"\"\n                parts.append(f\"- `{name}`: {desc}\")\n\n        # Add output format instructions\n        parts.append(\"\\n**Output Format:**\\nStructure your response with clear field markers:\\n\")\n        for name in output_fields:\n            parts.append(f\"[[ ## {name} ## ]]\\n&lt;your {name} here&gt;\")\n\n        return \"\\n\".join(parts)\n\n    def format_inputs(\n        self,\n        signature: type[Signature],\n        inputs: dict[str, Any],\n    ) -&gt; str:\n        \"\"\"Format input values into a message.\n\n        Args:\n            signature: The signature defining expected inputs\n            inputs: Dictionary of input values\n\n        Returns:\n            Formatted input string\n        \"\"\"\n        parts = []\n        input_fields = signature.get_input_fields()\n\n        for name, _field_info in input_fields.items():\n            if name in inputs:\n                value = inputs[name]\n                formatted = format_value(value)\n                parts.append(f\"[[ ## {name} ## ]]\\n{formatted}\")\n\n        return \"\\n\\n\".join(parts)\n\n    def parse_outputs(\n        self,\n        signature: type[Signature],\n        completion: str,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Parse LLM completion into structured outputs.\n\n        Args:\n            signature: The signature defining expected outputs\n            completion: Raw completion string from LLM\n\n        Returns:\n            Dictionary of parsed output values\n        \"\"\"\n        output_fields = signature.get_output_fields()\n        outputs: dict[str, Any] = {}\n\n        # Split completion into sections by field markers\n        sections: list[tuple[str | None, list[str]]] = [(None, [])]\n\n        for line in completion.splitlines():\n            # Check for field marker: [[ ## field_name ## ]]\n            if line.strip().startswith(\"[[ ## \") and line.strip().endswith(\" ## ]]\"):\n                field_name = line.strip()[6:-5].strip()\n                sections.append((field_name, []))\n            else:\n                sections[-1][1].append(line)\n\n        # Parse each section\n        for field_name, lines in sections:  # type: ignore[assignment]\n            if field_name and field_name in output_fields:\n                field_info = output_fields[field_name]\n                value_str = \"\\n\".join(lines).strip()\n\n                # Parse according to field type\n                try:\n                    outputs[field_name] = parse_value(value_str, field_info.annotation)  # type: ignore[arg-type]\n                except Exception:\n                    # Fallback: keep as string\n                    outputs[field_name] = value_str\n\n        return outputs\n\n    def format_tool_schemas(self, tools: list[Any]) -&gt; list[dict[str, Any]]:\n        \"\"\"Convert Tool objects or Pydantic models to OpenAI tool schemas.\n\n        Args:\n            tools: List of Tool objects or Pydantic model classes\n\n        Returns:\n            List of OpenAI tool schema dictionaries\n        \"\"\"\n        from udspy.tool import Tool\n\n        tool_schemas = []\n\n        for tool_item in tools:\n            if isinstance(tool_item, Tool):\n                # Tool decorator - use its built-in schema conversion\n                tool_schemas.append(tool_item.to_openai_schema())\n            else:\n                # Pydantic model - convert using existing logic\n                tool_model = tool_item\n                schema = tool_model.model_json_schema()\n\n                # Extract description from docstring or schema\n                description = (\n                    tool_model.__doc__.strip()\n                    if tool_model.__doc__\n                    else schema.get(\"description\", f\"Use {tool_model.__name__}\")\n                )\n\n                # Build OpenAI function schema\n                tool_schema = {\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": schema.get(\"title\", tool_model.__name__),\n                        \"description\": description,\n                        \"parameters\": {\n                            \"type\": \"object\",\n                            \"properties\": schema.get(\"properties\", {}),\n                            \"required\": schema.get(\"required\", []),\n                            \"additionalProperties\": False,\n                        },\n                    },\n                }\n\n                # Remove $defs if present (internal Pydantic references)\n                if \"$defs\" in tool_schema[\"function\"][\"parameters\"]:  # type: ignore[index]\n                    del tool_schema[\"function\"][\"parameters\"][\"$defs\"]  # type: ignore[index]\n\n                tool_schemas.append(tool_schema)\n\n        return tool_schemas\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter-functions","title":"Functions","text":""},{"location":"api/adapter/#udspy.adapter.ChatAdapter.format_inputs","title":"<code>format_inputs(signature, inputs)</code>","text":"<p>Format input values into a message.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature]</code> <p>The signature defining expected inputs</p> required <code>inputs</code> <code>dict[str, Any]</code> <p>Dictionary of input values</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted input string</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def format_inputs(\n    self,\n    signature: type[Signature],\n    inputs: dict[str, Any],\n) -&gt; str:\n    \"\"\"Format input values into a message.\n\n    Args:\n        signature: The signature defining expected inputs\n        inputs: Dictionary of input values\n\n    Returns:\n        Formatted input string\n    \"\"\"\n    parts = []\n    input_fields = signature.get_input_fields()\n\n    for name, _field_info in input_fields.items():\n        if name in inputs:\n            value = inputs[name]\n            formatted = format_value(value)\n            parts.append(f\"[[ ## {name} ## ]]\\n{formatted}\")\n\n    return \"\\n\\n\".join(parts)\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.format_instructions","title":"<code>format_instructions(signature)</code>","text":"<p>Format signature instructions and field descriptions.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature]</code> <p>The signature to format</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted instruction string</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def format_instructions(self, signature: type[Signature]) -&gt; str:\n    \"\"\"Format signature instructions and field descriptions.\n\n    Args:\n        signature: The signature to format\n\n    Returns:\n        Formatted instruction string\n    \"\"\"\n    parts = []\n\n    # Add main instructions\n    instructions = signature.get_instructions()\n    if instructions:\n        parts.append(instructions)\n\n    # Add input field descriptions\n    input_fields = signature.get_input_fields()\n    if input_fields:\n        parts.append(\"\\n**Inputs:**\")\n        for name, field_info in input_fields.items():\n            desc = field_info.description or \"\"\n            parts.append(f\"- `{name}`: {desc}\")\n\n    # Add output field descriptions\n    output_fields = signature.get_output_fields()\n    if output_fields:\n        parts.append(\"\\n**Required Outputs:**\")\n        for name, field_info in output_fields.items():\n            desc = field_info.description or \"\"\n            parts.append(f\"- `{name}`: {desc}\")\n\n    # Add output format instructions\n    parts.append(\"\\n**Output Format:**\\nStructure your response with clear field markers:\\n\")\n    for name in output_fields:\n        parts.append(f\"[[ ## {name} ## ]]\\n&lt;your {name} here&gt;\")\n\n    return \"\\n\".join(parts)\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.format_tool_schemas","title":"<code>format_tool_schemas(tools)</code>","text":"<p>Convert Tool objects or Pydantic models to OpenAI tool schemas.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>list[Any]</code> <p>List of Tool objects or Pydantic model classes</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of OpenAI tool schema dictionaries</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def format_tool_schemas(self, tools: list[Any]) -&gt; list[dict[str, Any]]:\n    \"\"\"Convert Tool objects or Pydantic models to OpenAI tool schemas.\n\n    Args:\n        tools: List of Tool objects or Pydantic model classes\n\n    Returns:\n        List of OpenAI tool schema dictionaries\n    \"\"\"\n    from udspy.tool import Tool\n\n    tool_schemas = []\n\n    for tool_item in tools:\n        if isinstance(tool_item, Tool):\n            # Tool decorator - use its built-in schema conversion\n            tool_schemas.append(tool_item.to_openai_schema())\n        else:\n            # Pydantic model - convert using existing logic\n            tool_model = tool_item\n            schema = tool_model.model_json_schema()\n\n            # Extract description from docstring or schema\n            description = (\n                tool_model.__doc__.strip()\n                if tool_model.__doc__\n                else schema.get(\"description\", f\"Use {tool_model.__name__}\")\n            )\n\n            # Build OpenAI function schema\n            tool_schema = {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": schema.get(\"title\", tool_model.__name__),\n                    \"description\": description,\n                    \"parameters\": {\n                        \"type\": \"object\",\n                        \"properties\": schema.get(\"properties\", {}),\n                        \"required\": schema.get(\"required\", []),\n                        \"additionalProperties\": False,\n                    },\n                },\n            }\n\n            # Remove $defs if present (internal Pydantic references)\n            if \"$defs\" in tool_schema[\"function\"][\"parameters\"]:  # type: ignore[index]\n                del tool_schema[\"function\"][\"parameters\"][\"$defs\"]  # type: ignore[index]\n\n            tool_schemas.append(tool_schema)\n\n    return tool_schemas\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.parse_outputs","title":"<code>parse_outputs(signature, completion)</code>","text":"<p>Parse LLM completion into structured outputs.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature]</code> <p>The signature defining expected outputs</p> required <code>completion</code> <code>str</code> <p>Raw completion string from LLM</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of parsed output values</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def parse_outputs(\n    self,\n    signature: type[Signature],\n    completion: str,\n) -&gt; dict[str, Any]:\n    \"\"\"Parse LLM completion into structured outputs.\n\n    Args:\n        signature: The signature defining expected outputs\n        completion: Raw completion string from LLM\n\n    Returns:\n        Dictionary of parsed output values\n    \"\"\"\n    output_fields = signature.get_output_fields()\n    outputs: dict[str, Any] = {}\n\n    # Split completion into sections by field markers\n    sections: list[tuple[str | None, list[str]]] = [(None, [])]\n\n    for line in completion.splitlines():\n        # Check for field marker: [[ ## field_name ## ]]\n        if line.strip().startswith(\"[[ ## \") and line.strip().endswith(\" ## ]]\"):\n            field_name = line.strip()[6:-5].strip()\n            sections.append((field_name, []))\n        else:\n            sections[-1][1].append(line)\n\n    # Parse each section\n    for field_name, lines in sections:  # type: ignore[assignment]\n        if field_name and field_name in output_fields:\n            field_info = output_fields[field_name]\n            value_str = \"\\n\".join(lines).strip()\n\n            # Parse according to field type\n            try:\n                outputs[field_name] = parse_value(value_str, field_info.annotation)  # type: ignore[arg-type]\n            except Exception:\n                # Fallback: keep as string\n                outputs[field_name] = value_str\n\n    return outputs\n</code></pre>"},{"location":"api/adapter/#udspy.adapter-functions","title":"Functions","text":""},{"location":"api/adapter/#udspy.adapter.format_value","title":"<code>format_value(value)</code>","text":"<p>Format a value for inclusion in a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to format</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted string representation</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def format_value(value: Any) -&gt; str:\n    \"\"\"Format a value for inclusion in a prompt.\n\n    Args:\n        value: The value to format\n\n    Returns:\n        Formatted string representation\n    \"\"\"\n    if isinstance(value, str):\n        return value\n    elif isinstance(value, (list, dict)):\n        return json.dumps(value, indent=2)\n    elif isinstance(value, BaseModel):\n        return value.model_dump_json(indent=2)\n    else:\n        return str(value)\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.parse_value","title":"<code>parse_value(value_str, type_)</code>","text":"<p>Parse a string value into the specified type.</p> <p>Parameters:</p> Name Type Description Default <code>value_str</code> <code>str</code> <p>String value to parse</p> required <code>type_</code> <code>type</code> <p>Target type</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Parsed value</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def parse_value(value_str: str, type_: type) -&gt; Any:\n    \"\"\"Parse a string value into the specified type.\n\n    Args:\n        value_str: String value to parse\n        type_: Target type\n\n    Returns:\n        Parsed value\n    \"\"\"\n    # Handle strings\n    if type_ is str:\n        return value_str.strip()\n\n    # Handle numeric types\n    if type_ is int:\n        return int(value_str.strip())\n    if type_ is float:\n        return float(value_str.strip())\n    if type_ is bool:\n        return value_str.strip().lower() in (\"true\", \"yes\", \"1\")\n\n    # Handle Pydantic models\n    try:\n        if isinstance(type_, type) and issubclass(type_, BaseModel):\n            # Try parsing as JSON first\n            try:\n                data = json.loads(value_str)\n                return type_.model_validate(data)\n            except json.JSONDecodeError:\n                # Fallback: treat as JSON string\n                return type_.model_validate_json(value_str)\n    except (TypeError, ValueError):\n        pass\n\n    # Handle lists and dicts\n    try:\n        parsed = json.loads(value_str)\n        if isinstance(parsed, (list, dict)):\n            return parsed\n    except json.JSONDecodeError:\n        pass\n\n    # Fallback: return as string\n    return value_str.strip()\n</code></pre>"},{"location":"api/history/","title":"History API Reference","text":"<p>The <code>History</code> class manages conversation history for multi-turn interactions.</p>"},{"location":"api/history/#class-history","title":"Class: History","text":"<pre><code>from udspy import History\n</code></pre>"},{"location":"api/history/#constructor","title":"Constructor","text":"<pre><code>History(messages: list[dict[str, Any]] | None = None)\n</code></pre> <p>Create a new History instance.</p> <p>Parameters: - <code>messages</code> (optional): Initial list of messages in OpenAI format</p> <p>Example: <pre><code># Empty history\nhistory = History()\n\n# With initial messages\nhistory = History(messages=[\n    {\"role\": \"system\", \"content\": \"You are helpful\"},\n    {\"role\": \"user\", \"content\": \"Hello\"}\n])\n</code></pre></p>"},{"location":"api/history/#attributes","title":"Attributes","text":""},{"location":"api/history/#messages","title":"messages","text":"<pre><code>history.messages: list[dict[str, Any]]\n</code></pre> <p>List of conversation messages in OpenAI format. Each message is a dictionary with at minimum: - <code>role</code>: One of \"system\", \"user\", \"assistant\", or \"tool\" - <code>content</code>: Message content string</p> <p>Assistant messages with tool calls also include: - <code>tool_calls</code>: List of tool call dictionaries</p> <p>Tool messages also include: - <code>tool_call_id</code>: ID of the tool call this result is for</p> <p>Example: <pre><code>for msg in history.messages:\n    print(f\"{msg['role']}: {msg['content']}\")\n</code></pre></p>"},{"location":"api/history/#methods","title":"Methods","text":""},{"location":"api/history/#add_message","title":"add_message","text":"<pre><code>add_message(\n    role: str,\n    content: str,\n    *,\n    tool_calls: list[dict[str, Any]] | None = None\n) -&gt; None\n</code></pre> <p>Add a message to the history.</p> <p>Parameters: - <code>role</code>: Message role (\"system\", \"user\", \"assistant\", \"tool\") - <code>content</code>: Message content - <code>tool_calls</code> (optional): Tool calls for assistant messages</p> <p>Example: <pre><code>history.add_message(\"user\", \"What is AI?\")\nhistory.add_message(\"assistant\", \"AI is...\", tool_calls=[...])\n</code></pre></p>"},{"location":"api/history/#add_user_message","title":"add_user_message","text":"<pre><code>add_user_message(content: str) -&gt; None\n</code></pre> <p>Add a user message.</p> <p>Parameters: - <code>content</code>: User message content</p> <p>Example: <pre><code>history.add_user_message(\"Tell me about Python\")\n</code></pre></p>"},{"location":"api/history/#add_assistant_message","title":"add_assistant_message","text":"<pre><code>add_assistant_message(\n    content: str,\n    tool_calls: list[dict[str, Any]] | None = None\n) -&gt; None\n</code></pre> <p>Add an assistant message.</p> <p>Parameters: - <code>content</code>: Assistant message content - <code>tool_calls</code> (optional): Tool calls made by assistant</p> <p>Example: <pre><code>history.add_assistant_message(\"Python is a programming language...\")\n</code></pre></p>"},{"location":"api/history/#add_system_message","title":"add_system_message","text":"<pre><code>add_system_message(content: str) -&gt; None\n</code></pre> <p>Add a system message.</p> <p>Parameters: - <code>content</code>: System message content</p> <p>Example: <pre><code>history.add_system_message(\"You are a helpful coding tutor\")\n</code></pre></p>"},{"location":"api/history/#add_tool_result","title":"add_tool_result","text":"<pre><code>add_tool_result(tool_call_id: str, content: str) -&gt; None\n</code></pre> <p>Add a tool result message.</p> <p>Parameters: - <code>tool_call_id</code>: ID of the tool call this result is for - <code>content</code>: Tool result content</p> <p>Example: <pre><code>history.add_tool_result(\"call_123\", \"Result: 42\")\n</code></pre></p>"},{"location":"api/history/#clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear all messages from history.</p> <p>Example: <pre><code>history.clear()\nprint(len(history))  # 0\n</code></pre></p>"},{"location":"api/history/#copy","title":"copy","text":"<pre><code>copy() -&gt; History\n</code></pre> <p>Create a copy of this history.</p> <p>Returns: - New <code>History</code> instance with copied messages</p> <p>Example: <pre><code>branch = history.copy()\n# Modify branch without affecting original\nbranch.add_user_message(\"New question\")\n</code></pre></p>"},{"location":"api/history/#magic-methods","title":"Magic Methods","text":""},{"location":"api/history/#__len__","title":"<code>__len__</code>","text":"<pre><code>len(history) -&gt; int\n</code></pre> <p>Get number of messages in history.</p> <p>Example: <pre><code>print(len(history))  # e.g., 5\n</code></pre></p>"},{"location":"api/history/#__repr__","title":"<code>__repr__</code>","text":"<pre><code>repr(history) -&gt; str\n</code></pre> <p>String representation showing number of messages.</p> <p>Example: <pre><code>print(repr(history))  # \"History(5 messages)\"\n</code></pre></p>"},{"location":"api/history/#__str__","title":"<code>__str__</code>","text":"<pre><code>str(history) -&gt; str\n</code></pre> <p>Human-readable formatted conversation history.</p> <p>Example: <pre><code>print(history)\n# History (3 messages):\n#   1. [user] What is Python?\n#   2. [assistant] Python is a programming language...\n#   3. [user] What are its features?\n</code></pre></p>"},{"location":"api/history/#usage-with-predict","title":"Usage with Predict","text":"<p>History integrates seamlessly with <code>Predict</code>:</p> <pre><code>from udspy import Predict, History\n\npredictor = Predict(QA)\nhistory = History()\n\n# History is automatically updated with each call\nresult = predictor(question=\"First question\", history=history)\nresult = predictor(question=\"Follow-up question\", history=history)\n</code></pre> <p>See History Examples for more usage patterns.</p>"},{"location":"api/module/","title":"API Reference: Modules","text":""},{"location":"api/module/#udspy.module","title":"<code>udspy.module</code>","text":"<p>Module package for composable LLM calls.</p>"},{"location":"api/module/#udspy.module-classes","title":"Classes","text":""},{"location":"api/module/#udspy.module.ChainOfThought","title":"<code>ChainOfThought</code>","text":"<p>               Bases: <code>Module</code></p> <p>Chain of Thought reasoning module.</p> <p>Automatically adds a reasoning step before generating outputs. This encourages the LLM to think step-by-step, improving answer quality.</p> Example <pre><code>class QA(Signature):\n    '''Answer questions.'''\n    question: str = InputField()\n    answer: str = OutputField()\n\n# Creates predictor with automatic reasoning\npredictor = ChainOfThought(QA)\nresult = predictor(question=\"What is 2+2?\")\n\nprint(result.reasoning)  # \"Let's think step by step...\"\nprint(result.answer)     # \"4\"\n</code></pre> Source code in <code>src/udspy/module/chain_of_thought.py</code> <pre><code>class ChainOfThought(Module):\n    \"\"\"Chain of Thought reasoning module.\n\n    Automatically adds a reasoning step before generating outputs.\n    This encourages the LLM to think step-by-step, improving answer quality.\n\n    Example:\n        ```python\n        class QA(Signature):\n            '''Answer questions.'''\n            question: str = InputField()\n            answer: str = OutputField()\n\n        # Creates predictor with automatic reasoning\n        predictor = ChainOfThought(QA)\n        result = predictor(question=\"What is 2+2?\")\n\n        print(result.reasoning)  # \"Let's think step by step...\"\n        print(result.answer)     # \"4\"\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        signature: type[Signature],\n        *,\n        reasoning_description: str = \"Step-by-step reasoning process\",\n        model: str | None = None,\n        tools: list[type] | None = None,\n        adapter: ChatAdapter | None = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Initialize a Chain of Thought module.\n\n        Args:\n            signature: Signature defining inputs and final outputs\n            reasoning_description: Description for the reasoning field\n            model: Model name (overrides global default)\n            tools: List of Pydantic tool models\n            adapter: Custom adapter\n            **kwargs: Additional arguments for chat completion\n        \"\"\"\n        self.original_signature = signature\n\n        # Create extended signature with reasoning field\n        input_fields = {\n            name: field.annotation for name, field in signature.get_input_fields().items()\n        }\n        output_fields = {\n            name: field.annotation for name, field in signature.get_output_fields().items()\n        }\n\n        # Prepend reasoning to outputs\n        extended_outputs = {\"reasoning\": str, **output_fields}\n\n        # Create new signature with reasoning\n        extended_signature = make_signature(\n            input_fields,  # type: ignore[arg-type]\n            extended_outputs,  # type: ignore[arg-type]\n            signature.get_instructions(),\n        )\n\n        # Override reasoning field description\n        extended_signature.model_fields[\"reasoning\"].description = reasoning_description\n\n        # Create predictor with extended signature\n        self.predict = Predict(\n            extended_signature,\n            model=model,\n            tools=tools,\n            adapter=adapter,\n            **kwargs,\n        )\n\n    async def astream(self, **inputs: Any) -&gt; AsyncGenerator[StreamEvent, None]:\n        \"\"\"Stream chain of thought prediction with reasoning.\n\n        Delegates to the wrapped Predict module's astream method, which yields\n        StreamEvent objects including reasoning field.\n\n        Args:\n            **inputs: Input values matching the signature's input fields\n\n        Yields:\n            StreamEvent objects (StreamChunk with reasoning and other fields,\n            final Prediction with all outputs including reasoning)\n        \"\"\"\n        async for event in self.predict.astream(**inputs):\n            yield event\n</code></pre>"},{"location":"api/module/#udspy.module.ChainOfThought-functions","title":"Functions","text":""},{"location":"api/module/#udspy.module.ChainOfThought.__init__","title":"<code>__init__(signature, *, reasoning_description='Step-by-step reasoning process', model=None, tools=None, adapter=None, **kwargs)</code>","text":"<p>Initialize a Chain of Thought module.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature]</code> <p>Signature defining inputs and final outputs</p> required <code>reasoning_description</code> <code>str</code> <p>Description for the reasoning field</p> <code>'Step-by-step reasoning process'</code> <code>model</code> <code>str | None</code> <p>Model name (overrides global default)</p> <code>None</code> <code>tools</code> <code>list[type] | None</code> <p>List of Pydantic tool models</p> <code>None</code> <code>adapter</code> <code>ChatAdapter | None</code> <p>Custom adapter</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for chat completion</p> <code>{}</code> Source code in <code>src/udspy/module/chain_of_thought.py</code> <pre><code>def __init__(\n    self,\n    signature: type[Signature],\n    *,\n    reasoning_description: str = \"Step-by-step reasoning process\",\n    model: str | None = None,\n    tools: list[type] | None = None,\n    adapter: ChatAdapter | None = None,\n    **kwargs: Any,\n):\n    \"\"\"Initialize a Chain of Thought module.\n\n    Args:\n        signature: Signature defining inputs and final outputs\n        reasoning_description: Description for the reasoning field\n        model: Model name (overrides global default)\n        tools: List of Pydantic tool models\n        adapter: Custom adapter\n        **kwargs: Additional arguments for chat completion\n    \"\"\"\n    self.original_signature = signature\n\n    # Create extended signature with reasoning field\n    input_fields = {\n        name: field.annotation for name, field in signature.get_input_fields().items()\n    }\n    output_fields = {\n        name: field.annotation for name, field in signature.get_output_fields().items()\n    }\n\n    # Prepend reasoning to outputs\n    extended_outputs = {\"reasoning\": str, **output_fields}\n\n    # Create new signature with reasoning\n    extended_signature = make_signature(\n        input_fields,  # type: ignore[arg-type]\n        extended_outputs,  # type: ignore[arg-type]\n        signature.get_instructions(),\n    )\n\n    # Override reasoning field description\n    extended_signature.model_fields[\"reasoning\"].description = reasoning_description\n\n    # Create predictor with extended signature\n    self.predict = Predict(\n        extended_signature,\n        model=model,\n        tools=tools,\n        adapter=adapter,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/module/#udspy.module.ChainOfThought.astream","title":"<code>astream(**inputs)</code>  <code>async</code>","text":"<p>Stream chain of thought prediction with reasoning.</p> <p>Delegates to the wrapped Predict module's astream method, which yields StreamEvent objects including reasoning field.</p> <p>Parameters:</p> Name Type Description Default <code>**inputs</code> <code>Any</code> <p>Input values matching the signature's input fields</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[StreamEvent, None]</code> <p>StreamEvent objects (StreamChunk with reasoning and other fields,</p> <code>AsyncGenerator[StreamEvent, None]</code> <p>final Prediction with all outputs including reasoning)</p> Source code in <code>src/udspy/module/chain_of_thought.py</code> <pre><code>async def astream(self, **inputs: Any) -&gt; AsyncGenerator[StreamEvent, None]:\n    \"\"\"Stream chain of thought prediction with reasoning.\n\n    Delegates to the wrapped Predict module's astream method, which yields\n    StreamEvent objects including reasoning field.\n\n    Args:\n        **inputs: Input values matching the signature's input fields\n\n    Yields:\n        StreamEvent objects (StreamChunk with reasoning and other fields,\n        final Prediction with all outputs including reasoning)\n    \"\"\"\n    async for event in self.predict.astream(**inputs):\n        yield event\n</code></pre>"},{"location":"api/module/#udspy.module.Module","title":"<code>Module</code>","text":"<p>Base class for all udspy modules.</p> <p>Modules are composable async-first units. The core method is <code>astream()</code> which yields StreamEvent objects. Sync wrappers are provided for convenience.</p> <p>Subclasses should implement <code>astream()</code> to define their behavior.</p> Example <pre><code># Async streaming (real-time)\nasync for event in module.astream(question=\"What is AI?\"):\n    if isinstance(event, StreamChunk):\n        print(event.delta, end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        result = event\n\n# Async non-streaming\nresult = await module.aforward(question=\"What is AI?\")\n\n# Sync (for scripts, notebooks)\nresult = module(question=\"What is AI?\")\nresult = module.forward(question=\"What is AI?\")\n</code></pre> Source code in <code>src/udspy/module/base.py</code> <pre><code>class Module:\n    \"\"\"Base class for all udspy modules.\n\n    Modules are composable async-first units. The core method is `astream()`\n    which yields StreamEvent objects. Sync wrappers are provided for convenience.\n\n    Subclasses should implement `astream()` to define their behavior.\n\n    Example:\n        ```python\n        # Async streaming (real-time)\n        async for event in module.astream(question=\"What is AI?\"):\n            if isinstance(event, StreamChunk):\n                print(event.delta, end=\"\", flush=True)\n            elif isinstance(event, Prediction):\n                result = event\n\n        # Async non-streaming\n        result = await module.aforward(question=\"What is AI?\")\n\n        # Sync (for scripts, notebooks)\n        result = module(question=\"What is AI?\")\n        result = module.forward(question=\"What is AI?\")\n        ```\n    \"\"\"\n\n    async def astream(self, **inputs: Any) -&gt; AsyncGenerator[StreamEvent, None]:\n        \"\"\"Core async streaming method. Must be implemented by subclasses.\n\n        This is the fundamental method that all modules must implement.\n        It yields StreamEvent objects (including StreamChunk and Prediction).\n\n        Args:\n            **inputs: Input values for the module\n\n        Yields:\n            StreamEvent objects (StreamChunk for incremental output,\n            Prediction for final result, and any custom events)\n\n        Raises:\n            NotImplementedError: If not implemented by subclass\n        \"\"\"\n        raise NotImplementedError(f\"{self.__class__.__name__} must implement astream() method\")\n        # Make this a generator to match the return type\n        yield  # This line will never execute but satisfies the generator type\n\n    async def aforward(self, **inputs: Any) -&gt; Prediction:\n        \"\"\"Async non-streaming method. Consumes astream() and returns final result.\n\n        This is a convenience method that collects all events from astream()\n        and returns only the final Prediction. Override this if you need\n        custom non-streaming behavior.\n\n        Args:\n            **inputs: Input values for the module\n\n        Returns:\n            Final Prediction object\n        \"\"\"\n        async for event in self.astream(**inputs):\n            if isinstance(event, Prediction):\n                return event\n\n        raise RuntimeError(f\"{self.__class__.__name__}.astream() did not yield a Prediction\")\n\n    def forward(self, **inputs: Any) -&gt; Prediction:\n        \"\"\"Sync non-streaming method. Wraps aforward() with async_to_sync.\n\n        This provides sync compatibility for scripts and notebooks. Cannot be\n        called from within an async context (use aforward() instead).\n\n        Args:\n            **inputs: Input values for the module (includes both input fields\n                and any module-specific parameters like auto_execute_tools)\n\n        Returns:\n            Final Prediction object\n\n        Raises:\n            RuntimeError: If called from within an async context\n        \"\"\"\n        # Check if we're already in an async context\n        try:\n            asyncio.get_running_loop()\n            raise RuntimeError(\n                f\"Cannot call {self.__class__.__name__}.forward() from async context. \"\n                f\"Use 'await {self.__class__.__name__[0].lower() + self.__class__.__name__[1:]}.aforward(...)' instead.\"\n            )\n        except RuntimeError as e:\n            # No running loop - we're in sync context, proceed\n            if \"no running event loop\" not in str(e).lower():\n                raise\n\n        # Run async code from sync context\n        return asyncio.run(self.aforward(**inputs))\n\n    def __call__(self, **inputs: Any) -&gt; Prediction:\n        \"\"\"Sync convenience method. Calls forward().\n\n        Args:\n            **inputs: Input values for the module\n\n        Returns:\n            Final Prediction object\n        \"\"\"\n        return self.forward(**inputs)\n</code></pre>"},{"location":"api/module/#udspy.module.Module-functions","title":"Functions","text":""},{"location":"api/module/#udspy.module.Module.__call__","title":"<code>__call__(**inputs)</code>","text":"<p>Sync convenience method. Calls forward().</p> <p>Parameters:</p> Name Type Description Default <code>**inputs</code> <code>Any</code> <p>Input values for the module</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>def __call__(self, **inputs: Any) -&gt; Prediction:\n    \"\"\"Sync convenience method. Calls forward().\n\n    Args:\n        **inputs: Input values for the module\n\n    Returns:\n        Final Prediction object\n    \"\"\"\n    return self.forward(**inputs)\n</code></pre>"},{"location":"api/module/#udspy.module.Module.aforward","title":"<code>aforward(**inputs)</code>  <code>async</code>","text":"<p>Async non-streaming method. Consumes astream() and returns final result.</p> <p>This is a convenience method that collects all events from astream() and returns only the final Prediction. Override this if you need custom non-streaming behavior.</p> <p>Parameters:</p> Name Type Description Default <code>**inputs</code> <code>Any</code> <p>Input values for the module</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>async def aforward(self, **inputs: Any) -&gt; Prediction:\n    \"\"\"Async non-streaming method. Consumes astream() and returns final result.\n\n    This is a convenience method that collects all events from astream()\n    and returns only the final Prediction. Override this if you need\n    custom non-streaming behavior.\n\n    Args:\n        **inputs: Input values for the module\n\n    Returns:\n        Final Prediction object\n    \"\"\"\n    async for event in self.astream(**inputs):\n        if isinstance(event, Prediction):\n            return event\n\n    raise RuntimeError(f\"{self.__class__.__name__}.astream() did not yield a Prediction\")\n</code></pre>"},{"location":"api/module/#udspy.module.Module.astream","title":"<code>astream(**inputs)</code>  <code>async</code>","text":"<p>Core async streaming method. Must be implemented by subclasses.</p> <p>This is the fundamental method that all modules must implement. It yields StreamEvent objects (including StreamChunk and Prediction).</p> <p>Parameters:</p> Name Type Description Default <code>**inputs</code> <code>Any</code> <p>Input values for the module</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[StreamEvent, None]</code> <p>StreamEvent objects (StreamChunk for incremental output,</p> <code>AsyncGenerator[StreamEvent, None]</code> <p>Prediction for final result, and any custom events)</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by subclass</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>async def astream(self, **inputs: Any) -&gt; AsyncGenerator[StreamEvent, None]:\n    \"\"\"Core async streaming method. Must be implemented by subclasses.\n\n    This is the fundamental method that all modules must implement.\n    It yields StreamEvent objects (including StreamChunk and Prediction).\n\n    Args:\n        **inputs: Input values for the module\n\n    Yields:\n        StreamEvent objects (StreamChunk for incremental output,\n        Prediction for final result, and any custom events)\n\n    Raises:\n        NotImplementedError: If not implemented by subclass\n    \"\"\"\n    raise NotImplementedError(f\"{self.__class__.__name__} must implement astream() method\")\n    # Make this a generator to match the return type\n    yield  # This line will never execute but satisfies the generator type\n</code></pre>"},{"location":"api/module/#udspy.module.Module.forward","title":"<code>forward(**inputs)</code>","text":"<p>Sync non-streaming method. Wraps aforward() with async_to_sync.</p> <p>This provides sync compatibility for scripts and notebooks. Cannot be called from within an async context (use aforward() instead).</p> <p>Parameters:</p> Name Type Description Default <code>**inputs</code> <code>Any</code> <p>Input values for the module (includes both input fields and any module-specific parameters like auto_execute_tools)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called from within an async context</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>def forward(self, **inputs: Any) -&gt; Prediction:\n    \"\"\"Sync non-streaming method. Wraps aforward() with async_to_sync.\n\n    This provides sync compatibility for scripts and notebooks. Cannot be\n    called from within an async context (use aforward() instead).\n\n    Args:\n        **inputs: Input values for the module (includes both input fields\n            and any module-specific parameters like auto_execute_tools)\n\n    Returns:\n        Final Prediction object\n\n    Raises:\n        RuntimeError: If called from within an async context\n    \"\"\"\n    # Check if we're already in an async context\n    try:\n        asyncio.get_running_loop()\n        raise RuntimeError(\n            f\"Cannot call {self.__class__.__name__}.forward() from async context. \"\n            f\"Use 'await {self.__class__.__name__[0].lower() + self.__class__.__name__[1:]}.aforward(...)' instead.\"\n        )\n    except RuntimeError as e:\n        # No running loop - we're in sync context, proceed\n        if \"no running event loop\" not in str(e).lower():\n            raise\n\n    # Run async code from sync context\n    return asyncio.run(self.aforward(**inputs))\n</code></pre>"},{"location":"api/module/#udspy.module.Predict","title":"<code>Predict</code>","text":"<p>               Bases: <code>Module</code></p> <p>Module for making LLM predictions based on a signature.</p> <p>This is an async-first module. The core method is <code>astream()</code> which yields StreamEvent objects. Use <code>aforward()</code> for async non-streaming, or <code>forward()</code> for sync usage.</p> Example <pre><code>from udspy import Predict, Signature, InputField, OutputField\n\nclass QA(Signature):\n    '''Answer questions.'''\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\n\n# Sync usage\nresult = predictor(question=\"What is 2+2?\")\nprint(result.answer)\n\n# Async non-streaming\nresult = await predictor.aforward(question=\"What is 2+2?\")\n\n# Async streaming\nasync for event in predictor.astream(question=\"What is 2+2?\"):\n    if isinstance(event, StreamChunk):\n        print(event.delta, end=\"\", flush=True)\n</code></pre> Source code in <code>src/udspy/module/predict.py</code> <pre><code>class Predict(Module):\n    \"\"\"Module for making LLM predictions based on a signature.\n\n    This is an async-first module. The core method is `astream()` which yields\n    StreamEvent objects. Use `aforward()` for async non-streaming, or `forward()`\n    for sync usage.\n\n    Example:\n        ```python\n        from udspy import Predict, Signature, InputField, OutputField\n\n        class QA(Signature):\n            '''Answer questions.'''\n            question: str = InputField()\n            answer: str = OutputField()\n\n        predictor = Predict(QA)\n\n        # Sync usage\n        result = predictor(question=\"What is 2+2?\")\n        print(result.answer)\n\n        # Async non-streaming\n        result = await predictor.aforward(question=\"What is 2+2?\")\n\n        # Async streaming\n        async for event in predictor.astream(question=\"What is 2+2?\"):\n            if isinstance(event, StreamChunk):\n                print(event.delta, end=\"\", flush=True)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        signature: type[Signature],\n        *,\n        model: str | None = None,\n        tools: list[Any] | None = None,\n        max_turns: int = 5,\n        adapter: ChatAdapter | None = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Initialize a Predict module.\n\n        Args:\n            signature: Signature defining inputs and outputs\n            model: Model name (overrides global default)\n            tools: List of tool functions (decorated with @tool) or Pydantic models\n            max_turns: Maximum number of LLM calls for tool execution loop (default: 5)\n            adapter: Custom adapter (defaults to ChatAdapter)\n            **kwargs: Additional arguments for chat completion (temperature, etc.)\n        \"\"\"\n        from udspy.tool import Tool\n\n        self.signature = signature\n        self.model = model or settings.default_model\n        self.max_turns = max_turns\n        self.adapter = adapter or ChatAdapter()\n        self.kwargs = {**settings.default_kwargs, **kwargs}\n\n        # Process tools - separate Tool objects from Pydantic models\n        self.tool_callables: dict[str, Tool] = {}\n        self.tool_schemas: list[Any] = []\n\n        for tool in tools or []:\n            if isinstance(tool, Tool):\n                # Tool decorator - store both callable and schema\n                self.tool_callables[tool.name] = tool\n                self.tool_schemas.append(tool)\n            else:\n                # Pydantic model - just schema (no automatic execution)\n                self.tool_schemas.append(tool)\n\n    async def astream(\n        self, *, auto_execute_tools: bool = True, history: Optional[\"History\"] = None, **inputs: Any\n    ) -&gt; AsyncGenerator[StreamEvent, None]:\n        \"\"\"Core async streaming method with optional automatic tool execution.\n\n        Yields StreamEvent objects. When auto_execute_tools=True (default),\n        automatically handles multi-turn conversation when tools are present.\n        When False, returns Prediction with tool_calls for manual handling.\n\n        Args:\n            auto_execute_tools: If True, automatically execute tools and continue\n                conversation. If False, return Prediction with tool_calls for\n                manual execution. Default: True.\n            history: Optional History object for multi-turn conversations. When\n                provided, conversation history is automatically managed.\n            **inputs: Input values matching the signature's input fields\n\n        Yields:\n            StreamEvent objects (StreamChunk, Prediction, custom events)\n\n        Raises:\n            ValueError: If required inputs are missing\n        \"\"\"\n        # Validate and build initial messages\n        self._validate_inputs(inputs)\n        messages = self._build_initial_messages(inputs, history)\n\n        # Multi-turn loop for tool execution\n        async for event in self._execute_with_tools(messages, auto_execute_tools, history):\n            yield event\n\n    def _validate_inputs(self, inputs: dict[str, Any]) -&gt; None:\n        \"\"\"Validate that all required inputs are provided.\"\"\"\n        input_fields = self.signature.get_input_fields()\n        for field_name in input_fields:\n            if field_name not in inputs:\n                raise ValueError(f\"Missing required input field: {field_name}\")\n\n    def _build_initial_messages(\n        self, inputs: dict[str, Any], history: Any = None\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Build initial messages from inputs and optional history.\n\n        Args:\n            inputs: Input values from user\n            history: Optional History object with existing conversation\n\n        Returns:\n            List of messages including history (if provided) and new user input\n        \"\"\"\n        from udspy.history import History\n\n        messages: list[dict[str, Any]] = []\n\n        # Start with history if provided\n        if history is not None:\n            if isinstance(history, History):\n                # Copy existing messages from history\n                messages.extend(history.messages)\n            else:\n                raise TypeError(f\"history must be a History object, got {type(history)}\")\n\n        # Add system message if not in history\n        if not messages or messages[0][\"role\"] != \"system\":\n            messages.insert(\n                0, {\"role\": \"system\", \"content\": self.adapter.format_instructions(self.signature)}\n            )\n\n        # Add new user input\n        user_content = self.adapter.format_inputs(self.signature, inputs)\n        user_msg = {\"role\": \"user\", \"content\": user_content}\n        messages.append(user_msg)\n\n        # Also add to history if provided\n        if history is not None and isinstance(history, History):\n            history.messages.append(user_msg)\n\n        return messages\n\n    async def _execute_with_tools(\n        self, messages: list[dict[str, Any]], auto_execute_tools: bool, history: Any = None\n    ) -&gt; AsyncGenerator[StreamEvent, None]:\n        \"\"\"Execute multi-turn conversation with optional automatic tool execution.\n\n        Args:\n            messages: Conversation messages\n            auto_execute_tools: If True, automatically execute tools. If False,\n                return after first tool call.\n            history: Optional History object to update with conversation\n        \"\"\"\n        for turn in range(self.max_turns):\n            # Stream one LLM turn\n            final_prediction = None\n            async for event in self._stream_one_turn(messages, turn):\n                if isinstance(event, Prediction):\n                    final_prediction = event\n                yield event\n\n            # Update history with prediction if provided\n            if history is not None and final_prediction:\n                self._update_history_with_prediction(history, final_prediction)\n\n            # Check if we have tool calls\n            if not (final_prediction and \"tool_calls\" in final_prediction):\n                break  # No tools requested, we're done\n\n            # If not auto-executing, stop here and return the tool_calls\n            if not auto_execute_tools:\n                break  # User will handle tool calls manually\n\n            # Check if we can execute (need tool_callables)\n            if not self.tool_callables:\n                break  # No executables, stop here\n\n            # Execute tools and add results to messages\n            self._execute_tool_calls(messages, final_prediction.tool_calls, history)\n\n        # Check if we exceeded max turns\n        if turn &gt;= self.max_turns - 1 and final_prediction and \"tool_calls\" in final_prediction:\n            raise RuntimeError(f\"Max turns ({self.max_turns}) reached without final answer\")\n\n    async def _stream_one_turn(\n        self, messages: list[dict[str, Any]], turn: int\n    ) -&gt; AsyncGenerator[StreamEvent, None]:\n        \"\"\"Stream one LLM turn.\"\"\"\n        completion_kwargs: dict[str, Any] = {\n            \"model\": self.model,\n            \"messages\": messages,\n            \"stream\": True,\n            **self.kwargs,\n        }\n\n        # Add tools on first turn\n        if turn == 0 and self.tool_schemas:\n            tool_schemas = self.adapter.format_tool_schemas(self.tool_schemas)\n            completion_kwargs[\"tools\"] = tool_schemas\n\n        async for event in self._stream_with_queue(completion_kwargs):\n            yield event\n\n    def _execute_tool_calls(\n        self, messages: list[dict[str, Any]], tool_calls: list[dict[str, Any]], history: Any = None\n    ) -&gt; None:\n        \"\"\"Execute tool calls and add results to messages.\n\n        Args:\n            messages: Conversation messages\n            tool_calls: List of tool calls to execute\n            history: Optional History object to update\n        \"\"\"\n        import json\n\n        # Add assistant message with tool calls\n        assistant_msg = {\n            \"role\": \"assistant\",\n            \"content\": \"\",\n            \"tool_calls\": [\n                {\n                    \"id\": tc[\"id\"],\n                    \"type\": \"function\",\n                    \"function\": {\"name\": tc[\"name\"], \"arguments\": tc[\"arguments\"]},\n                }\n                for tc in tool_calls\n            ],\n        }\n        messages.append(assistant_msg)\n\n        # Update history if provided\n        if history is not None:\n            from udspy.history import History\n\n            if isinstance(history, History):\n                history.messages.append(assistant_msg)\n\n        # Execute each tool\n        for tool_call in tool_calls:\n            tool_name = tool_call[\"name\"]\n            arguments = json.loads(tool_call[\"arguments\"])\n\n            if tool_name in self.tool_callables:\n                try:\n                    result = self.tool_callables[tool_name](**arguments)\n                    content = str(result)\n                except Exception as e:\n                    content = f\"Error executing tool: {e}\"\n            else:\n                content = f\"Error: Tool {tool_name} not found\"\n\n            tool_msg = {\"role\": \"tool\", \"tool_call_id\": tool_call[\"id\"], \"content\": content}\n            messages.append(tool_msg)\n\n            # Update history if provided\n            if history is not None:\n                from udspy.history import History\n\n                if isinstance(history, History):\n                    history.messages.append(tool_msg)\n\n    def _update_history_with_prediction(self, history: Any, prediction: Prediction) -&gt; None:\n        \"\"\"Update history with assistant's prediction.\n\n        Args:\n            history: History object to update\n            prediction: Prediction from assistant\n        \"\"\"\n        from udspy.history import History\n\n        if not isinstance(history, History):\n            return\n\n        # Build content from prediction output fields\n        output_fields = self.signature.get_output_fields()\n        content_parts = []\n\n        for field_name in output_fields:\n            if hasattr(prediction, field_name):\n                value = getattr(prediction, field_name)\n                if value:\n                    content_parts.append(f\"[[ ## {field_name} ## ]]\\n{value}\")\n\n        content = \"\\n\".join(content_parts) if content_parts else \"\"\n\n        # Check if this prediction has tool_calls\n        if hasattr(prediction, \"tool_calls\") and prediction.tool_calls:\n            # Don't add to history yet - will be added in _execute_tool_calls\n            pass\n        else:\n            # Regular assistant message\n            history.add_assistant_message(content)\n\n    async def aforward(\n        self, *, auto_execute_tools: bool = True, history: Any = None, **inputs: Any\n    ) -&gt; Prediction:\n        \"\"\"Async non-streaming method. Returns the final Prediction.\n\n        When tools are used with auto_execute_tools=True (default), this returns\n        the LAST prediction (after tool execution), not the first one (which might\n        only contain tool_calls). When auto_execute_tools=False, returns the first\n        Prediction with tool_calls for manual handling.\n\n        Args:\n            auto_execute_tools: If True, automatically execute tools and return\n                final answer. If False, return Prediction with tool_calls for\n                manual execution. Default: True.\n            history: Optional History object for multi-turn conversations.\n            **inputs: Input values for the module\n\n        Returns:\n            Final Prediction object (after all tool executions if auto_execute_tools=True)\n        \"\"\"\n        final_prediction: Prediction | None = None\n        async for event in self.astream(\n            auto_execute_tools=auto_execute_tools, history=history, **inputs\n        ):\n            if isinstance(event, Prediction):\n                final_prediction = event  # Keep updating to get the last one\n\n        if final_prediction is None:\n            raise RuntimeError(f\"{self.__class__.__name__}.astream() did not yield a Prediction\")\n\n        return final_prediction\n\n    def _process_tool_call_delta(\n        self, tool_calls: dict[int, dict[str, Any]], delta_tool_calls: list[Any]\n    ) -&gt; None:\n        \"\"\"Process tool call deltas and accumulate them.\n\n        Args:\n            tool_calls: Dictionary to accumulate tool calls in\n            delta_tool_calls: List of tool call deltas from the chunk\n        \"\"\"\n        for tool_call in delta_tool_calls:\n            idx = tool_call.index\n            if idx not in tool_calls:\n                tool_calls[idx] = {\n                    \"id\": tool_call.id or \"\",\n                    \"type\": tool_call.type or \"function\",\n                    \"function\": {\n                        \"name\": tool_call.function.name if tool_call.function else \"\",\n                        \"arguments\": \"\",\n                    },\n                }\n\n            # Accumulate function arguments\n            if tool_call.function and tool_call.function.arguments:\n                tool_calls[idx][\"function\"][\"arguments\"] += tool_call.function.arguments\n\n    async def _process_content_delta(\n        self,\n        delta: str,\n        acc_delta: str,\n        current_field: str | None,\n        accumulated_content: dict[str, list[str]],\n        output_fields: dict[str, Any],\n        field_pattern: re.Pattern[str],\n        queue: asyncio.Queue[StreamEvent | None],\n    ) -&gt; tuple[str, str | None]:\n        \"\"\"Process content delta and stream field chunks.\n\n        Args:\n            delta: New content delta\n            acc_delta: Accumulated delta so far\n            current_field: Current field being processed\n            accumulated_content: Dictionary of accumulated content per field\n            output_fields: Output fields from signature\n            field_pattern: Regex pattern for field markers\n            queue: Event queue to put chunks in\n\n        Returns:\n            Tuple of (updated acc_delta, updated current_field)\n        \"\"\"\n        acc_delta += delta\n\n        if not acc_delta:\n            return acc_delta, current_field\n\n        # Check for field markers\n        match = field_pattern.search(acc_delta)\n        if match:\n            # Entering a new field\n            if current_field:\n                # Mark previous field as complete\n                field_content = \"\".join(accumulated_content[current_field])\n                await queue.put(\n                    StreamChunk(self, current_field, \"\", field_content, is_complete=True)\n                )\n\n            current_field = match.group(1)\n            # Remove the marker from content\n            acc_delta = field_pattern.sub(\"\", acc_delta)\n            if acc_delta.startswith(\"\\n\"):\n                acc_delta = acc_delta[1:]\n\n        # Stream content for current field\n        if (\n            current_field\n            and current_field in output_fields\n            and not field_pattern.match(acc_delta, partial=True)\n        ):\n            accumulated_content[current_field].append(acc_delta)\n            field_content = \"\".join(accumulated_content[current_field])\n            await queue.put(\n                StreamChunk(self, current_field, acc_delta, field_content, is_complete=False)\n            )\n            acc_delta = \"\"\n\n        return acc_delta, current_field\n\n    async def _process_llm_stream(\n        self, queue: asyncio.Queue[StreamEvent | None], completion_kwargs: dict[str, Any]\n    ) -&gt; None:\n        \"\"\"Background task to process LLM stream and put events in queue.\n\n        Args:\n            queue: Event queue to put events in\n            completion_kwargs: Arguments for the completion API call\n        \"\"\"\n        try:\n            # Make streaming API call\n            client = settings.aclient\n            stream: AsyncStream[ChatCompletionChunk] = await client.chat.completions.create(\n                **completion_kwargs\n            )\n\n            # Initialize streaming state\n            output_fields = self.signature.get_output_fields()\n            field_pattern = re.compile(r\"\\[\\[ ## (\\w+) ## \\]\\]\")\n            current_field: str | None = None\n            accumulated_content: dict[str, list[str]] = {name: [] for name in output_fields}\n            full_completion: list[str] = []\n            acc_delta: str = \"\"\n            tool_calls: dict[int, dict[str, Any]] = {}\n\n            # Process stream\n            async for chunk in stream:\n                choice = chunk.choices[0]\n\n                # Handle tool calls\n                if choice.delta.tool_calls:\n                    self._process_tool_call_delta(tool_calls, choice.delta.tool_calls)\n\n                # Handle content\n                delta = choice.delta.content or \"\"\n                if delta:\n                    full_completion.append(delta)\n                    acc_delta, current_field = await self._process_content_delta(\n                        delta,\n                        acc_delta,\n                        current_field,\n                        accumulated_content,\n                        output_fields,\n                        field_pattern,\n                        queue,\n                    )\n\n            # Mark last field as complete\n            if current_field:\n                field_content = \"\".join(accumulated_content[current_field])\n                await queue.put(\n                    StreamChunk(self, current_field, \"\", field_content, is_complete=True)\n                )\n\n            # Parse final outputs\n            completion_text = \"\".join(full_completion)\n            outputs = self.adapter.parse_outputs(self.signature, completion_text)\n\n            # Add tool calls if present\n            if tool_calls:\n                outputs[\"tool_calls\"] = [\n                    {\n                        \"id\": tc[\"id\"],\n                        \"name\": tc[\"function\"][\"name\"],\n                        \"arguments\": tc[\"function\"][\"arguments\"],\n                    }\n                    for tc in tool_calls.values()\n                ]\n\n            # Put final prediction\n            await queue.put(Prediction(**outputs))\n\n        except Exception as e:\n            # On error, put exception in queue\n            import traceback\n\n            error_event = type(\n                \"StreamError\",\n                (StreamEvent,),\n                {\"error\": str(e), \"traceback\": traceback.format_exc()},\n            )()\n            await queue.put(error_event)\n        finally:\n            # Signal completion with sentinel\n            await queue.put(None)\n\n    async def _stream_with_queue(\n        self, completion_kwargs: dict[str, Any]\n    ) -&gt; AsyncGenerator[StreamEvent, None]:\n        \"\"\"Internal method to handle streaming with event queue.\n\n        This sets up the event queue context and processes both LLM stream\n        and tool events concurrently.\n        \"\"\"\n        # Create event queue for this stream\n        queue: asyncio.Queue[StreamEvent | None] = asyncio.Queue()\n        token = _stream_queue.set(queue)\n\n        try:\n            # Start background LLM processing\n            llm_task = asyncio.create_task(self._process_llm_stream(queue, completion_kwargs))\n\n            # Yield events from queue until sentinel\n            while True:\n                event = await queue.get()\n                if event is None:  # Sentinel - stream complete\n                    break\n                yield event\n\n            # Wait for background task to complete\n            await llm_task\n\n        finally:\n            # Clean up context\n            _stream_queue.reset(token)\n</code></pre>"},{"location":"api/module/#udspy.module.Predict-functions","title":"Functions","text":""},{"location":"api/module/#udspy.module.Predict.__init__","title":"<code>__init__(signature, *, model=None, tools=None, max_turns=5, adapter=None, **kwargs)</code>","text":"<p>Initialize a Predict module.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature]</code> <p>Signature defining inputs and outputs</p> required <code>model</code> <code>str | None</code> <p>Model name (overrides global default)</p> <code>None</code> <code>tools</code> <code>list[Any] | None</code> <p>List of tool functions (decorated with @tool) or Pydantic models</p> <code>None</code> <code>max_turns</code> <code>int</code> <p>Maximum number of LLM calls for tool execution loop (default: 5)</p> <code>5</code> <code>adapter</code> <code>ChatAdapter | None</code> <p>Custom adapter (defaults to ChatAdapter)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for chat completion (temperature, etc.)</p> <code>{}</code> Source code in <code>src/udspy/module/predict.py</code> <pre><code>def __init__(\n    self,\n    signature: type[Signature],\n    *,\n    model: str | None = None,\n    tools: list[Any] | None = None,\n    max_turns: int = 5,\n    adapter: ChatAdapter | None = None,\n    **kwargs: Any,\n):\n    \"\"\"Initialize a Predict module.\n\n    Args:\n        signature: Signature defining inputs and outputs\n        model: Model name (overrides global default)\n        tools: List of tool functions (decorated with @tool) or Pydantic models\n        max_turns: Maximum number of LLM calls for tool execution loop (default: 5)\n        adapter: Custom adapter (defaults to ChatAdapter)\n        **kwargs: Additional arguments for chat completion (temperature, etc.)\n    \"\"\"\n    from udspy.tool import Tool\n\n    self.signature = signature\n    self.model = model or settings.default_model\n    self.max_turns = max_turns\n    self.adapter = adapter or ChatAdapter()\n    self.kwargs = {**settings.default_kwargs, **kwargs}\n\n    # Process tools - separate Tool objects from Pydantic models\n    self.tool_callables: dict[str, Tool] = {}\n    self.tool_schemas: list[Any] = []\n\n    for tool in tools or []:\n        if isinstance(tool, Tool):\n            # Tool decorator - store both callable and schema\n            self.tool_callables[tool.name] = tool\n            self.tool_schemas.append(tool)\n        else:\n            # Pydantic model - just schema (no automatic execution)\n            self.tool_schemas.append(tool)\n</code></pre>"},{"location":"api/module/#udspy.module.Predict.aforward","title":"<code>aforward(*, auto_execute_tools=True, history=None, **inputs)</code>  <code>async</code>","text":"<p>Async non-streaming method. Returns the final Prediction.</p> <p>When tools are used with auto_execute_tools=True (default), this returns the LAST prediction (after tool execution), not the first one (which might only contain tool_calls). When auto_execute_tools=False, returns the first Prediction with tool_calls for manual handling.</p> <p>Parameters:</p> Name Type Description Default <code>auto_execute_tools</code> <code>bool</code> <p>If True, automatically execute tools and return final answer. If False, return Prediction with tool_calls for manual execution. Default: True.</p> <code>True</code> <code>history</code> <code>Any</code> <p>Optional History object for multi-turn conversations.</p> <code>None</code> <code>**inputs</code> <code>Any</code> <p>Input values for the module</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object (after all tool executions if auto_execute_tools=True)</p> Source code in <code>src/udspy/module/predict.py</code> <pre><code>async def aforward(\n    self, *, auto_execute_tools: bool = True, history: Any = None, **inputs: Any\n) -&gt; Prediction:\n    \"\"\"Async non-streaming method. Returns the final Prediction.\n\n    When tools are used with auto_execute_tools=True (default), this returns\n    the LAST prediction (after tool execution), not the first one (which might\n    only contain tool_calls). When auto_execute_tools=False, returns the first\n    Prediction with tool_calls for manual handling.\n\n    Args:\n        auto_execute_tools: If True, automatically execute tools and return\n            final answer. If False, return Prediction with tool_calls for\n            manual execution. Default: True.\n        history: Optional History object for multi-turn conversations.\n        **inputs: Input values for the module\n\n    Returns:\n        Final Prediction object (after all tool executions if auto_execute_tools=True)\n    \"\"\"\n    final_prediction: Prediction | None = None\n    async for event in self.astream(\n        auto_execute_tools=auto_execute_tools, history=history, **inputs\n    ):\n        if isinstance(event, Prediction):\n            final_prediction = event  # Keep updating to get the last one\n\n    if final_prediction is None:\n        raise RuntimeError(f\"{self.__class__.__name__}.astream() did not yield a Prediction\")\n\n    return final_prediction\n</code></pre>"},{"location":"api/module/#udspy.module.Predict.astream","title":"<code>astream(*, auto_execute_tools=True, history=None, **inputs)</code>  <code>async</code>","text":"<p>Core async streaming method with optional automatic tool execution.</p> <p>Yields StreamEvent objects. When auto_execute_tools=True (default), automatically handles multi-turn conversation when tools are present. When False, returns Prediction with tool_calls for manual handling.</p> <p>Parameters:</p> Name Type Description Default <code>auto_execute_tools</code> <code>bool</code> <p>If True, automatically execute tools and continue conversation. If False, return Prediction with tool_calls for manual execution. Default: True.</p> <code>True</code> <code>history</code> <code>Optional[History]</code> <p>Optional History object for multi-turn conversations. When provided, conversation history is automatically managed.</p> <code>None</code> <code>**inputs</code> <code>Any</code> <p>Input values matching the signature's input fields</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[StreamEvent, None]</code> <p>StreamEvent objects (StreamChunk, Prediction, custom events)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required inputs are missing</p> Source code in <code>src/udspy/module/predict.py</code> <pre><code>async def astream(\n    self, *, auto_execute_tools: bool = True, history: Optional[\"History\"] = None, **inputs: Any\n) -&gt; AsyncGenerator[StreamEvent, None]:\n    \"\"\"Core async streaming method with optional automatic tool execution.\n\n    Yields StreamEvent objects. When auto_execute_tools=True (default),\n    automatically handles multi-turn conversation when tools are present.\n    When False, returns Prediction with tool_calls for manual handling.\n\n    Args:\n        auto_execute_tools: If True, automatically execute tools and continue\n            conversation. If False, return Prediction with tool_calls for\n            manual execution. Default: True.\n        history: Optional History object for multi-turn conversations. When\n            provided, conversation history is automatically managed.\n        **inputs: Input values matching the signature's input fields\n\n    Yields:\n        StreamEvent objects (StreamChunk, Prediction, custom events)\n\n    Raises:\n        ValueError: If required inputs are missing\n    \"\"\"\n    # Validate and build initial messages\n    self._validate_inputs(inputs)\n    messages = self._build_initial_messages(inputs, history)\n\n    # Multi-turn loop for tool execution\n    async for event in self._execute_with_tools(messages, auto_execute_tools, history):\n        yield event\n</code></pre>"},{"location":"api/module/#udspy.module.Prediction","title":"<code>Prediction</code>","text":"<p>               Bases: <code>StreamEvent</code>, <code>dict[str, Any]</code></p> <p>Final prediction result with attribute access.</p> <p>This is both a StreamEvent (can be yielded from astream) and a dict (for convenient attribute access to outputs).</p> Example <pre><code>pred = Prediction(answer=\"Paris\", reasoning=\"France's capital\")\nprint(pred.answer)  # \"Paris\"\nprint(pred[\"answer\"])  # \"Paris\"\n</code></pre> Source code in <code>src/udspy/streaming.py</code> <pre><code>class Prediction(StreamEvent, dict[str, Any]):\n    \"\"\"Final prediction result with attribute access.\n\n    This is both a StreamEvent (can be yielded from astream) and a dict\n    (for convenient attribute access to outputs).\n\n    Example:\n        ```python\n        pred = Prediction(answer=\"Paris\", reasoning=\"France's capital\")\n        print(pred.answer)  # \"Paris\"\n        print(pred[\"answer\"])  # \"Paris\"\n        ```\n    \"\"\"\n\n    def __getattr__(self, name: str) -&gt; Any:\n        try:\n            return self[name]\n        except KeyError:\n            raise AttributeError(f\"Prediction has no attribute '{name}'\") from None\n\n    def __setattr__(self, name: str, value: Any) -&gt; None:\n        self[name] = value\n</code></pre>"},{"location":"api/settings/","title":"API Reference: Settings","text":""},{"location":"api/settings/#udspy.settings","title":"<code>udspy.settings</code>","text":"<p>Global settings and configuration.</p>"},{"location":"api/settings/#udspy.settings-classes","title":"Classes","text":""},{"location":"api/settings/#udspy.settings.Settings","title":"<code>Settings</code>","text":"<p>Global settings for udspy.</p> <p>Since udspy is async-first, we only need the async OpenAI client. Sync wrappers (forward(), call()) use asyncio.run() internally, which works fine with the async client.</p> Source code in <code>src/udspy/settings.py</code> <pre><code>class Settings:\n    \"\"\"Global settings for udspy.\n\n    Since udspy is async-first, we only need the async OpenAI client.\n    Sync wrappers (forward(), __call__()) use asyncio.run() internally,\n    which works fine with the async client.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self._aclient: AsyncOpenAI | None = None\n        self._default_model: str | None = None\n        self._default_kwargs: dict[str, Any] = {}\n\n        # Context-specific overrides (thread-safe)\n        self._context_aclient: ContextVar[AsyncOpenAI | None] = ContextVar(\n            \"context_aclient\", default=None\n        )\n        self._context_model: ContextVar[str | None] = ContextVar(\"context_model\", default=None)\n        self._context_kwargs: ContextVar[dict[str, Any] | None] = ContextVar(\n            \"context_kwargs\", default=None\n        )\n\n    def configure(\n        self,\n        api_key: str | None = None,\n        base_url: str | None = None,\n        model: str | None = None,\n        aclient: AsyncOpenAI | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Configure global OpenAI client and defaults.\n\n        Args:\n            api_key: OpenAI API key (creates default async client)\n            base_url: Base URL for OpenAI API\n            model: Default model to use for all predictions\n            aclient: Custom async OpenAI client\n            **kwargs: Default kwargs for all chat completions (temperature, etc.)\n\n        Example:\n            ```python\n            import udspy\n\n            # With API key\n            udspy.settings.configure(api_key=\"sk-...\", model=\"gpt-4o\")\n\n            # With custom client\n            from openai import AsyncOpenAI\n            client = AsyncOpenAI(api_key=\"sk-...\", timeout=30.0)\n            udspy.settings.configure(aclient=client, model=\"gpt-4o\")\n            ```\n        \"\"\"\n        if aclient:\n            self._aclient = aclient\n        elif api_key:\n            self._aclient = AsyncOpenAI(api_key=api_key, base_url=base_url)\n\n        if model:\n            self._default_model = model\n\n        self._default_kwargs.update(kwargs)\n\n    @property\n    def aclient(self) -&gt; AsyncOpenAI:\n        \"\"\"Get the async OpenAI client (context-aware).\n\n        This is used by all module operations, both async and sync.\n        Sync wrappers use asyncio.run() internally.\n        \"\"\"\n        # Check context first\n        context_aclient = self._context_aclient.get()\n        if context_aclient is not None:\n            return context_aclient\n\n        # Fall back to global client\n        if self._aclient is None:\n            raise RuntimeError(\n                \"OpenAI client not configured. Call udspy.settings.configure() first.\"\n            )\n        return self._aclient\n\n    @property\n    def default_model(self) -&gt; str:\n        \"\"\"Get the default model name (context-aware).\"\"\"\n        # Check context first\n        context_model = self._context_model.get()\n        if context_model is not None:\n            return context_model\n\n        # Fall back to global model\n        if self._default_model is None:\n            raise ValueError(\n                \"No model configured. Call settings.configure(model='...') or set in context.\"\n            )\n        return self._default_model\n\n    @property\n    def default_kwargs(self) -&gt; dict[str, Any]:\n        \"\"\"Get the default kwargs for chat completions (context-aware).\"\"\"\n        # Start with global defaults\n        result = self._default_kwargs.copy()\n\n        # Override with context-specific kwargs if present\n        context_kwargs = self._context_kwargs.get()\n        if context_kwargs is not None:\n            result.update(context_kwargs)\n\n        return result\n\n    @contextmanager\n    def context(\n        self,\n        api_key: str | None = None,\n        model: str | None = None,\n        aclient: AsyncOpenAI | None = None,\n        **kwargs: Any,\n    ) -&gt; Iterator[None]:\n        \"\"\"Context manager for temporary settings overrides.\n\n        This is thread-safe and allows you to use different API keys, models,\n        or other settings within a specific context. Useful for multi-tenant\n        applications.\n\n        Args:\n            api_key: Temporary OpenAI API key (creates temporary client)\n            model: Temporary model to use\n            aclient: Temporary async OpenAI client\n            **kwargs: Temporary kwargs for chat completions\n\n        Example:\n            ```python\n            import udspy\n            from udspy import Predict, Signature, InputField, OutputField\n\n            # Global settings\n            udspy.settings.configure(api_key=\"global-key\", model=\"gpt-4o-mini\")\n\n            class QA(Signature):\n                question: str = InputField()\n                answer: str = OutputField()\n\n            predictor = Predict(QA)\n\n            # Temporary override for a specific context (e.g., different tenant)\n            with udspy.settings.context(api_key=\"tenant-key\", model=\"gpt-4\"):\n                result = predictor(question=\"...\")  # Uses \"tenant-key\" and \"gpt-4\"\n\n            # Back to global settings\n            result = predictor(question=\"...\")  # Uses \"global-key\" and \"gpt-4o-mini\"\n            ```\n        \"\"\"\n        # Save current context values\n        prev_aclient = self._context_aclient.get()\n        prev_model = self._context_model.get()\n        prev_kwargs = self._context_kwargs.get()\n\n        try:\n            # Set context-specific values\n            if aclient:\n                self._context_aclient.set(aclient)\n            elif api_key:\n                self._context_aclient.set(AsyncOpenAI(api_key=api_key))\n\n            if model:\n                self._context_model.set(model)\n\n            if kwargs:\n                # Merge with previous context kwargs if any\n                merged_kwargs = (prev_kwargs or {}).copy()\n                merged_kwargs.update(kwargs)\n                self._context_kwargs.set(merged_kwargs)\n\n            yield\n\n        finally:\n            # Restore previous context values\n            self._context_aclient.set(prev_aclient)\n            self._context_model.set(prev_model)\n            self._context_kwargs.set(prev_kwargs)\n</code></pre>"},{"location":"api/settings/#udspy.settings.Settings-attributes","title":"Attributes","text":""},{"location":"api/settings/#udspy.settings.Settings.aclient","title":"<code>aclient</code>  <code>property</code>","text":"<p>Get the async OpenAI client (context-aware).</p> <p>This is used by all module operations, both async and sync. Sync wrappers use asyncio.run() internally.</p>"},{"location":"api/settings/#udspy.settings.Settings.default_kwargs","title":"<code>default_kwargs</code>  <code>property</code>","text":"<p>Get the default kwargs for chat completions (context-aware).</p>"},{"location":"api/settings/#udspy.settings.Settings.default_model","title":"<code>default_model</code>  <code>property</code>","text":"<p>Get the default model name (context-aware).</p>"},{"location":"api/settings/#udspy.settings.Settings-functions","title":"Functions","text":""},{"location":"api/settings/#udspy.settings.Settings.configure","title":"<code>configure(api_key=None, base_url=None, model=None, aclient=None, **kwargs)</code>","text":"<p>Configure global OpenAI client and defaults.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str | None</code> <p>OpenAI API key (creates default async client)</p> <code>None</code> <code>base_url</code> <code>str | None</code> <p>Base URL for OpenAI API</p> <code>None</code> <code>model</code> <code>str | None</code> <p>Default model to use for all predictions</p> <code>None</code> <code>aclient</code> <code>AsyncOpenAI | None</code> <p>Custom async OpenAI client</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Default kwargs for all chat completions (temperature, etc.)</p> <code>{}</code> Example <pre><code>import udspy\n\n# With API key\nudspy.settings.configure(api_key=\"sk-...\", model=\"gpt-4o\")\n\n# With custom client\nfrom openai import AsyncOpenAI\nclient = AsyncOpenAI(api_key=\"sk-...\", timeout=30.0)\nudspy.settings.configure(aclient=client, model=\"gpt-4o\")\n</code></pre> Source code in <code>src/udspy/settings.py</code> <pre><code>def configure(\n    self,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    model: str | None = None,\n    aclient: AsyncOpenAI | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Configure global OpenAI client and defaults.\n\n    Args:\n        api_key: OpenAI API key (creates default async client)\n        base_url: Base URL for OpenAI API\n        model: Default model to use for all predictions\n        aclient: Custom async OpenAI client\n        **kwargs: Default kwargs for all chat completions (temperature, etc.)\n\n    Example:\n        ```python\n        import udspy\n\n        # With API key\n        udspy.settings.configure(api_key=\"sk-...\", model=\"gpt-4o\")\n\n        # With custom client\n        from openai import AsyncOpenAI\n        client = AsyncOpenAI(api_key=\"sk-...\", timeout=30.0)\n        udspy.settings.configure(aclient=client, model=\"gpt-4o\")\n        ```\n    \"\"\"\n    if aclient:\n        self._aclient = aclient\n    elif api_key:\n        self._aclient = AsyncOpenAI(api_key=api_key, base_url=base_url)\n\n    if model:\n        self._default_model = model\n\n    self._default_kwargs.update(kwargs)\n</code></pre>"},{"location":"api/settings/#udspy.settings.Settings.context","title":"<code>context(api_key=None, model=None, aclient=None, **kwargs)</code>","text":"<p>Context manager for temporary settings overrides.</p> <p>This is thread-safe and allows you to use different API keys, models, or other settings within a specific context. Useful for multi-tenant applications.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str | None</code> <p>Temporary OpenAI API key (creates temporary client)</p> <code>None</code> <code>model</code> <code>str | None</code> <p>Temporary model to use</p> <code>None</code> <code>aclient</code> <code>AsyncOpenAI | None</code> <p>Temporary async OpenAI client</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Temporary kwargs for chat completions</p> <code>{}</code> Example <pre><code>import udspy\nfrom udspy import Predict, Signature, InputField, OutputField\n\n# Global settings\nudspy.settings.configure(api_key=\"global-key\", model=\"gpt-4o-mini\")\n\nclass QA(Signature):\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\n\n# Temporary override for a specific context (e.g., different tenant)\nwith udspy.settings.context(api_key=\"tenant-key\", model=\"gpt-4\"):\n    result = predictor(question=\"...\")  # Uses \"tenant-key\" and \"gpt-4\"\n\n# Back to global settings\nresult = predictor(question=\"...\")  # Uses \"global-key\" and \"gpt-4o-mini\"\n</code></pre> Source code in <code>src/udspy/settings.py</code> <pre><code>@contextmanager\ndef context(\n    self,\n    api_key: str | None = None,\n    model: str | None = None,\n    aclient: AsyncOpenAI | None = None,\n    **kwargs: Any,\n) -&gt; Iterator[None]:\n    \"\"\"Context manager for temporary settings overrides.\n\n    This is thread-safe and allows you to use different API keys, models,\n    or other settings within a specific context. Useful for multi-tenant\n    applications.\n\n    Args:\n        api_key: Temporary OpenAI API key (creates temporary client)\n        model: Temporary model to use\n        aclient: Temporary async OpenAI client\n        **kwargs: Temporary kwargs for chat completions\n\n    Example:\n        ```python\n        import udspy\n        from udspy import Predict, Signature, InputField, OutputField\n\n        # Global settings\n        udspy.settings.configure(api_key=\"global-key\", model=\"gpt-4o-mini\")\n\n        class QA(Signature):\n            question: str = InputField()\n            answer: str = OutputField()\n\n        predictor = Predict(QA)\n\n        # Temporary override for a specific context (e.g., different tenant)\n        with udspy.settings.context(api_key=\"tenant-key\", model=\"gpt-4\"):\n            result = predictor(question=\"...\")  # Uses \"tenant-key\" and \"gpt-4\"\n\n        # Back to global settings\n        result = predictor(question=\"...\")  # Uses \"global-key\" and \"gpt-4o-mini\"\n        ```\n    \"\"\"\n    # Save current context values\n    prev_aclient = self._context_aclient.get()\n    prev_model = self._context_model.get()\n    prev_kwargs = self._context_kwargs.get()\n\n    try:\n        # Set context-specific values\n        if aclient:\n            self._context_aclient.set(aclient)\n        elif api_key:\n            self._context_aclient.set(AsyncOpenAI(api_key=api_key))\n\n        if model:\n            self._context_model.set(model)\n\n        if kwargs:\n            # Merge with previous context kwargs if any\n            merged_kwargs = (prev_kwargs or {}).copy()\n            merged_kwargs.update(kwargs)\n            self._context_kwargs.set(merged_kwargs)\n\n        yield\n\n    finally:\n        # Restore previous context values\n        self._context_aclient.set(prev_aclient)\n        self._context_model.set(prev_model)\n        self._context_kwargs.set(prev_kwargs)\n</code></pre>"},{"location":"api/signature/","title":"API Reference: Signatures","text":""},{"location":"api/signature/#udspy.signature","title":"<code>udspy.signature</code>","text":"<p>Signature definitions for structured LLM inputs and outputs.</p>"},{"location":"api/signature/#udspy.signature-classes","title":"Classes","text":""},{"location":"api/signature/#udspy.signature.Signature","title":"<code>Signature</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for defining LLM task signatures.</p> <p>A Signature specifies the input and output fields for an LLM task, along with an optional instruction describing the task.</p> Example <pre><code>class QA(Signature):\n    '''Answer questions concisely.'''\n    question: str = InputField(description=\"Question to answer\")\n    answer: str = OutputField(description=\"Concise answer\")\n</code></pre> Source code in <code>src/udspy/signature.py</code> <pre><code>class Signature(BaseModel, metaclass=SignatureMeta):\n    \"\"\"Base class for defining LLM task signatures.\n\n    A Signature specifies the input and output fields for an LLM task,\n    along with an optional instruction describing the task.\n\n    Example:\n        ```python\n        class QA(Signature):\n            '''Answer questions concisely.'''\n            question: str = InputField(description=\"Question to answer\")\n            answer: str = OutputField(description=\"Concise answer\")\n        ```\n    \"\"\"\n\n    @classmethod\n    def get_input_fields(cls) -&gt; dict[str, FieldInfo]:\n        \"\"\"Get all input fields defined in this signature.\"\"\"\n        return {\n            name: field_info\n            for name, field_info in cls.model_fields.items()\n            if (field_info.json_schema_extra or {}).get(\"__udspy_field_type\") == \"input\"  # type: ignore[union-attr]\n        }\n\n    @classmethod\n    def get_output_fields(cls) -&gt; dict[str, FieldInfo]:\n        \"\"\"Get all output fields defined in this signature.\"\"\"\n        return {\n            name: field_info\n            for name, field_info in cls.model_fields.items()\n            if (field_info.json_schema_extra or {}).get(\"__udspy_field_type\") == \"output\"  # type: ignore[union-attr]\n        }\n\n    @classmethod\n    def get_instructions(cls) -&gt; str:\n        \"\"\"Get the task instructions from the docstring.\"\"\"\n        return (cls.__doc__ or \"\").strip()\n</code></pre>"},{"location":"api/signature/#udspy.signature.Signature-functions","title":"Functions","text":""},{"location":"api/signature/#udspy.signature.Signature.get_input_fields","title":"<code>get_input_fields()</code>  <code>classmethod</code>","text":"<p>Get all input fields defined in this signature.</p> Source code in <code>src/udspy/signature.py</code> <pre><code>@classmethod\ndef get_input_fields(cls) -&gt; dict[str, FieldInfo]:\n    \"\"\"Get all input fields defined in this signature.\"\"\"\n    return {\n        name: field_info\n        for name, field_info in cls.model_fields.items()\n        if (field_info.json_schema_extra or {}).get(\"__udspy_field_type\") == \"input\"  # type: ignore[union-attr]\n    }\n</code></pre>"},{"location":"api/signature/#udspy.signature.Signature.get_instructions","title":"<code>get_instructions()</code>  <code>classmethod</code>","text":"<p>Get the task instructions from the docstring.</p> Source code in <code>src/udspy/signature.py</code> <pre><code>@classmethod\ndef get_instructions(cls) -&gt; str:\n    \"\"\"Get the task instructions from the docstring.\"\"\"\n    return (cls.__doc__ or \"\").strip()\n</code></pre>"},{"location":"api/signature/#udspy.signature.Signature.get_output_fields","title":"<code>get_output_fields()</code>  <code>classmethod</code>","text":"<p>Get all output fields defined in this signature.</p> Source code in <code>src/udspy/signature.py</code> <pre><code>@classmethod\ndef get_output_fields(cls) -&gt; dict[str, FieldInfo]:\n    \"\"\"Get all output fields defined in this signature.\"\"\"\n    return {\n        name: field_info\n        for name, field_info in cls.model_fields.items()\n        if (field_info.json_schema_extra or {}).get(\"__udspy_field_type\") == \"output\"  # type: ignore[union-attr]\n    }\n</code></pre>"},{"location":"api/signature/#udspy.signature.SignatureMeta","title":"<code>SignatureMeta</code>","text":"<p>               Bases: <code>type(BaseModel)</code></p> <p>Metaclass for Signature that validates field types.</p> Source code in <code>src/udspy/signature.py</code> <pre><code>class SignatureMeta(type(BaseModel)):  # type: ignore[misc]\n    \"\"\"Metaclass for Signature that validates field types.\"\"\"\n\n    def __new__(\n        mcs,\n        name: str,\n        bases: tuple[type, ...],\n        namespace: dict[str, Any],\n        **kwargs: Any,\n    ) -&gt; type:\n        cls = super().__new__(mcs, name, bases, namespace, **kwargs)\n\n        # Skip validation for the base Signature class\n        if name == \"Signature\":\n            return cls\n\n        # Validate that all fields are marked as input or output\n        for field_name, field_info in cls.model_fields.items():\n            if not isinstance(field_info, FieldInfo):\n                continue\n\n            json_schema_extra = field_info.json_schema_extra or {}\n            field_type = json_schema_extra.get(\"__udspy_field_type\")  # type: ignore[union-attr]\n\n            if field_type not in (\"input\", \"output\"):\n                raise TypeError(\n                    f\"Field '{field_name}' in {name} must be declared with \"\n                    f\"InputField() or OutputField()\"\n                )\n\n        return cls\n</code></pre>"},{"location":"api/signature/#udspy.signature-functions","title":"Functions","text":""},{"location":"api/signature/#udspy.signature.InputField","title":"<code>InputField(default=..., *, description=None, **kwargs)</code>","text":"<p>Define an input field for a Signature.</p> <p>Parameters:</p> Name Type Description Default <code>default</code> <code>Any</code> <p>Default value for the field</p> <code>...</code> <code>description</code> <code>str | None</code> <p>Human-readable description of the field's purpose</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional Pydantic field arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic FieldInfo with input metadata</p> Source code in <code>src/udspy/signature.py</code> <pre><code>def InputField(\n    default: Any = ...,\n    *,\n    description: str | None = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Define an input field for a Signature.\n\n    Args:\n        default: Default value for the field\n        description: Human-readable description of the field's purpose\n        **kwargs: Additional Pydantic field arguments\n\n    Returns:\n        A Pydantic FieldInfo with input metadata\n    \"\"\"\n    json_schema_extra = kwargs.pop(\"json_schema_extra\", {})\n    json_schema_extra[\"__udspy_field_type\"] = \"input\"\n\n    return Field(\n        default=default,\n        description=description,\n        json_schema_extra=json_schema_extra,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/signature/#udspy.signature.OutputField","title":"<code>OutputField(default=..., *, description=None, **kwargs)</code>","text":"<p>Define an output field for a Signature.</p> <p>Parameters:</p> Name Type Description Default <code>default</code> <code>Any</code> <p>Default value for the field</p> <code>...</code> <code>description</code> <code>str | None</code> <p>Human-readable description of the field's purpose</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional Pydantic field arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic FieldInfo with output metadata</p> Source code in <code>src/udspy/signature.py</code> <pre><code>def OutputField(\n    default: Any = ...,\n    *,\n    description: str | None = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Define an output field for a Signature.\n\n    Args:\n        default: Default value for the field\n        description: Human-readable description of the field's purpose\n        **kwargs: Additional Pydantic field arguments\n\n    Returns:\n        A Pydantic FieldInfo with output metadata\n    \"\"\"\n    json_schema_extra = kwargs.pop(\"json_schema_extra\", {})\n    json_schema_extra[\"__udspy_field_type\"] = \"output\"\n\n    return Field(\n        default=default,\n        description=description,\n        json_schema_extra=json_schema_extra,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/signature/#udspy.signature.make_signature","title":"<code>make_signature(input_fields, output_fields, instructions='')</code>","text":"<p>Dynamically create a Signature class.</p> <p>Parameters:</p> Name Type Description Default <code>input_fields</code> <code>dict[str, type]</code> <p>Dictionary mapping field names to types for inputs</p> required <code>output_fields</code> <code>dict[str, type]</code> <p>Dictionary mapping field names to types for outputs</p> required <code>instructions</code> <code>str</code> <p>Task instructions</p> <code>''</code> <p>Returns:</p> Type Description <code>type[Signature]</code> <p>A new Signature class</p> Example <pre><code>QA = make_signature(\n    {\"question\": str},\n    {\"answer\": str},\n    \"Answer questions concisely\"\n)\n</code></pre> Source code in <code>src/udspy/signature.py</code> <pre><code>def make_signature(\n    input_fields: dict[str, type],\n    output_fields: dict[str, type],\n    instructions: str = \"\",\n) -&gt; type[Signature]:\n    \"\"\"Dynamically create a Signature class.\n\n    Args:\n        input_fields: Dictionary mapping field names to types for inputs\n        output_fields: Dictionary mapping field names to types for outputs\n        instructions: Task instructions\n\n    Returns:\n        A new Signature class\n\n    Example:\n        ```python\n        QA = make_signature(\n            {\"question\": str},\n            {\"answer\": str},\n            \"Answer questions concisely\"\n        )\n        ```\n    \"\"\"\n    fields = {}\n\n    for name, type_ in input_fields.items():\n        fields[name] = (type_, InputField())\n\n    for name, type_ in output_fields.items():\n        fields[name] = (type_, OutputField())\n\n    sig = create_model(\n        \"DynamicSignature\",\n        __base__=Signature,\n        **fields,  # type: ignore\n    )\n\n    if instructions:\n        sig.__doc__ = instructions\n\n    return sig\n</code></pre>"},{"location":"api/streaming/","title":"API Reference: Streaming","text":""},{"location":"api/streaming/#udspy.streaming","title":"<code>udspy.streaming</code>","text":"<p>Streaming support with event queue for incremental LLM outputs and tool updates.</p>"},{"location":"api/streaming/#udspy.streaming-classes","title":"Classes","text":""},{"location":"api/streaming/#udspy.streaming.Prediction","title":"<code>Prediction</code>","text":"<p>               Bases: <code>StreamEvent</code>, <code>dict[str, Any]</code></p> <p>Final prediction result with attribute access.</p> <p>This is both a StreamEvent (can be yielded from astream) and a dict (for convenient attribute access to outputs).</p> Example <pre><code>pred = Prediction(answer=\"Paris\", reasoning=\"France's capital\")\nprint(pred.answer)  # \"Paris\"\nprint(pred[\"answer\"])  # \"Paris\"\n</code></pre> Source code in <code>src/udspy/streaming.py</code> <pre><code>class Prediction(StreamEvent, dict[str, Any]):\n    \"\"\"Final prediction result with attribute access.\n\n    This is both a StreamEvent (can be yielded from astream) and a dict\n    (for convenient attribute access to outputs).\n\n    Example:\n        ```python\n        pred = Prediction(answer=\"Paris\", reasoning=\"France's capital\")\n        print(pred.answer)  # \"Paris\"\n        print(pred[\"answer\"])  # \"Paris\"\n        ```\n    \"\"\"\n\n    def __getattr__(self, name: str) -&gt; Any:\n        try:\n            return self[name]\n        except KeyError:\n            raise AttributeError(f\"Prediction has no attribute '{name}'\") from None\n\n    def __setattr__(self, name: str, value: Any) -&gt; None:\n        self[name] = value\n</code></pre>"},{"location":"api/streaming/#udspy.streaming.StreamChunk","title":"<code>StreamChunk</code>","text":"<p>               Bases: <code>StreamEvent</code></p> <p>A chunk of streamed LLM output for a specific field.</p> <p>Attributes:</p> Name Type Description <code>field_name</code> <p>Name of the output field</p> <code>delta</code> <p>Incremental content for this field (new text since last chunk)</p> <code>content</code> <p>Full accumulated content for this field so far</p> <code>is_complete</code> <p>Whether this field is finished streaming</p> Source code in <code>src/udspy/streaming.py</code> <pre><code>class StreamChunk(StreamEvent):\n    \"\"\"A chunk of streamed LLM output for a specific field.\n\n    Attributes:\n        field_name: Name of the output field\n        delta: Incremental content for this field (new text since last chunk)\n        content: Full accumulated content for this field so far\n        is_complete: Whether this field is finished streaming\n    \"\"\"\n\n    def __init__(\n        self, module: \"Module\", field_name: str, delta: str, content: str, is_complete: bool = False\n    ):\n        self.module = module\n        self.field_name = field_name\n        self.delta = delta\n        self.content = content\n        self.is_complete = is_complete\n\n    def __repr__(self) -&gt; str:\n        status = \"complete\" if self.is_complete else \"streaming\"\n        return (\n            f\"StreamChunk(field={self.field_name}, status={status}, \"\n            f\"delta={self.delta!r}, content={self.content!r})\"\n        )\n</code></pre>"},{"location":"api/streaming/#udspy.streaming.StreamEvent","title":"<code>StreamEvent</code>","text":"<p>Base class for all stream events.</p> <p>Users can define custom event types by inheriting from this class. The only built-in events are StreamChunk and Prediction.</p> Example <pre><code>from dataclasses import dataclass\nfrom udspy.streaming import StreamEvent, emit_event\n\n@dataclass\nclass ToolProgress(StreamEvent):\n    tool_name: str\n    message: str\n    progress: float  # 0.0 to 1.0\n\n# In your tool:\nasync def my_tool():\n    await emit_event(ToolProgress(\"search\", \"Searching...\", 0.5))\n</code></pre> Source code in <code>src/udspy/streaming.py</code> <pre><code>class StreamEvent:\n    \"\"\"Base class for all stream events.\n\n    Users can define custom event types by inheriting from this class.\n    The only built-in events are StreamChunk and Prediction.\n\n    Example:\n        ```python\n        from dataclasses import dataclass\n        from udspy.streaming import StreamEvent, emit_event\n\n        @dataclass\n        class ToolProgress(StreamEvent):\n            tool_name: str\n            message: str\n            progress: float  # 0.0 to 1.0\n\n        # In your tool:\n        async def my_tool():\n            await emit_event(ToolProgress(\"search\", \"Searching...\", 0.5))\n        ```\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/streaming/#udspy.streaming-functions","title":"Functions","text":""},{"location":"api/streaming/#udspy.streaming.emit_event","title":"<code>emit_event(event)</code>  <code>async</code>","text":"<p>Emit an event to the active stream.</p> <p>This can be called from anywhere (tools, callbacks, etc.) to inject events into the current streaming context. If no stream is active, this is a no-op (silently ignored).</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>StreamEvent</code> <p>The event to emit (any subclass of StreamEvent)</p> required Example <pre><code>from udspy.streaming import emit_event, StreamEvent\nfrom dataclasses import dataclass\n\n@dataclass\nclass ToolStatus(StreamEvent):\n    message: str\n\nasync def my_tool():\n    await emit_event(ToolStatus(\"Starting search...\"))\n    result = await do_search()\n    await emit_event(ToolStatus(\"Search complete\"))\n    return result\n\n# In the stream consumer:\nasync for event in predictor.astream(question=\"...\"):\n    if isinstance(event, ToolStatus):\n        print(f\"\ud83d\udcca {event.message}\")\n    elif isinstance(event, StreamChunk):\n        print(event.delta, end=\"\", flush=True)\n</code></pre> Source code in <code>src/udspy/streaming.py</code> <pre><code>async def emit_event(event: StreamEvent) -&gt; None:\n    \"\"\"Emit an event to the active stream.\n\n    This can be called from anywhere (tools, callbacks, etc.) to inject\n    events into the current streaming context. If no stream is active,\n    this is a no-op (silently ignored).\n\n    Args:\n        event: The event to emit (any subclass of StreamEvent)\n\n    Example:\n        ```python\n        from udspy.streaming import emit_event, StreamEvent\n        from dataclasses import dataclass\n\n        @dataclass\n        class ToolStatus(StreamEvent):\n            message: str\n\n        async def my_tool():\n            await emit_event(ToolStatus(\"Starting search...\"))\n            result = await do_search()\n            await emit_event(ToolStatus(\"Search complete\"))\n            return result\n\n        # In the stream consumer:\n        async for event in predictor.astream(question=\"...\"):\n            if isinstance(event, ToolStatus):\n                print(f\"\ud83d\udcca {event.message}\")\n            elif isinstance(event, StreamChunk):\n                print(event.delta, end=\"\", flush=True)\n        ```\n    \"\"\"\n    queue = _stream_queue.get()\n    if queue is not None:\n        await queue.put(event)\n</code></pre>"},{"location":"architecture/adapters/","title":"Adapters","text":"<p>Adapters handle the translation between Signatures and LLM-specific message formats.</p>"},{"location":"architecture/adapters/#overview","title":"Overview","text":"<p>The <code>ChatAdapter</code> is responsible for:</p> <ol> <li>Converting signatures into system prompts</li> <li>Formatting inputs into user messages</li> <li>Parsing LLM completions into structured outputs</li> <li>Converting Pydantic models to tool schemas</li> </ol>"},{"location":"architecture/adapters/#usage","title":"Usage","text":"<pre><code>from udspy import ChatAdapter\n\nadapter = ChatAdapter()\n</code></pre> <p>Adapters are typically used internally by modules, but can be used directly:</p> <pre><code># Format instructions\ninstructions = adapter.format_instructions(signature)\n\n# Format inputs\nformatted = adapter.format_inputs(signature, {\"question\": \"What is AI?\"})\n\n# Parse outputs\noutputs = adapter.parse_outputs(signature, completion_text)\n</code></pre>"},{"location":"architecture/adapters/#custom-adapters","title":"Custom Adapters","text":"<p>You can create custom adapters by subclassing <code>ChatAdapter</code>:</p> <pre><code>class CustomAdapter(ChatAdapter):\n    def format_instructions(self, signature):\n        # Custom instruction formatting\n        return super().format_instructions(signature) + \"\\nBe creative!\"\n</code></pre> <p>See API: Adapters for detailed documentation.</p>"},{"location":"architecture/chain_of_thought/","title":"Chain of Thought","text":"<p>Chain of Thought (CoT) is a prompting technique that improves reasoning by explicitly requesting step-by-step thinking.</p>"},{"location":"architecture/chain_of_thought/#overview","title":"Overview","text":"<p>The <code>ChainOfThought</code> module is a wrapper around <code>Predict</code> that automatically adds a \"reasoning\" field to any signature:</p> <pre><code>class QA(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\n# ChainOfThought extends the signature:\n# question -&gt; reasoning, answer\ncot = ChainOfThought(QA)\n</code></pre>"},{"location":"architecture/chain_of_thought/#how-it-works","title":"How It Works","text":""},{"location":"architecture/chain_of_thought/#1-signature-extension","title":"1. Signature Extension","text":"<p>ChainOfThought takes the original signature and creates an extended version with a reasoning field:</p> <pre><code># Original signature\nquestion: str -&gt; answer: str\n\n# Extended signature (automatically)\nquestion: str -&gt; reasoning: str, answer: str\n</code></pre>"},{"location":"architecture/chain_of_thought/#2-implementation","title":"2. Implementation","text":"<pre><code>class ChainOfThought(Module):\n    def __init__(self, signature, **kwargs):\n        # Extract input and output fields\n        input_fields = signature.get_input_fields()\n        output_fields = signature.get_output_fields()\n\n        # Prepend reasoning to outputs\n        extended_outputs = {\"reasoning\": str, **output_fields}\n\n        # Create new signature\n        extended_signature = make_signature(\n            input_fields,\n            extended_outputs,\n            signature.get_instructions()\n        )\n\n        # Use Predict with extended signature\n        self.predict = Predict(extended_signature, **kwargs)\n</code></pre>"},{"location":"architecture/chain_of_thought/#3-prompt-engineering","title":"3. Prompt Engineering","text":"<p>The reasoning field encourages step-by-step thinking through:</p> <ol> <li>Field description: \"Step-by-step reasoning process\" (customizable)</li> <li>Field ordering: Reasoning comes before the answer</li> <li>Output format: Uses field markers to structure the response</li> </ol> <p>Example prompt structure: <pre><code>[System]\nAnswer questions with clear reasoning.\n\nRequired Outputs:\n- reasoning: Step-by-step reasoning process\n- answer: Final answer\n\n[User]\n[[ ## question ## ]]\nWhat is 15 * 23?\n\n[Assistant]\n[[ ## reasoning ## ]]\nLet me calculate: 15 * 23 = 15 * 20 + 15 * 3 = 300 + 45 = 345\n\n[[ ## answer ## ]]\n345\n</code></pre></p>"},{"location":"architecture/chain_of_thought/#benefits","title":"Benefits","text":""},{"location":"architecture/chain_of_thought/#improved-accuracy","title":"Improved Accuracy","text":"<p>Chain of Thought improves accuracy on:</p> <ul> <li>Math problems: 67% \u2192 92% accuracy (typical improvement)</li> <li>Logic puzzles: Forces explicit reasoning steps</li> <li>Multi-step tasks: Prevents skipping intermediate steps</li> <li>Complex analysis: Organizes thinking</li> </ul>"},{"location":"architecture/chain_of_thought/#transparency","title":"Transparency","text":"<p>Shows the reasoning process:</p> <pre><code>result = cot(question=\"Is 17 prime?\")\n\nprint(result.reasoning)\n# \"To check if 17 is prime, I need to test divisibility\n#  by all primes up to \u221a17 \u2248 4.12.\n#  Testing: 17 \u00f7 2 = 8.5 (not divisible)\n#          17 \u00f7 3 = 5.67 (not divisible)\n#  No divisors found, so 17 is prime.\"\n\nprint(result.answer)\n# \"Yes, 17 is prime\"\n</code></pre>"},{"location":"architecture/chain_of_thought/#debugging","title":"Debugging","text":"<p>Easier to identify issues:</p> <pre><code>result = cot(question=\"What is 2^10?\")\n\nif \"1024\" not in result.answer:\n    # Check reasoning to see where it went wrong\n    print(\"Error in reasoning:\", result.reasoning)\n</code></pre>"},{"location":"architecture/chain_of_thought/#customization","title":"Customization","text":""},{"location":"architecture/chain_of_thought/#custom-reasoning-description","title":"Custom Reasoning Description","text":"<pre><code>cot = ChainOfThought(\n    signature,\n    reasoning_description=\"Detailed mathematical proof with all steps\"\n)\n</code></pre>"},{"location":"architecture/chain_of_thought/#model-parameters","title":"Model Parameters","text":"<pre><code># Deterministic reasoning for math\ncot = ChainOfThought(QA, temperature=0.0)\n\n# Creative reasoning for analysis\ncot = ChainOfThought(Analysis, temperature=0.7)\n</code></pre>"},{"location":"architecture/chain_of_thought/#multiple-outputs","title":"Multiple Outputs","text":"<p>Works seamlessly with multiple output fields:</p> <pre><code>class ComplexTask(Signature):\n    \"\"\"Complex task.\"\"\"\n    input: str = InputField()\n    analysis: str = OutputField()\n    recommendation: str = OutputField()\n    confidence: float = OutputField()\n\ncot = ChainOfThought(ComplexTask)\nresult = cot(input=\"...\")\n\n# All outputs available\nresult.reasoning       # Added automatically\nresult.analysis        # Original output\nresult.recommendation  # Original output\nresult.confidence      # Original output\n</code></pre>"},{"location":"architecture/chain_of_thought/#comparison-with-dspy","title":"Comparison with DSPy","text":""},{"location":"architecture/chain_of_thought/#similarities","title":"Similarities","text":"<ul> <li>Same concept: adds reasoning field to signature</li> <li>Improves accuracy through explicit reasoning</li> <li>Transparent reasoning process</li> </ul>"},{"location":"architecture/chain_of_thought/#differences","title":"Differences","text":"Feature udspy DSPy Implementation Signature extension Signature prepend method Customization reasoning_description rationale_field parameter Complexity ~45 lines ~40 lines Dependencies make_signature signature.prepend()"},{"location":"architecture/chain_of_thought/#udspy-approach","title":"udspy Approach","text":"<pre><code># Simpler API\ncot = ChainOfThought(QA)\n</code></pre>"},{"location":"architecture/chain_of_thought/#dspy-approach","title":"DSPy Approach","text":"<pre><code># More flexible but complex\ncot = dspy.ChainOfThought(\n    \"question -&gt; answer\",\n    rationale_field=dspy.OutputField(\n        prefix=\"Reasoning: Let's think step by step\",\n        desc=\"${reasoning}\"\n    )\n)\n</code></pre>"},{"location":"architecture/chain_of_thought/#when-to-use","title":"When to Use","text":""},{"location":"architecture/chain_of_thought/#good-use-cases","title":"Good Use Cases \u2713","text":"<ul> <li>Math problems requiring calculation</li> <li>Logic puzzles and reasoning tasks</li> <li>Multi-step analysis or planning</li> <li>Tasks where you want to verify reasoning</li> <li>Educational applications (show work)</li> <li>High-stakes decisions requiring justification</li> </ul>"},{"location":"architecture/chain_of_thought/#less-useful","title":"Less Useful \u2717","text":"<ul> <li>Simple factual recall (\"What is the capital of France?\")</li> <li>Binary classification without reasoning</li> <li>Very short outputs where reasoning overhead is large</li> <li>Real-time systems with strict latency requirements</li> </ul>"},{"location":"architecture/chain_of_thought/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/chain_of_thought/#token-usage","title":"Token Usage","text":"<p>Chain of Thought uses more tokens:</p> <pre><code># Without CoT: ~50 tokens\nresult = predict(question=\"What is 2+2?\")\n# answer: \"4\"\n\n# With CoT: ~150 tokens\nresult = cot(question=\"What is 2+2?\")\n# reasoning: \"This is basic arithmetic. 2+2 = 4\"\n# answer: \"4\"\n</code></pre> <p>Trade-off: Higher cost/latency for better accuracy and transparency.</p>"},{"location":"architecture/chain_of_thought/#optimization","title":"Optimization","text":"<p>For cost-sensitive applications:</p> <pre><code># Use CoT only for complex queries\nif is_complex(question):\n    result = cot(question=question)\nelse:\n    result = simple_predict(question=question)\n</code></pre>"},{"location":"architecture/chain_of_thought/#best-practices","title":"Best Practices","text":"<ol> <li>Use appropriate temperature</li> <li><code>0.0</code> for math/logic (deterministic)</li> <li> <p><code>0.3-0.7</code> for analysis/planning</p> </li> <li> <p>Customize reasoning description for domain-specific tasks    <pre><code>ChainOfThought(\n    MedicalDiagnosis,\n    reasoning_description=\"Clinical reasoning with differential diagnosis\"\n)\n</code></pre></p> </li> <li> <p>Validate reasoning quality in production    <pre><code>if len(result.reasoning) &lt; 100:\n    logger.warning(\"Reasoning too brief\")\n</code></pre></p> </li> <li> <p>Cache for repeated queries to save costs    <pre><code>@lru_cache(maxsize=100)\ndef cached_cot(question):\n    return cot(question=question)\n</code></pre></p> </li> </ol> <p>See Examples for more details.</p>"},{"location":"architecture/decisions/","title":"Design Decisions","text":"<p>This document explains key architectural decisions made in udspy.</p>"},{"location":"architecture/decisions/#native-tool-calling","title":"Native Tool Calling","text":"<p>Decision: Use OpenAI's native function calling instead of custom prompt-based tools.</p> <p>Rationale: - OpenAI's tool calling is optimized and well-tested - Reduces prompt complexity and token usage - Better reliability for structured tool invocation - Forward compatible with future OpenAI improvements</p> <p>Trade-offs: - Couples to OpenAI's API (less provider-agnostic) - May need adapters for other providers in future</p> <p>Alternative Considered: Custom field markers like DSPy's adapter system</p>"},{"location":"architecture/decisions/#pydantic-v2-for-models","title":"Pydantic v2 for Models","text":"<p>Decision: Use Pydantic v2 exclusively for all data modeling.</p> <p>Rationale: - Modern, fast, well-maintained - Excellent JSON schema generation for tools - Built-in validation and type coercion - Great developer experience with IDE support</p> <p>Trade-offs: - Breaking changes from Pydantic v1 (not an issue for new project) - Requires Python 3.7+ (we target 3.11+)</p>"},{"location":"architecture/decisions/#async-first-streaming","title":"Async-First Streaming","text":"<p>Decision: Streaming uses async/await, not callbacks or threads.</p> <p>Rationale: - Python's async is the standard for I/O-bound operations - Better composability with other async code - Easier to reason about than callbacks - Native support from OpenAI SDK</p> <p>Trade-offs: - Requires async runtime (asyncio) - Steeper learning curve for beginners</p> <p>Alternative Considered: Synchronous generator with threads</p>"},{"location":"architecture/decisions/#field-markers-for-parsing","title":"Field Markers for Parsing","text":"<p>Decision: Use <code>[[ ## field_name ## ]]</code> markers to delineate fields in completions.</p> <p>Rationale: - Simple, regex-parseable format - Clear visual separation - Consistent with DSPy's approach (proven) - Fallback when native tools aren't available</p> <p>Trade-offs: - Requires careful prompt engineering - LLM might not always respect markers - Uses extra tokens</p> <p>Alternative Considered: JSON-only output format (less readable in prompts)</p>"},{"location":"architecture/decisions/#minimal-dependencies","title":"Minimal Dependencies","text":"<p>Decision: Core library only depends on <code>openai</code> and <code>pydantic</code>.</p> <p>Rationale: - Easier to maintain and debug - Faster installation - Fewer security vulnerabilities - Clearer responsibility boundaries</p> <p>Trade-offs: - Users need to install extras for dev tools - Can't leverage ecosystem for advanced features</p>"},{"location":"architecture/decisions/#module-composition","title":"Module Composition","text":"<p>Decision: Modules compose via Python class inheritance and composition.</p> <p>Rationale: - Familiar Python patterns - No custom DSL to learn - Good IDE and type checker support - Easy to test and mock</p> <p>Trade-offs: - Less \"magical\" than DSPy's meta-programming - Requires more explicit code</p>"},{"location":"architecture/decisions/#settings-as-global-singleton","title":"Settings as Global Singleton","text":"<p>Decision: Configuration via global <code>settings</code> object.</p> <p>Rationale: - Convenient for most use cases - Matches Django/Flask patterns - Easy to override per-module - Thread-safe with context managers (future)</p> <p>Trade-offs: - Global state can complicate testing - Not ideal for multi-tenant applications</p> <p>Alternative Considered: Dependency injection (more verbose)</p>"},{"location":"architecture/decisions/#see-also","title":"See Also","text":"<ul> <li>CLAUDE.md - Chronological architectural changes</li> <li>Architecture Overview - Component relationships</li> </ul>"},{"location":"architecture/modules/","title":"Modules","text":"<p>Modules are composable units that encapsulate LLM calls.</p>"},{"location":"architecture/modules/#predict-module","title":"Predict Module","text":"<p>The core module is <code>Predict</code>, which maps inputs to outputs via an LLM:</p> <pre><code>from udspy import Predict\n\npredictor = Predict(\n    signature=QA,\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n)\n\nresult = predictor(question=\"What is AI?\")\n</code></pre>"},{"location":"architecture/modules/#custom-modules","title":"Custom Modules","text":"<p>Create custom modules by subclassing <code>Module</code>:</p> <pre><code>from udspy import Module, Predict, Prediction\n\nclass ChainOfThought(Module):\n    def __init__(self, signature):\n        self.think = Predict(make_signature(\n            signature.get_input_fields(),\n            {\"reasoning\": str},\n            \"Think step by step\",\n        ))\n        self.answer = Predict(signature)\n\n    def forward(self, **inputs):\n        # First, generate reasoning\n        thought = self.think(**inputs)\n\n        # Then, generate answer with reasoning\n        result = self.answer(**inputs, reasoning=thought.reasoning)\n\n        return result\n</code></pre>"},{"location":"architecture/modules/#composition","title":"Composition","text":"<p>Modules can be composed to build complex behaviors:</p> <pre><code>class Pipeline(Module):\n    def __init__(self):\n        self.analyze = Predict(AnalysisSignature)\n        self.summarize = Predict(SummarySignature)\n\n    def forward(self, text):\n        analysis = self.analyze(text=text)\n        summary = self.summarize(\n            text=text,\n            analysis=analysis.result,\n        )\n        return Prediction(\n            analysis=analysis.result,\n            summary=summary.result,\n        )\n</code></pre> <p>See API: Modules for detailed documentation.</p>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>udspy consists of four main components that work together to provide a clean abstraction for LLM interactions.</p>"},{"location":"architecture/overview/#core-components","title":"Core Components","text":""},{"location":"architecture/overview/#1-signatures","title":"1. Signatures","text":"<p>Signatures define the input/output contract for an LLM task using Pydantic models.</p> <pre><code>class QA(Signature):\n    \"\"\"Answer questions concisely.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n</code></pre> <p>Key responsibilities: - Define input and output fields with type information - Provide field descriptions for prompt construction - Validate data at runtime using Pydantic</p> <p>Learn more about Signatures \u2192</p>"},{"location":"architecture/overview/#2-adapters","title":"2. Adapters","text":"<p>Adapters handle the formatting of signatures into LLM-specific message formats and parsing responses back.</p> <pre><code>adapter = ChatAdapter()\ninstructions = adapter.format_instructions(signature)\nformatted_input = adapter.format_inputs(signature, inputs)\noutputs = adapter.parse_outputs(signature, completion)\n</code></pre> <p>Key responsibilities: - Convert signature definitions to system prompts - Format input values into user messages - Parse LLM completions into structured outputs - Convert Pydantic models to OpenAI tool schemas</p> <p>Learn more about Adapters \u2192</p>"},{"location":"architecture/overview/#3-modules","title":"3. Modules","text":"<p>Modules are composable units that encapsulate LLM calls. The core module is <code>Predict</code>.</p> <pre><code>predictor = Predict(signature, model=\"gpt-4o-mini\")\nresult = predictor(question=\"What is AI?\")\n</code></pre> <p>Key responsibilities: - Manage signature, model, and configuration - Orchestrate adapter formatting and API calls - Return structured <code>Prediction</code> objects - Support composition and reuse</p> <p>Learn more about Modules \u2192</p>"},{"location":"architecture/overview/#4-streaming","title":"4. Streaming","text":"<p>Streaming support allows incremental output processing for better UX.</p> <pre><code>predictor = StreamingPredict(signature)\nasync for chunk in predictor.stream(question=\"Explain AI\"):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <p>Key responsibilities: - Process streaming API responses incrementally - Detect field boundaries in streams - Emit field-specific chunks - Provide final parsed prediction</p> <p>Learn more about Streaming \u2192</p>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":"<pre><code>User Input \u2192 Signature \u2192 Adapter \u2192 OpenAI API \u2192 Adapter \u2192 Prediction \u2192 User\n              \u2193           \u2193                        \u2191\n           Validate    Format                   Parse\n</code></pre> <ol> <li>User provides input matching signature's input fields</li> <li>Signature validates input types</li> <li>Adapter formats signature + inputs into messages</li> <li>OpenAI API generates completion</li> <li>Adapter parses completion into structured outputs</li> <li>User receives <code>Prediction</code> with typed outputs</li> </ol>"},{"location":"architecture/overview/#design-principles","title":"Design Principles","text":""},{"location":"architecture/overview/#native-tool-calling","title":"Native Tool Calling","text":"<p>Unlike DSPy which uses custom field markers in prompts, udspy uses OpenAI's native function calling:</p> <pre><code>class Calculator(BaseModel):\n    \"\"\"Perform arithmetic.\"\"\"\n    operation: str\n    a: float\n    b: float\n\npredictor = Predict(signature, tools=[Calculator])\n</code></pre> <p>This provides: - Better performance (optimized by OpenAI) - More reliable parsing - Cleaner prompts - Forward compatibility</p>"},{"location":"architecture/overview/#minimal-abstractions","title":"Minimal Abstractions","text":"<p>Every component has a clear, focused responsibility:</p> <ul> <li>Signatures: Define I/O contracts</li> <li>Adapters: Handle format translation</li> <li>Modules: Encapsulate LLM calls</li> <li>Streaming: Process incremental outputs</li> </ul> <p>No hidden magic, no over-engineering.</p>"},{"location":"architecture/overview/#type-safety","title":"Type Safety","text":"<p>Pydantic provides runtime type checking throughout:</p> <pre><code>class QA(Signature):\n    question: str = InputField()\n    answer: int = OutputField()  # Will validate output is int\n\npredictor = Predict(QA)\nresult = predictor(question=\"What is 2+2?\")\nassert isinstance(result.answer, int)\n</code></pre>"},{"location":"architecture/overview/#configuration","title":"Configuration","text":"<p>Global configuration via <code>settings</code>:</p> <pre><code>import udspy\n\nudspy.settings.configure(\n    api_key=\"sk-...\",\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n)\n</code></pre> <p>Per-module overrides:</p> <pre><code>predictor = Predict(\n    signature,\n    model=\"gpt-4\",\n    temperature=0.0,\n)\n</code></pre>"},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Deep dive into Signatures</li> <li>Understand Adapters</li> <li>Explore Modules</li> <li>Learn about Streaming</li> </ul>"},{"location":"architecture/signatures/","title":"Signatures","text":"<p>Signatures define the input/output contract for LLM tasks using Pydantic models.</p>"},{"location":"architecture/signatures/#basic-signature","title":"Basic Signature","text":"<pre><code>from udspy import Signature, InputField, OutputField\n\nclass QA(Signature):\n    \"\"\"Answer questions concisely and accurately.\"\"\"\n    question: str = InputField(description=\"Question to answer\")\n    answer: str = OutputField(description=\"Concise answer\")\n</code></pre>"},{"location":"architecture/signatures/#components","title":"Components","text":""},{"location":"architecture/signatures/#docstring","title":"Docstring","text":"<p>The class docstring becomes the task instruction in the system prompt:</p> <pre><code>class Summarize(Signature):\n    \"\"\"Summarize the given text in 2-3 sentences.\"\"\"\n    text: str = InputField()\n    summary: str = OutputField()\n</code></pre>"},{"location":"architecture/signatures/#inputfield","title":"InputField","text":"<p>Marks a field as an input:</p> <pre><code>question: str = InputField(\n    description=\"Question to answer\",  # Used in prompt\n    default=\"\",  # Optional default value\n)\n</code></pre>"},{"location":"architecture/signatures/#outputfield","title":"OutputField","text":"<p>Marks a field as an output:</p> <pre><code>answer: str = OutputField(\n    description=\"Concise answer\",\n)\n</code></pre>"},{"location":"architecture/signatures/#field-types","title":"Field Types","text":"<p>Signatures support various field types:</p>"},{"location":"architecture/signatures/#primitives","title":"Primitives","text":"<pre><code>class Example(Signature):\n    \"\"\"Example signature.\"\"\"\n    text: str = InputField()\n    count: int = InputField()\n    score: float = InputField()\n    enabled: bool = InputField()\n</code></pre>"},{"location":"architecture/signatures/#collections","title":"Collections","text":"<pre><code>class Example(Signature):\n    \"\"\"Example signature.\"\"\"\n    tags: list[str] = InputField()\n    metadata: dict[str, Any] = InputField()\n</code></pre>"},{"location":"architecture/signatures/#pydantic-models","title":"Pydantic Models","text":"<pre><code>from pydantic import BaseModel\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\nclass Example(Signature):\n    \"\"\"Example signature.\"\"\"\n    person: Person = InputField()\n    related: list[Person] = OutputField()\n</code></pre>"},{"location":"architecture/signatures/#dynamic-signatures","title":"Dynamic Signatures","text":"<p>Create signatures programmatically:</p> <pre><code>from udspy import make_signature\n\nQA = make_signature(\n    input_fields={\"question\": str},\n    output_fields={\"answer\": str},\n    instructions=\"Answer questions concisely\",\n)\n</code></pre>"},{"location":"architecture/signatures/#validation","title":"Validation","text":"<p>Signatures use Pydantic for validation:</p> <pre><code>class Sentiment(Signature):\n    \"\"\"Analyze sentiment.\"\"\"\n    text: str = InputField()\n    sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = OutputField()\n\n# Output will be validated to match literal values\n</code></pre>"},{"location":"architecture/signatures/#multi-output-signatures","title":"Multi-Output Signatures","text":"<p>Signatures can have multiple outputs:</p> <pre><code>class ReasonedQA(Signature):\n    \"\"\"Answer with step-by-step reasoning.\"\"\"\n    question: str = InputField()\n    reasoning: str = OutputField(description=\"Reasoning process\")\n    answer: str = OutputField(description=\"Final answer\")\n</code></pre>"},{"location":"architecture/signatures/#best-practices","title":"Best Practices","text":""},{"location":"architecture/signatures/#1-clear-descriptions","title":"1. Clear Descriptions","text":"<pre><code># Good\nquestion: str = InputField(description=\"User's question about the product\")\n\n# Bad\nquestion: str = InputField()\n</code></pre>"},{"location":"architecture/signatures/#2-specific-instructions","title":"2. Specific Instructions","text":"<pre><code># Good\nclass Summarize(Signature):\n    \"\"\"Summarize in exactly 3 bullet points, each under 20 words.\"\"\"\n\n# Bad\nclass Summarize(Signature):\n    \"\"\"Summarize.\"\"\"\n</code></pre>"},{"location":"architecture/signatures/#3-structured-outputs","title":"3. Structured Outputs","text":"<pre><code># Good - use Pydantic models for complex outputs\nclass Analysis(BaseModel):\n    sentiment: str\n    confidence: float\n    keywords: list[str]\n\nclass Analyze(Signature):\n    \"\"\"Analyze text.\"\"\"\n    text: str = InputField()\n    analysis: Analysis = OutputField()\n\n# Bad - use many separate fields\nclass Analyze(Signature):\n    \"\"\"Analyze text.\"\"\"\n    text: str = InputField()\n    sentiment: str = OutputField()\n    confidence: float = OutputField()\n    keywords: list[str] = OutputField()\n</code></pre>"},{"location":"architecture/signatures/#api-reference","title":"API Reference","text":"<p>See API: Signatures for detailed API documentation.</p>"},{"location":"architecture/streaming/","title":"Streaming","text":"<p>Streaming support allows incremental processing of LLM outputs.</p>"},{"location":"architecture/streaming/#overview","title":"Overview","text":"<p>Streaming provides better user experience by showing results as they're generated:</p> <pre><code>from udspy import StreamingPredict\n\npredictor = StreamingPredict(signature)\n\nasync for chunk in predictor.stream(question=\"Explain AI\"):\n    if isinstance(chunk, StreamChunk):\n        print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"architecture/streaming/#streamchunk","title":"StreamChunk","text":"<p>Each chunk contains:</p> <ul> <li><code>field_name</code>: Which output field this chunk belongs to</li> <li><code>content</code>: Incremental content</li> <li><code>is_complete</code>: Whether the field is finished</li> </ul> <pre><code>async for item in predictor.stream(**inputs):\n    if isinstance(item, StreamChunk):\n        print(f\"[{item.field_name}] {item.content}\", end=\"\")\n        if item.is_complete:\n            print(f\"\\n--- {item.field_name} complete ---\")\n    elif isinstance(item, Prediction):\n        print(f\"\\nFinal result: {item}\")\n</code></pre>"},{"location":"architecture/streaming/#converting-predictors","title":"Converting Predictors","text":"<p>Convert any <code>Predict</code> to streaming:</p> <pre><code>from udspy import Predict, streamify\n\npredictor = Predict(signature, temperature=0.7)\nstreaming_predictor = streamify(predictor)\n\nasync for chunk in streaming_predictor.stream(**inputs):\n    ...\n</code></pre>"},{"location":"architecture/streaming/#field-specific-streaming","title":"Field-Specific Streaming","text":"<p>Streaming automatically detects field boundaries:</p> <pre><code>class ReasonedQA(Signature):\n    \"\"\"Answer with reasoning.\"\"\"\n    question: str = InputField()\n    reasoning: str = OutputField()\n    answer: str = OutputField()\n\n# Will emit chunks for 'reasoning' then 'answer'\nasync for chunk in predictor.stream(question=\"Why is sky blue?\"):\n    if chunk.field_name == \"reasoning\":\n        print(f\"Thinking: {chunk.content}\", end=\"\")\n    elif chunk.field_name == \"answer\":\n        print(f\"Answer: {chunk.content}\", end=\"\")\n</code></pre> <p>See API: Streaming and Examples: Streaming for more details.</p>"},{"location":"examples/advanced/","title":"Advanced Examples","text":"<p>Advanced patterns and techniques.</p>"},{"location":"examples/advanced/#module-composition","title":"Module Composition","text":"<p>Build complex modules from simpler ones:</p> <pre><code>from udspy import Module, Predict, Prediction\n\nclass ChainOfThought(Module):\n    \"\"\"Answer questions with explicit reasoning.\"\"\"\n\n    def __init__(self, signature):\n        # Create intermediate signature for reasoning\n        self.reason = Predict(make_signature(\n            signature.get_input_fields(),\n            {\"reasoning\": str},\n            \"Think step-by-step about this problem\",\n        ))\n\n        # Final answer with reasoning context\n        self.answer = Predict(signature)\n\n    def forward(self, **inputs):\n        # Generate reasoning\n        thought = self.reason(**inputs)\n\n        # Generate answer (could inject reasoning into prompt)\n        result = self.answer(**inputs)\n        result[\"reasoning\"] = thought.reasoning\n\n        return Prediction(**result)\n</code></pre>"},{"location":"examples/advanced/#retry-logic","title":"Retry Logic","text":"<p>Implement retry with validation:</p> <pre><code>from pydantic import ValidationError\n\nclass ValidatedPredict(Module):\n    def __init__(self, signature, max_retries=3):\n        self.predictor = Predict(signature)\n        self.signature = signature\n        self.max_retries = max_retries\n\n    def forward(self, **inputs):\n        for attempt in range(self.max_retries):\n            try:\n                result = self.predictor(**inputs)\n                # Validate result matches expected schema\n                return result\n            except ValidationError as e:\n                if attempt == self.max_retries - 1:\n                    raise\n                # Could inject error message to help LLM correct\n                continue\n</code></pre>"},{"location":"examples/advanced/#prompt-caching","title":"Prompt Caching","text":"<p>Cache prompts for repeated queries:</p> <pre><code>from functools import lru_cache\n\nclass CachedPredict(Module):\n    def __init__(self, signature):\n        self.predictor = Predict(signature)\n        self._cached_predict = lru_cache(maxsize=128)(self._predict)\n\n    def _predict(self, **inputs):\n        # Convert inputs to hashable tuple\n        key = tuple(sorted(inputs.items()))\n        return self.predictor(**dict(key))\n\n    def forward(self, **inputs):\n        return self._cached_predict(**inputs)\n</code></pre>"},{"location":"examples/advanced/#custom-adapters","title":"Custom Adapters","text":"<p>Create custom formatting:</p> <pre><code>from udspy import ChatAdapter\n\nclass VerboseAdapter(ChatAdapter):\n    def format_instructions(self, signature):\n        base = super().format_instructions(signature)\n        return f\"{base}\\n\\nIMPORTANT: Be extremely detailed in your response.\"\n\n    def format_inputs(self, signature, inputs):\n        base = super().format_inputs(signature, inputs)\n        return f\"Input data:\\n{base}\\n\\nAnalyze thoroughly:\"\n\npredictor = Predict(signature, adapter=VerboseAdapter())\n</code></pre>"},{"location":"examples/advanced/#testing-helpers","title":"Testing Helpers","text":"<p>Utilities for testing:</p> <pre><code>from unittest.mock import MagicMock\nfrom openai.types.chat import ChatCompletion, ChatCompletionMessage, Choice\n\ndef mock_openai_response(content: str) -&gt; ChatCompletion:\n    \"\"\"Create a mock OpenAI response.\"\"\"\n    return ChatCompletion(\n        id=\"test\",\n        model=\"gpt-4o-mini\",\n        object=\"chat.completion\",\n        created=1234567890,\n        choices=[\n            Choice(\n                index=0,\n                message=ChatCompletionMessage(\n                    role=\"assistant\",\n                    content=content,\n                ),\n                finish_reason=\"stop\",\n            )\n        ],\n    )\n\ndef test_with_mock():\n    mock_client = MagicMock()\n    mock_client.chat.completions.create.return_value = mock_openai_response(\n        \"[[ ## answer ## ]]\\nTest answer\"\n    )\n\n    udspy.settings.configure(client=mock_client)\n\n    predictor = Predict(QA)\n    result = predictor(question=\"Test?\")\n    assert result.answer == \"Test answer\"\n</code></pre>"},{"location":"examples/basic_usage/","title":"Basic Usage","text":"<p>This guide covers the fundamentals of using udspy.</p>"},{"location":"examples/basic_usage/#setup","title":"Setup","text":"<p>First, configure the OpenAI client:</p> <pre><code>import udspy\n\nudspy.settings.configure(api_key=\"sk-...\")\n</code></pre> <p>Or use environment variables:</p> <pre><code>import os\nimport udspy\n\nudspy.settings.configure(api_key=os.getenv(\"OPENAI_API_KEY\"))\n</code></pre>"},{"location":"examples/basic_usage/#simple-question-answering","title":"Simple Question Answering","text":"<pre><code>from udspy import Signature, InputField, OutputField, Predict\n\nclass QA(Signature):\n    \"\"\"Answer questions concisely.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\nresult = predictor(question=\"What is the capital of France?\")\nprint(result.answer)  # \"Paris\"\n</code></pre>"},{"location":"examples/basic_usage/#with-reasoning","title":"With Reasoning","text":"<pre><code>class ReasonedQA(Signature):\n    \"\"\"Answer questions with reasoning.\"\"\"\n    question: str = InputField()\n    reasoning: str = OutputField(description=\"Step-by-step reasoning\")\n    answer: str = OutputField(description=\"Final answer\")\n\npredictor = Predict(ReasonedQA)\nresult = predictor(question=\"What is 15 * 23?\")\nprint(f\"Reasoning: {result.reasoning}\")\nprint(f\"Answer: {result.answer}\")\n</code></pre>"},{"location":"examples/basic_usage/#custom-model-parameters","title":"Custom Model Parameters","text":"<pre><code>predictor = Predict(\n    signature=QA,\n    model=\"gpt-4\",\n    temperature=0.7,\n    max_tokens=100,\n)\n</code></pre>"},{"location":"examples/basic_usage/#global-defaults","title":"Global Defaults","text":"<pre><code>udspy.settings.configure(\n    api_key=\"sk-...\",\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n)\n\n# All predictors use these defaults unless overridden\npredictor = Predict(QA)\n</code></pre>"},{"location":"examples/basic_usage/#error-handling","title":"Error Handling","text":"<pre><code>from pydantic import ValidationError\n\ntry:\n    result = predictor(question=\"What is AI?\")\nexcept ValidationError as e:\n    print(f\"Output validation failed: {e}\")\nexcept Exception as e:\n    print(f\"API error: {e}\")\n</code></pre>"},{"location":"examples/basic_usage/#testing","title":"Testing","text":"<p>Mock the OpenAI client for testing:</p> <pre><code>from unittest.mock import MagicMock\nfrom openai.types.chat import ChatCompletion, ChatCompletionMessage, Choice\n\ndef test_qa():\n    # Mock response\n    mock_response = ChatCompletion(\n        id=\"test\",\n        model=\"gpt-4o-mini\",\n        object=\"chat.completion\",\n        created=1234567890,\n        choices=[\n            Choice(\n                index=0,\n                message=ChatCompletionMessage(\n                    role=\"assistant\",\n                    content=\"[[ ## answer ## ]]\\nParis\",\n                ),\n                finish_reason=\"stop\",\n            )\n        ],\n    )\n\n    # Configure mock client\n    mock_client = MagicMock()\n    mock_client.chat.completions.create.return_value = mock_response\n\n    udspy.settings.configure(client=mock_client)\n\n    # Test\n    predictor = Predict(QA)\n    result = predictor(question=\"What is the capital of France?\")\n    assert result.answer == \"Paris\"\n</code></pre>"},{"location":"examples/basic_usage/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Streaming</li> <li>Explore Tool Calling</li> <li>See Advanced Examples</li> </ul>"},{"location":"examples/chain_of_thought/","title":"Chain of Thought Examples","text":"<p>Chain of Thought (CoT) is a prompting technique that improves LLM reasoning by explicitly requesting step-by-step thinking before producing the final answer.</p>"},{"location":"examples/chain_of_thought/#overview","title":"Overview","text":"<p>The <code>ChainOfThought</code> module automatically adds a \"reasoning\" field to any signature, encouraging the LLM to show its work:</p> <pre><code>from udspy import ChainOfThought, Signature, InputField, OutputField\n\nclass QA(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\n# Automatically adds reasoning step\ncot = ChainOfThought(QA)\nresult = cot(question=\"What is 15 * 23?\")\n\nprint(result.reasoning)  # Step-by-step calculation\nprint(result.answer)     # \"345\"\n</code></pre>"},{"location":"examples/chain_of_thought/#basic-usage","title":"Basic Usage","text":""},{"location":"examples/chain_of_thought/#simple-question-answering","title":"Simple Question Answering","text":"<pre><code>import udspy\n\nudspy.settings.configure(api_key=\"your-key\")\n\nclass QA(Signature):\n    \"\"\"Answer questions clearly.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = ChainOfThought(QA)\nresult = predictor(question=\"What is the capital of France?\")\n\nprint(\"Reasoning:\", result.reasoning)\n# \"Let me recall the capital cities of European countries.\n#  France is a major European nation, and its capital is Paris.\"\n\nprint(\"Answer:\", result.answer)\n# \"Paris\"\n</code></pre>"},{"location":"examples/chain_of_thought/#math-problems","title":"Math Problems","text":"<p>Chain of Thought excels at mathematical reasoning:</p> <pre><code>result = predictor(question=\"What is 17 * 24?\")\n\nprint(\"Reasoning:\", result.reasoning)\n# \"I'll break this down: 17 * 24 = 17 * 20 + 17 * 4 = 340 + 68 = 408\"\n\nprint(\"Answer:\", result.answer)\n# \"408\"\n</code></pre>"},{"location":"examples/chain_of_thought/#advanced-usage","title":"Advanced Usage","text":""},{"location":"examples/chain_of_thought/#custom-reasoning-description","title":"Custom Reasoning Description","text":"<p>Customize how the reasoning field is described:</p> <pre><code>cot = ChainOfThought(\n    QA,\n    reasoning_description=\"Detailed mathematical proof with all intermediate steps\"\n)\n\nresult = cot(question=\"Prove that the sum of angles in a triangle is 180 degrees\")\n</code></pre>"},{"location":"examples/chain_of_thought/#multiple-output-fields","title":"Multiple Output Fields","text":"<p>Chain of Thought works with signatures that have multiple outputs:</p> <pre><code>class Analysis(Signature):\n    \"\"\"Analyze text comprehensively.\"\"\"\n    text: str = InputField()\n    summary: str = OutputField(description=\"Brief summary\")\n    sentiment: str = OutputField(description=\"Sentiment analysis\")\n    keywords: list[str] = OutputField(description=\"Key terms\")\n\nanalyzer = ChainOfThought(Analysis)\nresult = analyzer(text=\"Long article text here...\")\n\n# Access all outputs plus reasoning\nprint(result.reasoning)   # Analysis process\nprint(result.summary)     # Summary\nprint(result.sentiment)   # Sentiment\nprint(result.keywords)    # Keywords\n</code></pre>"},{"location":"examples/chain_of_thought/#with-custom-model-parameters","title":"With Custom Model Parameters","text":"<pre><code># Use with specific model and temperature\ncot = ChainOfThought(\n    QA,\n    model=\"gpt-4\",\n    temperature=0.0,  # Deterministic for math\n)\n\nresult = cot(question=\"What is the square root of 144?\")\n</code></pre>"},{"location":"examples/chain_of_thought/#comparison-with-vs-without-cot","title":"Comparison: With vs Without CoT","text":""},{"location":"examples/chain_of_thought/#without-chain-of-thought","title":"Without Chain of Thought","text":"<pre><code>from udspy import Predict\n\npredictor = Predict(QA)\nresult = predictor(question=\"Why is the sky blue?\")\n\nprint(result.answer)\n# \"The sky is blue due to Rayleigh scattering.\"\n</code></pre>"},{"location":"examples/chain_of_thought/#with-chain-of-thought","title":"With Chain of Thought","text":"<pre><code>cot_predictor = ChainOfThought(QA)\nresult = cot_predictor(question=\"Why is the sky blue?\")\n\nprint(result.reasoning)\n# \"Let me explain the physics: Sunlight contains all colors. As it enters\n#  the atmosphere, it interacts with air molecules. Blue light has shorter\n#  wavelengths and scatters more than other colors (Rayleigh scattering).\n#  This scattered blue light reaches our eyes from all directions.\"\n\nprint(result.answer)\n# \"The sky appears blue because blue light scatters more in the atmosphere\n#  due to its shorter wavelength (Rayleigh scattering).\"\n</code></pre> <p>Benefits: - More detailed and accurate answers - Shows the reasoning process - Better for complex or multi-step problems - Easier to verify correctness</p>"},{"location":"examples/chain_of_thought/#best-practices","title":"Best Practices","text":""},{"location":"examples/chain_of_thought/#1-use-for-complex-tasks","title":"1. Use for Complex Tasks","text":"<p>Chain of Thought shines for tasks requiring reasoning:</p> <pre><code># Good use cases\n- Math problems\n- Logic puzzles\n- Multi-step analysis\n- Proof generation\n- Planning tasks\n\n# Less useful for\n- Simple factual recall (\"What is 2+2?\")\n- Classification without reasoning\n- Direct information retrieval\n</code></pre>"},{"location":"examples/chain_of_thought/#2-adjust-temperature","title":"2. Adjust Temperature","text":"<pre><code># For deterministic tasks (math, logic)\ncot = ChainOfThought(QA, temperature=0.0)\n\n# For creative reasoning\ncot = ChainOfThought(QA, temperature=0.7)\n</code></pre>"},{"location":"examples/chain_of_thought/#3-review-reasoning-quality","title":"3. Review Reasoning Quality","text":"<p>Always check if reasoning makes sense:</p> <pre><code>result = cot(question=\"Complex problem\")\n\nif \"step\" in result.reasoning.lower():\n    print(\"\u2713 Good reasoning structure\")\n\nif len(result.reasoning) &lt; 50:\n    print(\"\u26a0 Reasoning might be too brief\")\n</code></pre>"},{"location":"examples/chain_of_thought/#real-world-examples","title":"Real-World Examples","text":""},{"location":"examples/chain_of_thought/#code-review-reasoning","title":"Code Review Reasoning","text":"<pre><code>class CodeReview(Signature):\n    \"\"\"Review code for issues.\"\"\"\n    code: str = InputField()\n    issues: list[str] = OutputField()\n    severity: str = OutputField()\n\nreviewer = ChainOfThought(CodeReview)\nresult = reviewer(code=\"\"\"\ndef divide(a, b):\n    return a / b\n\"\"\")\n\nprint(result.reasoning)\n# \"Let me analyze this code:\n#  1. No error handling for division by zero\n#  2. No type checking\n#  3. No documentation\n#  These are significant issues.\"\n\nprint(result.issues)\n# [\"Division by zero not handled\", \"Missing type hints\", \"No docstring\"]\n\nprint(result.severity)\n# \"High - can cause runtime errors\"\n</code></pre>"},{"location":"examples/chain_of_thought/#decision-making","title":"Decision Making","text":"<pre><code>class Decision(Signature):\n    \"\"\"Make informed decisions.\"\"\"\n    situation: str = InputField()\n    options: list[str] = InputField()\n    decision: str = OutputField()\n    justification: str = OutputField()\n\ndecider = ChainOfThought(Decision)\nresult = decider(\n    situation=\"Need to scale database, budget is tight\",\n    options=[\"Vertical scaling\", \"Horizontal scaling\", \"Managed service\"]\n)\n\nprint(result.reasoning)\n# \"Let me evaluate each option:\n#  - Vertical: Quick but limited and expensive long-term\n#  - Horizontal: Complex but scalable\n#  - Managed: Higher cost but less maintenance\n#  Given budget constraints...\"\n\nprint(result.decision)\nprint(result.justification)\n</code></pre> <p>See the full example in the repository.</p>"},{"location":"examples/context_settings/","title":"Context-Specific Settings","text":"<p>Learn how to use different API keys, models, and settings in different contexts.</p>"},{"location":"examples/context_settings/#overview","title":"Overview","text":"<p>The <code>settings.context()</code> context manager allows you to temporarily override global settings for specific operations. This is useful for:</p> <ul> <li>Multi-tenant applications with different API keys per user</li> <li>Testing with different models</li> <li>Varying temperature or other parameters per request</li> <li>Isolating settings in async operations</li> </ul> <p>The context manager is thread-safe using Python's <code>contextvars</code>, making it safe for concurrent operations.</p>"},{"location":"examples/context_settings/#basic-usage","title":"Basic Usage","text":""},{"location":"examples/context_settings/#override-model","title":"Override Model","text":"<pre><code>import udspy\n\n# Configure global settings\nudspy.settings.configure(api_key=\"sk-global\", model=\"gpt-4o-mini\")\n\n# Temporarily use a different model\nwith udspy.settings.context(model=\"gpt-4\"):\n    predictor = Predict(QA)\n    result = predictor(question=\"What is AI?\")\n    # Uses gpt-4\n\n# Back to global settings (gpt-4o-mini)\nresult = predictor(question=\"What is ML?\")\n</code></pre>"},{"location":"examples/context_settings/#override-api-key","title":"Override API Key","text":"<pre><code># Use a different API key for specific requests\nwith udspy.settings.context(api_key=\"sk-user-specific\"):\n    result = predictor(question=\"User-specific query\")\n    # Uses the user-specific API key\n</code></pre>"},{"location":"examples/context_settings/#override-multiple-settings","title":"Override Multiple Settings","text":"<pre><code>with udspy.settings.context(\n    model=\"gpt-4\",\n    temperature=0.0,\n    max_tokens=500\n):\n    result = predictor(question=\"Deterministic response needed\")\n</code></pre>"},{"location":"examples/context_settings/#multi-tenant-applications","title":"Multi-Tenant Applications","text":"<p>Handle different users with different API keys:</p> <pre><code>def handle_user_request(user_id: str, question: str):\n    \"\"\"Handle a request from a specific user.\"\"\"\n    # Get user-specific API key from database\n    user_api_key = get_user_api_key(user_id)\n\n    # Use user's API key for this request\n    with udspy.settings.context(api_key=user_api_key):\n        predictor = Predict(QA)\n        result = predictor(question=question)\n\n    return result.answer\n\n# Each user's request uses their own API key\nanswer1 = handle_user_request(\"user1\", \"What is Python?\")\nanswer2 = handle_user_request(\"user2\", \"What is Rust?\")\n</code></pre>"},{"location":"examples/context_settings/#nested-contexts","title":"Nested Contexts","text":"<p>Contexts can be nested, with inner contexts overriding outer ones:</p> <pre><code>udspy.settings.configure(model=\"gpt-4o-mini\", temperature=0.7)\n\nwith udspy.settings.context(model=\"gpt-4\", temperature=0.5):\n    # Uses gpt-4, temp=0.5\n\n    with udspy.settings.context(temperature=0.0):\n        # Uses gpt-4 (inherited), temp=0.0 (overridden)\n        pass\n\n    # Back to gpt-4, temp=0.5\n\n# Back to gpt-4o-mini, temp=0.7\n</code></pre>"},{"location":"examples/context_settings/#async-support","title":"Async Support","text":"<p>Context managers work seamlessly with async code:</p> <pre><code>import asyncio\n\nasync def generate_response(question: str, user_api_key: str):\n    with udspy.settings.context(api_key=user_api_key):\n        predictor = StreamingPredict(QA)\n        async for chunk in predictor.stream(question=question):\n            yield chunk\n\n# Handle multiple users concurrently\nasync def main():\n    tasks = [\n        generate_response(\"Question 1\", \"sk-user1\"),\n        generate_response(\"Question 2\", \"sk-user2\"),\n    ]\n    await asyncio.gather(*tasks)\n</code></pre>"},{"location":"examples/context_settings/#testing","title":"Testing","text":"<p>Use contexts to isolate test settings:</p> <pre><code>def test_with_specific_model():\n    \"\"\"Test behavior with a specific model.\"\"\"\n    with udspy.settings.context(\n        api_key=\"sk-test\",\n        model=\"gpt-4\",\n        temperature=0.0,  # Deterministic for testing\n    ):\n        predictor = Predict(QA)\n        result = predictor(question=\"2+2\")\n        assert \"4\" in result.answer\n</code></pre>"},{"location":"examples/context_settings/#custom-clients","title":"Custom Clients","text":"<p>You can also provide custom OpenAI clients:</p> <pre><code>from openai import OpenAI, AsyncOpenAI\n\ncustom_client = OpenAI(\n    api_key=\"sk-custom\",\n    base_url=\"https://custom-endpoint.example.com\",\n)\n\nwith udspy.settings.context(client=custom_client):\n    # Uses custom client with custom endpoint\n    result = predictor(question=\"...\")\n</code></pre>"},{"location":"examples/context_settings/#complete-example","title":"Complete Example","text":"<pre><code>import udspy\nfrom udspy import Signature, InputField, OutputField, Predict\n\n# Global configuration\nudspy.settings.configure(\n    api_key=\"sk-default\",\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n)\n\nclass QA(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\n\n# Scenario 1: Default settings\nresult = predictor(question=\"What is AI?\")\n\n# Scenario 2: High-quality request (use GPT-4)\nwith udspy.settings.context(model=\"gpt-4\"):\n    result = predictor(question=\"Explain quantum computing\")\n\n# Scenario 3: Deterministic response\nwith udspy.settings.context(temperature=0.0):\n    result = predictor(question=\"What is 2+2?\")\n\n# Scenario 4: User-specific API key\nwith udspy.settings.context(api_key=user.api_key):\n    result = predictor(question=user.question)\n</code></pre> <p>See the full example in the repository.</p>"},{"location":"examples/history/","title":"Conversation History","text":"<p>The <code>History</code> class manages conversation history for multi-turn interactions. When passed to <code>Predict</code>, it automatically maintains context across multiple calls.</p>"},{"location":"examples/history/#basic-usage","title":"Basic Usage","text":"<pre><code>from udspy import History, Predict, Signature, InputField, OutputField\n\nclass QA(Signature):\n    '''Answer questions.'''\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\nhistory = History()\n\n# First turn\nresult = predictor(question=\"What is Python?\", history=history)\nprint(result.answer)\n\n# Second turn - context is maintained\nresult = predictor(question=\"What are its main features?\", history=history)\nprint(result.answer)  # Assistant knows we're still talking about Python\n</code></pre>"},{"location":"examples/history/#how-it-works","title":"How It Works","text":"<p><code>History</code> stores messages in OpenAI format and automatically: - Adds user messages when you call the predictor - Adds assistant responses after generation - Maintains tool calls and results (when using tool calling) - Preserves conversation context across turns</p>"},{"location":"examples/history/#api","title":"API","text":""},{"location":"examples/history/#creating-history","title":"Creating History","text":"<pre><code># Empty history\nhistory = History()\n\n# With initial messages\nhistory = History(messages=[\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"}\n])\n</code></pre>"},{"location":"examples/history/#adding-messages","title":"Adding Messages","text":"<pre><code># Add user message\nhistory.add_user_message(\"What is AI?\")\n\n# Add assistant message\nhistory.add_assistant_message(\"AI stands for Artificial Intelligence...\")\n\n# Add system message\nhistory.add_system_message(\"You are a helpful tutor\")\n\n# Add tool result\nhistory.add_tool_result(tool_call_id=\"call_123\", content=\"Result: 42\")\n\n# Add generic message\nhistory.add_message(\"user\", \"Custom message\")\n</code></pre>"},{"location":"examples/history/#managing-history","title":"Managing History","text":"<pre><code># Get number of messages\nprint(len(history))  # e.g., 5\n\n# Clear all messages\nhistory.clear()\n\n# Copy history (for branching conversations)\nbranch = history.copy()\n\n# Access messages directly\nfor msg in history.messages:\n    print(f\"{msg['role']}: {msg['content']}\")\n\n# String representation\nprint(history)  # Shows formatted conversation\n</code></pre>"},{"location":"examples/history/#use-cases","title":"Use Cases","text":""},{"location":"examples/history/#multi-turn-conversations","title":"Multi-Turn Conversations","text":"<pre><code>predictor = Predict(QA)\nhistory = History()\n\n# Each call maintains context\npredictor(question=\"What is machine learning?\", history=history)\npredictor(question=\"How does it differ from traditional programming?\", history=history)\npredictor(question=\"Can you give me an example?\", history=history)\n</code></pre>"},{"location":"examples/history/#pre-populating-context","title":"Pre-Populating Context","text":"<pre><code>history = History()\n\n# Set up initial context\nhistory.add_system_message(\"You are a Python expert. Keep answers concise.\")\nhistory.add_user_message(\"I'm learning Python\")\nhistory.add_assistant_message(\"Great! I'm here to help.\")\n\n# Now ask questions with this context\nresult = predictor(question=\"How do I use list comprehensions?\", history=history)\n</code></pre>"},{"location":"examples/history/#branching-conversations","title":"Branching Conversations","text":"<pre><code>main_history = History()\n\n# Start main conversation\npredictor(question=\"Tell me about programming languages\", history=main_history)\n\n# Branch 1: Explore Python\npython_branch = main_history.copy()\npredictor(question=\"Tell me more about Python\", history=python_branch)\n\n# Branch 2: Explore JavaScript\njs_branch = main_history.copy()\npredictor(question=\"Tell me more about JavaScript\", history=js_branch)\n\n# Each branch maintains independent context\n</code></pre>"},{"location":"examples/history/#conversation-reset","title":"Conversation Reset","text":"<pre><code>history = History()\n\n# First conversation\npredictor(question=\"What is Python?\", history=history)\n\n# Reset for new topic\nhistory.clear()\n\n# New conversation with no context\npredictor(question=\"What is JavaScript?\", history=history)\n</code></pre>"},{"location":"examples/history/#history-with-tool-calling","title":"History with Tool Calling","text":"<pre><code>from udspy import tool\nfrom pydantic import Field\n\n@tool(name=\"Calculator\", description=\"Perform calculations\")\ndef calculator(operation: str = Field(...), a: float = Field(...), b: float = Field(...)) -&gt; float:\n    ops = {\"add\": a + b, \"subtract\": a - b, \"multiply\": a * b, \"divide\": a / b}\n    return ops[operation]\n\npredictor = Predict(QA, tools=[calculator])\nhistory = History()\n\n# Tool calls are automatically recorded in history\nresult = predictor(question=\"What is 15 times 23?\", history=history)\n# History now contains: user message, assistant tool call, tool result, final assistant answer\n\n# Next turn has full context including tool usage\nresult = predictor(question=\"Now add 100 to that\", history=history)\n</code></pre>"},{"location":"examples/history/#best-practices","title":"Best Practices","text":"<ol> <li>One History per Conversation Thread: Create a new <code>History</code> instance for each independent conversation</li> <li>Use <code>copy()</code> for Branching: When you want to explore different paths from the same starting point</li> <li>Clear When Changing Topics: Use <code>history.clear()</code> when starting a completely new conversation</li> <li>Pre-populate for Context: Add system messages or previous conversation history to set context</li> <li>Inspect Messages: Access <code>history.messages</code> directly when you need to debug or log conversations</li> </ol>"},{"location":"examples/history/#async-support","title":"Async Support","text":"<p>History works seamlessly with all async patterns:</p> <pre><code># Async streaming\nasync for event in predictor.astream(question=\"...\", history=history):\n    if isinstance(event, StreamChunk):\n        print(event.delta, end=\"\", flush=True)\n\n# Async non-streaming\nresult = await predictor.aforward(question=\"...\", history=history)\n\n# Sync (uses asyncio.run internally)\nresult = predictor(question=\"...\", history=history)\n</code></pre>"},{"location":"examples/history/#examples","title":"Examples","text":"<p>See history.py for complete working examples.</p>"},{"location":"examples/streaming/","title":"Streaming Examples","text":"<p>Learn how to use streaming for better user experience.</p>"},{"location":"examples/streaming/#basic-streaming","title":"Basic Streaming","text":"<pre><code>import asyncio\nfrom udspy import StreamingPredict, Signature, InputField, OutputField\n\nclass QA(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\nasync def main():\n    predictor = StreamingPredict(QA)\n\n    async for chunk in predictor.stream(question=\"What is AI?\"):\n        if isinstance(chunk, StreamChunk):\n            print(chunk.delta, end=\"\", flush=True)\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/streaming/#multi-field-streaming","title":"Multi-Field Streaming","text":"<p>Stream reasoning and answer separately:</p> <pre><code>class ReasonedQA(Signature):\n    \"\"\"Answer with reasoning.\"\"\"\n    question: str = InputField()\n    reasoning: str = OutputField()\n    answer: str = OutputField()\n\nasync def main():\n    predictor = StreamingPredict(ReasonedQA)\n\n    print(\"Question: What is the sum of first 10 primes?\\n\")\n\n    async for item in predictor.stream(\n        question=\"What is the sum of first 10 primes?\"\n    ):\n        if isinstance(item, StreamChunk):\n            if item.field_name == \"reasoning\":\n                print(f\"\ud83d\udcad {item.delta}\", end=\"\", flush=True)\n            elif item.field_name == \"answer\":\n                print(f\"\\n\u2713 {item.delta}\", end=\"\", flush=True)\n\n            if item.is_complete:\n                print()  # Newline after field completes\n\n        elif isinstance(item, Prediction):\n            print(f\"\\n\\nFinal: {item.answer}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/streaming/#web-application-integration","title":"Web Application Integration","text":"<p>Use streaming in a web application:</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\n\napp = FastAPI()\n\n@app.get(\"/ask\")\nasync def ask_question(question: str):\n    async def generate():\n        predictor = StreamingPredict(QA)\n        async for chunk in predictor.stream(question=question):\n            if isinstance(chunk, StreamChunk) and not chunk.is_complete:\n                # chunk.delta contains the new incremental text\n                # chunk.content contains the full accumulated text so far\n                yield f\"data: {chunk.delta}\\n\\n\"\n\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n</code></pre> <p>See the full example in the repository.</p>"},{"location":"examples/tool_calling/","title":"Tool Calling","text":"<p>Learn how to use OpenAI's native tool calling with udspy.</p>"},{"location":"examples/tool_calling/#two-ways-to-use-tools","title":"Two Ways to Use Tools","text":"<p>udspy supports two approaches to tool calling:</p> <ol> <li>Automatic Execution with <code>@tool</code> decorator (Recommended) - Tools are automatically executed</li> <li>Manual Execution with Pydantic models - You handle tool execution yourself</li> </ol>"},{"location":"examples/tool_calling/#automatic-tool-execution-recommended","title":"Automatic Tool Execution (Recommended)","text":"<p>Use the <code>@tool</code> decorator to mark functions as executable tools. udspy will automatically execute them and handle multi-turn conversations:</p> <pre><code>from pydantic import Field\nfrom udspy import tool, Predict, Signature, InputField, OutputField\n\n@tool(name=\"Calculator\", description=\"Perform arithmetic operations\")\ndef calculator(\n    operation: str = Field(description=\"add, subtract, multiply, or divide\"),\n    a: float = Field(description=\"First number\"),\n    b: float = Field(description=\"Second number\"),\n) -&gt; float:\n    \"\"\"Execute calculator operation.\"\"\"\n    ops = {\"add\": a + b, \"subtract\": a - b, \"multiply\": a * b, \"divide\": a / b}\n    return ops[operation]\n\nclass MathQuery(Signature):\n    \"\"\"Answer math questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\n# Tools decorated with @tool are automatically executed\npredictor = Predict(MathQuery, tools=[calculator])\nresult = predictor(question=\"What is 157 times 234?\")\nprint(result.answer)  # \"The answer is 36738\"\n</code></pre> <p>The predictor automatically: 1. Detects when the LLM wants to call a tool 2. Executes the tool function 3. Sends the result back to the LLM 4. Returns the final answer</p>"},{"location":"examples/tool_calling/#optional-tool-execution","title":"Optional Tool Execution","text":"<p>You can control whether tools are automatically executed:</p> <pre><code># Default: auto_execute_tools=True\nresult = predictor(question=\"What is 5 + 3?\")\nprint(result.answer)  # \"The answer is 8\"\n\n# Get tool calls without execution\nresult = predictor(question=\"What is 5 + 3?\", auto_execute_tools=False)\nif \"tool_calls\" in result:\n    print(f\"LLM wants to call: {result.tool_calls[0]['name']}\")\n    print(f\"With arguments: {result.tool_calls[0]['arguments']}\")\n    # Now you can execute manually or log/analyze the tool calls\n</code></pre> <p>This is useful for: - Requiring user approval before executing tools - Logging or analyzing tool usage patterns - Implementing custom execution logic - Rate limiting or caching tool results</p>"},{"location":"examples/tool_calling/#manual-tool-execution","title":"Manual Tool Execution","text":"<p>Define tools as Pydantic models when you want full control:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass Calculator(BaseModel):\n    \"\"\"Perform arithmetic operations.\"\"\"\n    operation: str = Field(description=\"add, subtract, multiply, or divide\")\n    a: float = Field(description=\"First number\")\n    b: float = Field(description=\"Second number\")\n\n# Pydantic models are schema-only (not automatically executed)\npredictor = Predict(MathQuery, tools=[Calculator])\nresult = predictor(question=\"What is 157 times 234?\")\n\n# You must check for and execute tool calls yourself\nif \"tool_calls\" in result:\n    for tool_call in result.tool_calls:\n        print(f\"Called: {tool_call['name']}\")\n        print(f\"Arguments: {tool_call['arguments']}\")\n        # Execute manually and construct follow-up messages\n</code></pre>"},{"location":"examples/tool_calling/#multiple-tools","title":"Multiple Tools","text":"<p>Provide multiple tools for different operations:</p> <pre><code>class Calculator(BaseModel):\n    \"\"\"Perform arithmetic operations.\"\"\"\n    operation: str\n    a: float\n    b: float\n\nclass WebSearch(BaseModel):\n    \"\"\"Search the web.\"\"\"\n    query: str = Field(description=\"Search query\")\n\nclass DateInfo(BaseModel):\n    \"\"\"Get date information.\"\"\"\n    timezone: str = Field(description=\"Timezone name\")\n\npredictor = Predict(\n    signature,\n    tools=[Calculator, WebSearch, DateInfo],\n)\n</code></pre>"},{"location":"examples/tool_calling/#tool-execution","title":"Tool Execution","text":"<p>Execute tool calls and continue the conversation:</p> <pre><code>def execute_calculator(operation: str, a: float, b: float) -&gt; float:\n    \"\"\"Execute calculator tool.\"\"\"\n    ops = {\n        \"add\": lambda x, y: x + y,\n        \"subtract\": lambda x, y: x - y,\n        \"multiply\": lambda x, y: x * y,\n        \"divide\": lambda x, y: x / y,\n    }\n    return ops[operation](a, b)\n\n# Get initial response with tool calls\nresult = predictor(question=\"What is 15 * 23?\")\n\nif \"tool_calls\" in result:\n    # Execute tool calls\n    tool_results = []\n    for tool_call in result.tool_calls:\n        if tool_call[\"name\"] == \"Calculator\":\n            args = json.loads(tool_call[\"arguments\"])\n            result_value = execute_calculator(**args)\n            tool_results.append({\n                \"id\": tool_call[\"id\"],\n                \"result\": result_value,\n            })\n\n    # Continue conversation with tool results\n    # (requires manual message construction - see advanced examples)\n</code></pre> <p>See the full example in the repository.</p>"}]}