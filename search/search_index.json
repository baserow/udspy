{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"udspy","text":"<p>A minimal DSPy-inspired library with native OpenAI tool calling.</p>"},{"location":"#overview","title":"Overview","text":"<p>udspy provides a clean, minimal abstraction for building LLM-powered applications with structured inputs and outputs. Heavily inspired by DSPy, it aims to mimic DSPy's excellent API patterns while avoiding the LiteLLM dependency for resource-constrained environments.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Pydantic-based Signatures: Define clear input/output contracts using Pydantic models</li> <li>Native Tool Calling: First-class support for OpenAI's function calling API</li> <li>Module Abstraction: Compose LLM calls into reusable, testable modules</li> <li>Streaming Support: Stream reasoning and outputs incrementally for better UX</li> <li>Minimal Dependencies: Only requires <code>openai</code> and <code>pydantic</code></li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install udspy\n</code></pre> <p>Or with uv:</p> <pre><code>uv add udspy\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import udspy\nfrom udspy import Signature, InputField, OutputField, Predict, LM\n\n# Configure with LM instance\nlm = LM(model=\"gpt-4o-mini\", api_key=\"your-api-key\")\nudspy.settings.configure(lm=lm)\n\n# Define a signature\nclass QA(Signature):\n    \"\"\"Answer questions concisely.\"\"\"\n    question: str = InputField(description=\"Question to answer\")\n    answer: str = OutputField(description=\"Concise answer\")\n\n# Create and use a predictor\npredictor = Predict(QA)\nresult = predictor(question=\"What is the capital of France?\")\nprint(result.answer)  # \"Paris\"\n</code></pre>"},{"location":"#philosophy","title":"Philosophy","text":"<p>udspy is designed with these principles:</p> <ol> <li>Simplicity First: Start minimal, iterate based on real needs</li> <li>Type Safety: Leverage Pydantic for runtime validation</li> <li>Native Integration: Use platform features (like OpenAI tools) instead of reinventing</li> <li>Testability: Make it easy to test LLM-powered code</li> <li>Composability: Build complex behavior from simple, reusable modules</li> </ol>"},{"location":"#relationship-with-dspy","title":"Relationship with DSPy","text":"<p>udspy is heavily inspired by DSPy and aims to provide a compatible API for common use cases. The main differences are:</p> Aspect udspy DSPy Philosophy Minimal abstractions for specific use cases Full-featured framework with optimizers Dependencies ~10MB (openai, pydantic) ~200MB (includes LiteLLM, many providers) Target Resource-constrained environments, Baserow AI General-purpose LLM applications Scope Core patterns (Predict, ChainOfThought, ReAct) Extensive toolkit with teleprompters, optimizers Tool Calling OpenAI-native function calling Provider-agnostic adapter layer <p>Use DSPy if you need: Multiple LLM providers, optimization/teleprompters, research capabilities, full ecosystem.</p> <p>Use udspy if you need: Minimal footprint, OpenAI-focused, simpler deployment, dynamic tool calling, reasoning and good streaming.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Read the Architecture Overview</li> <li>Check out Examples</li> <li>Browse the API Reference</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE file for details.</p>"},{"location":"api/adapter/","title":"API Reference: Adapters","text":""},{"location":"api/adapter/#udspy.adapter","title":"<code>udspy.adapter</code>","text":"<p>Adapter for formatting LLM inputs/outputs with Pydantic models.</p>"},{"location":"api/adapter/#udspy.adapter-classes","title":"Classes","text":""},{"location":"api/adapter/#udspy.adapter.ChatAdapter","title":"<code>ChatAdapter</code>","text":"<p>Adapter for formatting signatures into OpenAI chat messages.</p> <p>This adapter converts Signature inputs into properly formatted chat messages and parses LLM responses back into structured outputs.</p> <p>The adapter handles both streaming and non-streaming responses: - For streaming: Call process_message() for each chunk, then finalize() - For non-streaming: Call process_message() once with the complete response</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>class ChatAdapter:\n    \"\"\"Adapter for formatting signatures into OpenAI chat messages.\n\n    This adapter converts Signature inputs into properly formatted\n    chat messages and parses LLM responses back into structured outputs.\n\n    The adapter handles both streaming and non-streaming responses:\n    - For streaming: Call process_message() for each chunk, then finalize()\n    - For non-streaming: Call process_message() once with the complete response\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the adapter.\"\"\"\n        self._streaming_parser: StreamingParser | None = None\n\n    def _get_or_create_parser(\n        self,\n        module: Any,\n        signature: type[Signature],\n    ) -&gt; StreamingParser:\n        \"\"\"Get existing parser or create a new one.\n\n        Args:\n            module: Module instance\n            signature: The signature defining expected outputs\n\n        Returns:\n            StreamingParser instance\n        \"\"\"\n        if self._streaming_parser is None:\n            self._streaming_parser = StreamingParser(self, module, signature)\n        return self._streaming_parser\n\n    def reset_parser(self) -&gt; None:\n        \"\"\"Reset the streaming parser for a new request.\"\"\"\n        self._streaming_parser = None\n\n    async def process_chunk(\n        self,\n        chunk: \"ChatCompletionChunk\",\n        module: Any,\n        signature: type[Signature],\n    ) -&gt; Any:\n        \"\"\"Process an LLM streaming chunk.\n\n        This method processes streaming chunks and yields StreamEvent objects.\n        After processing all chunks, call finalize() to get validated outputs.\n\n        Args:\n            chunk: ChatCompletionChunk from streaming LLM\n            module: Module instance\n            signature: Signature defining expected outputs\n\n        Yields:\n            StreamEvent objects (ThoughtStreamChunk, OutputStreamChunk, etc.)\n        \"\"\"\n\n        parser = self._get_or_create_parser(module, signature)\n        async for event in parser.process_chunk(chunk):\n            yield event\n\n    async def finalize(\n        self,\n        signature: type[Signature],\n    ) -&gt; tuple[dict[str, Any], list[Any], str]:\n        \"\"\"Finalize streaming response and validate outputs.\n\n        Must be called after processing all streaming chunks.\n\n        Args:\n            signature: Signature defining expected outputs\n\n        Returns:\n            Tuple of (outputs, native_tool_calls, completion_text)\n\n        Raises:\n            AdapterParseError: If outputs don't match signature or parsing fails\n        \"\"\"\n        if self._streaming_parser is None:\n            raise RuntimeError(\"No streaming parser available - call process_message first\")\n\n        (\n            outputs,\n            native_tool_calls,\n            completion_text,\n        ) = await self._streaming_parser.finalize()\n\n        # Validate outputs match signature\n        self.validate_outputs(signature, outputs, native_tool_calls, completion_text)\n\n        # Reset for next request\n        self.reset_parser()\n\n        return outputs, native_tool_calls, completion_text\n\n    def validate_outputs(\n        self,\n        signature: type[Signature],\n        outputs: dict[str, Any],\n        native_tool_calls: list[Any],\n        completion_text: str,\n    ) -&gt; None:\n        \"\"\"Validate that outputs match the signature.\n\n        Args:\n            signature: Signature defining expected outputs\n            outputs: Parsed output fields\n            native_tool_calls: Tool calls from LLM\n            completion_text: Raw completion text\n\n        Raises:\n            AdapterParseError: If outputs don't match signature\n        \"\"\"\n\n        # If no tool calls, outputs must match signature exactly\n        output_match_keys = outputs.keys() == signature.get_output_fields().keys()\n        if not native_tool_calls and not output_match_keys:\n            raise AdapterParseError(\n                adapter_name=self.__class__.__name__,\n                signature=signature,\n                lm_response=completion_text,\n                parsed_result=outputs,\n            )\n\n    def split_reasoning_and_content_delta(\n        self,\n        response_or_chunk: Union[\"ChatCompletion\", \"ChatCompletionChunk\"],\n    ) -&gt; tuple[str, str]:\n        \"\"\"Split reasoning and content delta from a streaming chunk.\n\n        This handles provider-specific reasoning formats:\n        - OpenAI: choice.delta.reasoning (structured field)\n        - AWS Bedrock: &lt;reasoning&gt;...&lt;/reasoning&gt; tags in content\n        - Other providers: may use different formats\n\n        Args:\n            response_or_chunk: ChatCompletion or ChatCompletionChunk from streaming LLM\n\n        Returns:\n            Tuple of (reasoning_delta, content_delta) where:\n            - reasoning_delta: New reasoning content in this chunk\n            - content_delta: Content excluding reasoning\n        \"\"\"\n        from openai.types.chat import ChatCompletion\n\n        if isinstance(response_or_chunk, ChatCompletion):\n            message = response_or_chunk.choices[0].message\n            message_content = message.content or \"\"\n            reasoning_delta = getattr(message, \"reasoning\", None) or \"\"\n        else:\n            delta = response_or_chunk.choices[0].delta\n            message_content = delta.content or \"\"\n            reasoning_delta = getattr(delta, \"reasoning\", None) or \"\"\n\n        # For some providers (like AWS Bedrock), reasoning is returned as\n        # &lt;reasoning&gt; tags inside choice.delta.content instead of choice.delta.reasoning\n        remaining_content = message_content\n        if match := re.search(\n            r\"&lt;reasoning&gt;(.*?)&lt;/reasoning&gt;(.*)\",\n            message_content,\n            re.DOTALL | re.IGNORECASE,\n        ):\n            reasoning_delta = match.group(1).strip()\n            remaining_content = match.group(2).strip()\n\n        return reasoning_delta, remaining_content\n\n    def process_tool_call_deltas(\n        self,\n        tool_calls_accumulator: dict[int, dict[str, Any]],\n        delta_tool_calls: list[Any],\n    ) -&gt; None:\n        \"\"\"Process tool call deltas from streaming response.\n\n        Accumulates tool call information across multiple chunks, handling\n        incremental updates to tool call names and arguments.\n\n        Args:\n            tool_calls_accumulator: Dictionary mapping tool call index to accumulated data\n            delta_tool_calls: List of tool call delta objects from the current chunk\n        \"\"\"\n        for tc_delta in delta_tool_calls:\n            idx = tc_delta.index\n            if idx not in tool_calls_accumulator:\n                tool_calls_accumulator[idx] = {\n                    \"id\": tc_delta.id,\n                    \"type\": tc_delta.type,\n                    \"function\": {\"name\": \"\", \"arguments\": \"\"},\n                }\n\n            if tc_delta.function:\n                if tc_delta.function.name:\n                    tool_calls_accumulator[idx][\"function\"][\"name\"] += tc_delta.function.name\n                if tc_delta.function.arguments:\n                    tool_calls_accumulator[idx][\"function\"][\"arguments\"] += (\n                        tc_delta.function.arguments\n                    )\n\n    def finalize_tool_calls(self, tool_calls_accumulator: dict[int, dict[str, Any]]) -&gt; list[Any]:\n        \"\"\"Convert accumulated tool calls to ToolCall objects.\n\n        Parses JSON arguments and creates properly formatted ToolCall instances.\n\n        Args:\n            tool_calls_accumulator: Dictionary of accumulated tool call data\n\n        Returns:\n            List of ToolCall objects\n\n        Raises:\n            AdapterParseError: If tool call arguments are not valid JSON\n        \"\"\"\n\n        native_tool_calls = []\n        for tc in tool_calls_accumulator.values():\n            try:\n                arguments = (\n                    json.loads(tc[\"function\"][\"arguments\"])\n                    if isinstance(tc[\"function\"][\"arguments\"], str)\n                    else tc[\"function\"][\"arguments\"]\n                )\n            except json.JSONDecodeError as exc:\n                raise AdapterParseError(\n                    adapter_name=self.__class__.__name__,\n                    signature=None,  # Tool calls don't have a signature\n                    lm_response=tc[\"function\"][\"arguments\"],\n                    message=f\"Failed to parse tool call {tc['id']} arguments as JSON: {exc}\",\n                ) from exc\n\n            native_tool_calls.append(\n                ToolCall(\n                    call_id=tc[\"id\"],\n                    name=tc[\"function\"][\"name\"],\n                    args=arguments,\n                )\n            )\n\n        return native_tool_calls\n\n    def format_field_structure(self, signature: type[Signature]) -&gt; str:\n        \"\"\"Format example field structure with type hints for the LLM.\n\n        Shows the LLM exactly how to structure inputs and outputs, including\n        type constraints for non-string fields. This helps the LLM understand\n        what format each field should use (e.g., integers, booleans, JSON objects).\n\n        Args:\n            signature: The signature defining input/output fields\n\n        Returns:\n            Formatted string showing field structure with type hints\n        \"\"\"\n        parts = []\n        parts.append(\n            \"All interactions will be structured in the following way, with the appropriate values filled in.\"\n        )\n\n        # Format input fields\n        input_fields = signature.get_input_fields()\n        if input_fields:\n            for name, field_info in input_fields.items():\n                type_hint = translate_field_type(name, field_info)\n                parts.append(f\"[[ ## {name} ## ]]\\n{type_hint}\")\n\n        # Format output fields\n        output_fields = signature.get_output_fields()\n        if output_fields:\n            for name, field_info in output_fields.items():\n                type_hint = translate_field_type(name, field_info)\n                parts.append(f\"[[ ## {name} ## ]]\\n{type_hint}\")\n\n        # Add completion marker\n        parts.append(\"[[ ## completed ## ]]\")\n\n        return \"\\n\\n\".join(parts).strip()\n\n    def format_instructions(self, signature: type[Signature]) -&gt; str:\n        \"\"\"Format signature instructions and field descriptions for system message.\n\n        This now only includes the task description and input/output field descriptions,\n        without the output formatting structure (which is moved to the user message).\n\n        Args:\n            signature: The signature to format\n\n        Returns:\n            Formatted instruction string for system message\n        \"\"\"\n        parts = []\n\n        instructions = signature.get_instructions()\n        if instructions:\n            parts.append(instructions)\n\n        input_fields = [f\"`{name}`\" for name in signature.get_input_fields().keys()]\n        output_fields = [f\"`{name}`\" for name in signature.get_output_fields().keys()]\n        parts.append(\n            f\"Given the input fields: {', '.join(input_fields)}, produce the output fields: {', '.join(output_fields)}.\"\n        )\n\n        return \"\\n\".join(parts).strip()\n\n    def format_output_instructions(self, signature: type[Signature]) -&gt; str:\n        \"\"\"Format instructions for how to structure output fields in JSON.\n\n        This generates the part that tells the LLM how to respond with output fields\n        as a JSON object.\n\n        Args:\n            signature: The signature defining expected outputs\n\n        Returns:\n            Formatted output instructions string\n        \"\"\"\n        output_fields = signature.get_output_fields()\n        if not output_fields:\n            return \"\"\n\n        parts = []\n        parts.append(\"\\n\\nRespond with a JSON object containing the following fields:\\n\")\n\n        # List all required fields\n        for name, field_info in output_fields.items():\n            type_hint = translate_field_type(name, field_info)\n            # Extract constraint if exists\n            constraint = \"\"\n            if \"# note:\" in type_hint:\n                constraint = \" - \" + type_hint.split(\"# note:\", 1)[1].strip()\n\n            type_name = (\n                getattr(field_info.annotation, \"__name__\", \"string\")\n                if field_info.annotation\n                else \"string\"\n            )\n            parts.append(f\"- `{name}`: {type_name}{constraint}\\n\")\n\n        parts.append(\"\\nReturn ONLY valid JSON with no additional text or markdown formatting.\")\n\n        return \"\".join(parts)\n\n    def format_inputs(\n        self,\n        signature: type[Signature],\n        inputs: dict[str, Any],\n    ) -&gt; str:\n        \"\"\"Format input values into a message.\n\n        Args:\n            signature: The signature defining expected inputs\n            inputs: Dictionary of input values\n\n        Returns:\n            Formatted input string\n        \"\"\"\n        parts = []\n        input_fields = signature.get_input_fields()\n\n        for name, _ in input_fields.items():\n            if name in inputs:\n                value = inputs[name]\n                formatted = format_value(value)\n                parts.append(f\"[[ ## {name} ## ]]\\n{formatted}\")\n\n        return \"\\n\\n\".join(parts)\n\n    def format_user_request(\n        self,\n        signature: type[Signature],\n        inputs: dict[str, Any],\n    ) -&gt; str:\n        \"\"\"Format complete user request with inputs and output instructions.\n\n        This combines the input values with instructions on how to format outputs,\n        creating a complete user message that tells the LLM what data it has and\n        how to respond.\n\n        Args:\n            signature: The signature defining inputs and outputs\n            inputs: Dictionary of input values\n\n        Returns:\n            Formatted user request string combining inputs + output instructions\n        \"\"\"\n        try:\n            formatted_inputs = self.format_inputs(signature, inputs)\n        except Exception as e:\n            raise ValueError(f\"Failed to format inputs: {e}\") from e\n        output_instructions = self.format_output_instructions(signature)\n\n        return formatted_inputs + output_instructions\n\n    def parse_outputs(\n        self,\n        signature: type[Signature],\n        completion: str,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Parse LLM completion into structured outputs.\n\n        Expects JSON format matching the signature's output fields.\n\n        Args:\n            signature: The signature defining expected outputs\n            completion: Raw completion string from LLM (should be JSON)\n\n        Returns:\n            Dictionary of parsed output values\n\n        Raises:\n            AdapterParseError: If completion is not valid JSON\n        \"\"\"\n\n        output_fields = signature.get_output_fields()\n\n        # Handle empty completion (tool calls without content)\n        if not completion or completion.strip() == \"\":\n            return {}\n\n        # Parse JSON completion\n        try:\n            outputs = json.loads(completion)\n        except json.JSONDecodeError as e:\n            raise AdapterParseError(\n                adapter_name=self.__class__.__name__,\n                signature=signature,\n                lm_response=completion,\n                message=f\"Failed to parse JSON output: {e}\",\n            ) from e\n\n        if not isinstance(outputs, dict):\n            raise AdapterParseError(\n                adapter_name=self.__class__.__name__,\n                signature=signature,\n                lm_response=completion,\n                message=f\"Expected JSON object, got {type(outputs).__name__}\",\n            )\n\n        parsed_outputs: dict[str, Any] = {}\n        for field_name, field_info in output_fields.items():\n            if field_name in outputs:\n                value = outputs[field_name]\n                field_type = field_info.annotation\n\n                # Parse value according to field type\n                try:\n                    # Check if field type is a Pydantic model\n                    if (\n                        field_type\n                        and isinstance(field_type, type)\n                        and issubclass(field_type, BaseModel)\n                    ):\n                        # Convert dict to Pydantic model\n                        if isinstance(value, dict):\n                            parsed_outputs[field_name] = field_type.model_validate(value)\n                        else:\n                            # Try parsing as string\n                            parsed_outputs[field_name] = parse_value(str(value), field_type)\n                    elif isinstance(value, str):\n                        # String value - parse according to type\n                        parsed_outputs[field_name] = parse_value(value, field_type)  # type: ignore[arg-type]\n                    else:\n                        # Value is already correct type (int, float, list, dict, etc.)\n                        parsed_outputs[field_name] = value\n                except Exception:\n                    # Fallback: keep original value\n                    parsed_outputs[field_name] = value\n\n        return parsed_outputs\n\n    def format_tool_schema(self, tool: Any) -&gt; dict[str, Any]:\n        \"\"\"Convert a Tool object or Pydantic model to OpenAI tool schema.\n\n        This is where provider-specific schema formatting happens. The adapter\n        takes the tool's normalized schema and converts it to OpenAI's expected format.\n\n        Args:\n            tool: Tool object or Pydantic model class\n\n        Returns:\n            OpenAI tool schema dictionary in the format:\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": str,\n                    \"description\": str,\n                    \"parameters\": dict  # Full JSON schema with type, properties, required\n                }\n            }\n        \"\"\"\n\n        if isinstance(tool, Tool):\n            # Tool decorator - construct OpenAI schema from Tool properties\n            # Tool.parameters gives us the complete resolved schema (type, properties, required)\n            return {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": tool.name,\n                    \"description\": tool.description,\n                    \"parameters\": tool.parameters,  # Already resolved, ready for OpenAI\n                },\n            }\n        else:\n            # Pydantic model - convert using existing logic\n            tool_model = tool\n            schema = tool_model.model_json_schema()\n\n            # Extract description from docstring or schema\n            description = (\n                tool_model.__doc__.strip()\n                if tool_model.__doc__\n                else schema.get(\"description\", f\"Use {tool_model.__name__}\")\n            )\n\n            # Build OpenAI function schema\n            # Resolve any $defs references in the Pydantic schema\n            tool_schema = resolve_json_schema_reference(\n                {\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": schema.get(\"title\", tool_model.__name__),\n                        \"description\": description,\n                        \"parameters\": {\n                            \"type\": \"object\",\n                            \"properties\": schema.get(\"properties\", {}),\n                            \"required\": schema.get(\"required\", []),\n                            \"additionalProperties\": False,\n                        },\n                    },\n                }\n            )\n\n            return tool_schema\n\n    def format_tool_schemas(self, tools: list[Any]) -&gt; list[dict[str, Any]]:\n        \"\"\"Convert Tool objects or Pydantic models to OpenAI tool schemas.\n\n        Args:\n            tools: List of Tool objects or Pydantic model classes\n\n        Returns:\n            List of OpenAI tool schema dictionaries\n        \"\"\"\n\n        tool_schemas = []\n\n        for tool_item in tools:\n            tool_schema = self.format_tool_schema(tool_item)\n            tool_schemas.append(tool_schema)\n\n        return tool_schemas\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter-functions","title":"Functions","text":""},{"location":"api/adapter/#udspy.adapter.ChatAdapter.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the adapter.</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the adapter.\"\"\"\n    self._streaming_parser: StreamingParser | None = None\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.finalize","title":"<code>finalize(signature)</code>  <code>async</code>","text":"<p>Finalize streaming response and validate outputs.</p> <p>Must be called after processing all streaming chunks.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature]</code> <p>Signature defining expected outputs</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], list[Any], str]</code> <p>Tuple of (outputs, native_tool_calls, completion_text)</p> <p>Raises:</p> Type Description <code>AdapterParseError</code> <p>If outputs don't match signature or parsing fails</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>async def finalize(\n    self,\n    signature: type[Signature],\n) -&gt; tuple[dict[str, Any], list[Any], str]:\n    \"\"\"Finalize streaming response and validate outputs.\n\n    Must be called after processing all streaming chunks.\n\n    Args:\n        signature: Signature defining expected outputs\n\n    Returns:\n        Tuple of (outputs, native_tool_calls, completion_text)\n\n    Raises:\n        AdapterParseError: If outputs don't match signature or parsing fails\n    \"\"\"\n    if self._streaming_parser is None:\n        raise RuntimeError(\"No streaming parser available - call process_message first\")\n\n    (\n        outputs,\n        native_tool_calls,\n        completion_text,\n    ) = await self._streaming_parser.finalize()\n\n    # Validate outputs match signature\n    self.validate_outputs(signature, outputs, native_tool_calls, completion_text)\n\n    # Reset for next request\n    self.reset_parser()\n\n    return outputs, native_tool_calls, completion_text\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.finalize_tool_calls","title":"<code>finalize_tool_calls(tool_calls_accumulator)</code>","text":"<p>Convert accumulated tool calls to ToolCall objects.</p> <p>Parses JSON arguments and creates properly formatted ToolCall instances.</p> <p>Parameters:</p> Name Type Description Default <code>tool_calls_accumulator</code> <code>dict[int, dict[str, Any]]</code> <p>Dictionary of accumulated tool call data</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of ToolCall objects</p> <p>Raises:</p> Type Description <code>AdapterParseError</code> <p>If tool call arguments are not valid JSON</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def finalize_tool_calls(self, tool_calls_accumulator: dict[int, dict[str, Any]]) -&gt; list[Any]:\n    \"\"\"Convert accumulated tool calls to ToolCall objects.\n\n    Parses JSON arguments and creates properly formatted ToolCall instances.\n\n    Args:\n        tool_calls_accumulator: Dictionary of accumulated tool call data\n\n    Returns:\n        List of ToolCall objects\n\n    Raises:\n        AdapterParseError: If tool call arguments are not valid JSON\n    \"\"\"\n\n    native_tool_calls = []\n    for tc in tool_calls_accumulator.values():\n        try:\n            arguments = (\n                json.loads(tc[\"function\"][\"arguments\"])\n                if isinstance(tc[\"function\"][\"arguments\"], str)\n                else tc[\"function\"][\"arguments\"]\n            )\n        except json.JSONDecodeError as exc:\n            raise AdapterParseError(\n                adapter_name=self.__class__.__name__,\n                signature=None,  # Tool calls don't have a signature\n                lm_response=tc[\"function\"][\"arguments\"],\n                message=f\"Failed to parse tool call {tc['id']} arguments as JSON: {exc}\",\n            ) from exc\n\n        native_tool_calls.append(\n            ToolCall(\n                call_id=tc[\"id\"],\n                name=tc[\"function\"][\"name\"],\n                args=arguments,\n            )\n        )\n\n    return native_tool_calls\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.format_field_structure","title":"<code>format_field_structure(signature)</code>","text":"<p>Format example field structure with type hints for the LLM.</p> <p>Shows the LLM exactly how to structure inputs and outputs, including type constraints for non-string fields. This helps the LLM understand what format each field should use (e.g., integers, booleans, JSON objects).</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature]</code> <p>The signature defining input/output fields</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted string showing field structure with type hints</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def format_field_structure(self, signature: type[Signature]) -&gt; str:\n    \"\"\"Format example field structure with type hints for the LLM.\n\n    Shows the LLM exactly how to structure inputs and outputs, including\n    type constraints for non-string fields. This helps the LLM understand\n    what format each field should use (e.g., integers, booleans, JSON objects).\n\n    Args:\n        signature: The signature defining input/output fields\n\n    Returns:\n        Formatted string showing field structure with type hints\n    \"\"\"\n    parts = []\n    parts.append(\n        \"All interactions will be structured in the following way, with the appropriate values filled in.\"\n    )\n\n    # Format input fields\n    input_fields = signature.get_input_fields()\n    if input_fields:\n        for name, field_info in input_fields.items():\n            type_hint = translate_field_type(name, field_info)\n            parts.append(f\"[[ ## {name} ## ]]\\n{type_hint}\")\n\n    # Format output fields\n    output_fields = signature.get_output_fields()\n    if output_fields:\n        for name, field_info in output_fields.items():\n            type_hint = translate_field_type(name, field_info)\n            parts.append(f\"[[ ## {name} ## ]]\\n{type_hint}\")\n\n    # Add completion marker\n    parts.append(\"[[ ## completed ## ]]\")\n\n    return \"\\n\\n\".join(parts).strip()\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.format_inputs","title":"<code>format_inputs(signature, inputs)</code>","text":"<p>Format input values into a message.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature]</code> <p>The signature defining expected inputs</p> required <code>inputs</code> <code>dict[str, Any]</code> <p>Dictionary of input values</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted input string</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def format_inputs(\n    self,\n    signature: type[Signature],\n    inputs: dict[str, Any],\n) -&gt; str:\n    \"\"\"Format input values into a message.\n\n    Args:\n        signature: The signature defining expected inputs\n        inputs: Dictionary of input values\n\n    Returns:\n        Formatted input string\n    \"\"\"\n    parts = []\n    input_fields = signature.get_input_fields()\n\n    for name, _ in input_fields.items():\n        if name in inputs:\n            value = inputs[name]\n            formatted = format_value(value)\n            parts.append(f\"[[ ## {name} ## ]]\\n{formatted}\")\n\n    return \"\\n\\n\".join(parts)\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.format_instructions","title":"<code>format_instructions(signature)</code>","text":"<p>Format signature instructions and field descriptions for system message.</p> <p>This now only includes the task description and input/output field descriptions, without the output formatting structure (which is moved to the user message).</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature]</code> <p>The signature to format</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted instruction string for system message</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def format_instructions(self, signature: type[Signature]) -&gt; str:\n    \"\"\"Format signature instructions and field descriptions for system message.\n\n    This now only includes the task description and input/output field descriptions,\n    without the output formatting structure (which is moved to the user message).\n\n    Args:\n        signature: The signature to format\n\n    Returns:\n        Formatted instruction string for system message\n    \"\"\"\n    parts = []\n\n    instructions = signature.get_instructions()\n    if instructions:\n        parts.append(instructions)\n\n    input_fields = [f\"`{name}`\" for name in signature.get_input_fields().keys()]\n    output_fields = [f\"`{name}`\" for name in signature.get_output_fields().keys()]\n    parts.append(\n        f\"Given the input fields: {', '.join(input_fields)}, produce the output fields: {', '.join(output_fields)}.\"\n    )\n\n    return \"\\n\".join(parts).strip()\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.format_output_instructions","title":"<code>format_output_instructions(signature)</code>","text":"<p>Format instructions for how to structure output fields in JSON.</p> <p>This generates the part that tells the LLM how to respond with output fields as a JSON object.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature]</code> <p>The signature defining expected outputs</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted output instructions string</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def format_output_instructions(self, signature: type[Signature]) -&gt; str:\n    \"\"\"Format instructions for how to structure output fields in JSON.\n\n    This generates the part that tells the LLM how to respond with output fields\n    as a JSON object.\n\n    Args:\n        signature: The signature defining expected outputs\n\n    Returns:\n        Formatted output instructions string\n    \"\"\"\n    output_fields = signature.get_output_fields()\n    if not output_fields:\n        return \"\"\n\n    parts = []\n    parts.append(\"\\n\\nRespond with a JSON object containing the following fields:\\n\")\n\n    # List all required fields\n    for name, field_info in output_fields.items():\n        type_hint = translate_field_type(name, field_info)\n        # Extract constraint if exists\n        constraint = \"\"\n        if \"# note:\" in type_hint:\n            constraint = \" - \" + type_hint.split(\"# note:\", 1)[1].strip()\n\n        type_name = (\n            getattr(field_info.annotation, \"__name__\", \"string\")\n            if field_info.annotation\n            else \"string\"\n        )\n        parts.append(f\"- `{name}`: {type_name}{constraint}\\n\")\n\n    parts.append(\"\\nReturn ONLY valid JSON with no additional text or markdown formatting.\")\n\n    return \"\".join(parts)\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.format_tool_schema","title":"<code>format_tool_schema(tool)</code>","text":"<p>Convert a Tool object or Pydantic model to OpenAI tool schema.</p> <p>This is where provider-specific schema formatting happens. The adapter takes the tool's normalized schema and converts it to OpenAI's expected format.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>Any</code> <p>Tool object or Pydantic model class</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>OpenAI tool schema dictionary in the format:</p> <code>dict[str, Any]</code> <p>{ \"type\": \"function\", \"function\": {     \"name\": str,     \"description\": str,     \"parameters\": dict  # Full JSON schema with type, properties, required }</p> <code>dict[str, Any]</code> <p>}</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def format_tool_schema(self, tool: Any) -&gt; dict[str, Any]:\n    \"\"\"Convert a Tool object or Pydantic model to OpenAI tool schema.\n\n    This is where provider-specific schema formatting happens. The adapter\n    takes the tool's normalized schema and converts it to OpenAI's expected format.\n\n    Args:\n        tool: Tool object or Pydantic model class\n\n    Returns:\n        OpenAI tool schema dictionary in the format:\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": str,\n                \"description\": str,\n                \"parameters\": dict  # Full JSON schema with type, properties, required\n            }\n        }\n    \"\"\"\n\n    if isinstance(tool, Tool):\n        # Tool decorator - construct OpenAI schema from Tool properties\n        # Tool.parameters gives us the complete resolved schema (type, properties, required)\n        return {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tool.name,\n                \"description\": tool.description,\n                \"parameters\": tool.parameters,  # Already resolved, ready for OpenAI\n            },\n        }\n    else:\n        # Pydantic model - convert using existing logic\n        tool_model = tool\n        schema = tool_model.model_json_schema()\n\n        # Extract description from docstring or schema\n        description = (\n            tool_model.__doc__.strip()\n            if tool_model.__doc__\n            else schema.get(\"description\", f\"Use {tool_model.__name__}\")\n        )\n\n        # Build OpenAI function schema\n        # Resolve any $defs references in the Pydantic schema\n        tool_schema = resolve_json_schema_reference(\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": schema.get(\"title\", tool_model.__name__),\n                    \"description\": description,\n                    \"parameters\": {\n                        \"type\": \"object\",\n                        \"properties\": schema.get(\"properties\", {}),\n                        \"required\": schema.get(\"required\", []),\n                        \"additionalProperties\": False,\n                    },\n                },\n            }\n        )\n\n        return tool_schema\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.format_tool_schemas","title":"<code>format_tool_schemas(tools)</code>","text":"<p>Convert Tool objects or Pydantic models to OpenAI tool schemas.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>list[Any]</code> <p>List of Tool objects or Pydantic model classes</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of OpenAI tool schema dictionaries</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def format_tool_schemas(self, tools: list[Any]) -&gt; list[dict[str, Any]]:\n    \"\"\"Convert Tool objects or Pydantic models to OpenAI tool schemas.\n\n    Args:\n        tools: List of Tool objects or Pydantic model classes\n\n    Returns:\n        List of OpenAI tool schema dictionaries\n    \"\"\"\n\n    tool_schemas = []\n\n    for tool_item in tools:\n        tool_schema = self.format_tool_schema(tool_item)\n        tool_schemas.append(tool_schema)\n\n    return tool_schemas\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.format_user_request","title":"<code>format_user_request(signature, inputs)</code>","text":"<p>Format complete user request with inputs and output instructions.</p> <p>This combines the input values with instructions on how to format outputs, creating a complete user message that tells the LLM what data it has and how to respond.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature]</code> <p>The signature defining inputs and outputs</p> required <code>inputs</code> <code>dict[str, Any]</code> <p>Dictionary of input values</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted user request string combining inputs + output instructions</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def format_user_request(\n    self,\n    signature: type[Signature],\n    inputs: dict[str, Any],\n) -&gt; str:\n    \"\"\"Format complete user request with inputs and output instructions.\n\n    This combines the input values with instructions on how to format outputs,\n    creating a complete user message that tells the LLM what data it has and\n    how to respond.\n\n    Args:\n        signature: The signature defining inputs and outputs\n        inputs: Dictionary of input values\n\n    Returns:\n        Formatted user request string combining inputs + output instructions\n    \"\"\"\n    try:\n        formatted_inputs = self.format_inputs(signature, inputs)\n    except Exception as e:\n        raise ValueError(f\"Failed to format inputs: {e}\") from e\n    output_instructions = self.format_output_instructions(signature)\n\n    return formatted_inputs + output_instructions\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.parse_outputs","title":"<code>parse_outputs(signature, completion)</code>","text":"<p>Parse LLM completion into structured outputs.</p> <p>Expects JSON format matching the signature's output fields.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature]</code> <p>The signature defining expected outputs</p> required <code>completion</code> <code>str</code> <p>Raw completion string from LLM (should be JSON)</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of parsed output values</p> <p>Raises:</p> Type Description <code>AdapterParseError</code> <p>If completion is not valid JSON</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def parse_outputs(\n    self,\n    signature: type[Signature],\n    completion: str,\n) -&gt; dict[str, Any]:\n    \"\"\"Parse LLM completion into structured outputs.\n\n    Expects JSON format matching the signature's output fields.\n\n    Args:\n        signature: The signature defining expected outputs\n        completion: Raw completion string from LLM (should be JSON)\n\n    Returns:\n        Dictionary of parsed output values\n\n    Raises:\n        AdapterParseError: If completion is not valid JSON\n    \"\"\"\n\n    output_fields = signature.get_output_fields()\n\n    # Handle empty completion (tool calls without content)\n    if not completion or completion.strip() == \"\":\n        return {}\n\n    # Parse JSON completion\n    try:\n        outputs = json.loads(completion)\n    except json.JSONDecodeError as e:\n        raise AdapterParseError(\n            adapter_name=self.__class__.__name__,\n            signature=signature,\n            lm_response=completion,\n            message=f\"Failed to parse JSON output: {e}\",\n        ) from e\n\n    if not isinstance(outputs, dict):\n        raise AdapterParseError(\n            adapter_name=self.__class__.__name__,\n            signature=signature,\n            lm_response=completion,\n            message=f\"Expected JSON object, got {type(outputs).__name__}\",\n        )\n\n    parsed_outputs: dict[str, Any] = {}\n    for field_name, field_info in output_fields.items():\n        if field_name in outputs:\n            value = outputs[field_name]\n            field_type = field_info.annotation\n\n            # Parse value according to field type\n            try:\n                # Check if field type is a Pydantic model\n                if (\n                    field_type\n                    and isinstance(field_type, type)\n                    and issubclass(field_type, BaseModel)\n                ):\n                    # Convert dict to Pydantic model\n                    if isinstance(value, dict):\n                        parsed_outputs[field_name] = field_type.model_validate(value)\n                    else:\n                        # Try parsing as string\n                        parsed_outputs[field_name] = parse_value(str(value), field_type)\n                elif isinstance(value, str):\n                    # String value - parse according to type\n                    parsed_outputs[field_name] = parse_value(value, field_type)  # type: ignore[arg-type]\n                else:\n                    # Value is already correct type (int, float, list, dict, etc.)\n                    parsed_outputs[field_name] = value\n            except Exception:\n                # Fallback: keep original value\n                parsed_outputs[field_name] = value\n\n    return parsed_outputs\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.process_chunk","title":"<code>process_chunk(chunk, module, signature)</code>  <code>async</code>","text":"<p>Process an LLM streaming chunk.</p> <p>This method processes streaming chunks and yields StreamEvent objects. After processing all chunks, call finalize() to get validated outputs.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>ChatCompletionChunk</code> <p>ChatCompletionChunk from streaming LLM</p> required <code>module</code> <code>Any</code> <p>Module instance</p> required <code>signature</code> <code>type[Signature]</code> <p>Signature defining expected outputs</p> required <p>Yields:</p> Type Description <code>Any</code> <p>StreamEvent objects (ThoughtStreamChunk, OutputStreamChunk, etc.)</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>async def process_chunk(\n    self,\n    chunk: \"ChatCompletionChunk\",\n    module: Any,\n    signature: type[Signature],\n) -&gt; Any:\n    \"\"\"Process an LLM streaming chunk.\n\n    This method processes streaming chunks and yields StreamEvent objects.\n    After processing all chunks, call finalize() to get validated outputs.\n\n    Args:\n        chunk: ChatCompletionChunk from streaming LLM\n        module: Module instance\n        signature: Signature defining expected outputs\n\n    Yields:\n        StreamEvent objects (ThoughtStreamChunk, OutputStreamChunk, etc.)\n    \"\"\"\n\n    parser = self._get_or_create_parser(module, signature)\n    async for event in parser.process_chunk(chunk):\n        yield event\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.process_tool_call_deltas","title":"<code>process_tool_call_deltas(tool_calls_accumulator, delta_tool_calls)</code>","text":"<p>Process tool call deltas from streaming response.</p> <p>Accumulates tool call information across multiple chunks, handling incremental updates to tool call names and arguments.</p> <p>Parameters:</p> Name Type Description Default <code>tool_calls_accumulator</code> <code>dict[int, dict[str, Any]]</code> <p>Dictionary mapping tool call index to accumulated data</p> required <code>delta_tool_calls</code> <code>list[Any]</code> <p>List of tool call delta objects from the current chunk</p> required Source code in <code>src/udspy/adapter.py</code> <pre><code>def process_tool_call_deltas(\n    self,\n    tool_calls_accumulator: dict[int, dict[str, Any]],\n    delta_tool_calls: list[Any],\n) -&gt; None:\n    \"\"\"Process tool call deltas from streaming response.\n\n    Accumulates tool call information across multiple chunks, handling\n    incremental updates to tool call names and arguments.\n\n    Args:\n        tool_calls_accumulator: Dictionary mapping tool call index to accumulated data\n        delta_tool_calls: List of tool call delta objects from the current chunk\n    \"\"\"\n    for tc_delta in delta_tool_calls:\n        idx = tc_delta.index\n        if idx not in tool_calls_accumulator:\n            tool_calls_accumulator[idx] = {\n                \"id\": tc_delta.id,\n                \"type\": tc_delta.type,\n                \"function\": {\"name\": \"\", \"arguments\": \"\"},\n            }\n\n        if tc_delta.function:\n            if tc_delta.function.name:\n                tool_calls_accumulator[idx][\"function\"][\"name\"] += tc_delta.function.name\n            if tc_delta.function.arguments:\n                tool_calls_accumulator[idx][\"function\"][\"arguments\"] += (\n                    tc_delta.function.arguments\n                )\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.reset_parser","title":"<code>reset_parser()</code>","text":"<p>Reset the streaming parser for a new request.</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def reset_parser(self) -&gt; None:\n    \"\"\"Reset the streaming parser for a new request.\"\"\"\n    self._streaming_parser = None\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.split_reasoning_and_content_delta","title":"<code>split_reasoning_and_content_delta(response_or_chunk)</code>","text":"<p>Split reasoning and content delta from a streaming chunk.</p> <p>This handles provider-specific reasoning formats: - OpenAI: choice.delta.reasoning (structured field) - AWS Bedrock: ... tags in content - Other providers: may use different formats</p> <p>Parameters:</p> Name Type Description Default <code>response_or_chunk</code> <code>Union[ChatCompletion, ChatCompletionChunk]</code> <p>ChatCompletion or ChatCompletionChunk from streaming LLM</p> required <p>Returns:</p> Type Description <code>str</code> <p>Tuple of (reasoning_delta, content_delta) where:</p> <code>str</code> <ul> <li>reasoning_delta: New reasoning content in this chunk</li> </ul> <code>tuple[str, str]</code> <ul> <li>content_delta: Content excluding reasoning</li> </ul> Source code in <code>src/udspy/adapter.py</code> <pre><code>def split_reasoning_and_content_delta(\n    self,\n    response_or_chunk: Union[\"ChatCompletion\", \"ChatCompletionChunk\"],\n) -&gt; tuple[str, str]:\n    \"\"\"Split reasoning and content delta from a streaming chunk.\n\n    This handles provider-specific reasoning formats:\n    - OpenAI: choice.delta.reasoning (structured field)\n    - AWS Bedrock: &lt;reasoning&gt;...&lt;/reasoning&gt; tags in content\n    - Other providers: may use different formats\n\n    Args:\n        response_or_chunk: ChatCompletion or ChatCompletionChunk from streaming LLM\n\n    Returns:\n        Tuple of (reasoning_delta, content_delta) where:\n        - reasoning_delta: New reasoning content in this chunk\n        - content_delta: Content excluding reasoning\n    \"\"\"\n    from openai.types.chat import ChatCompletion\n\n    if isinstance(response_or_chunk, ChatCompletion):\n        message = response_or_chunk.choices[0].message\n        message_content = message.content or \"\"\n        reasoning_delta = getattr(message, \"reasoning\", None) or \"\"\n    else:\n        delta = response_or_chunk.choices[0].delta\n        message_content = delta.content or \"\"\n        reasoning_delta = getattr(delta, \"reasoning\", None) or \"\"\n\n    # For some providers (like AWS Bedrock), reasoning is returned as\n    # &lt;reasoning&gt; tags inside choice.delta.content instead of choice.delta.reasoning\n    remaining_content = message_content\n    if match := re.search(\n        r\"&lt;reasoning&gt;(.*?)&lt;/reasoning&gt;(.*)\",\n        message_content,\n        re.DOTALL | re.IGNORECASE,\n    ):\n        reasoning_delta = match.group(1).strip()\n        remaining_content = match.group(2).strip()\n\n    return reasoning_delta, remaining_content\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.ChatAdapter.validate_outputs","title":"<code>validate_outputs(signature, outputs, native_tool_calls, completion_text)</code>","text":"<p>Validate that outputs match the signature.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature]</code> <p>Signature defining expected outputs</p> required <code>outputs</code> <code>dict[str, Any]</code> <p>Parsed output fields</p> required <code>native_tool_calls</code> <code>list[Any]</code> <p>Tool calls from LLM</p> required <code>completion_text</code> <code>str</code> <p>Raw completion text</p> required <p>Raises:</p> Type Description <code>AdapterParseError</code> <p>If outputs don't match signature</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def validate_outputs(\n    self,\n    signature: type[Signature],\n    outputs: dict[str, Any],\n    native_tool_calls: list[Any],\n    completion_text: str,\n) -&gt; None:\n    \"\"\"Validate that outputs match the signature.\n\n    Args:\n        signature: Signature defining expected outputs\n        outputs: Parsed output fields\n        native_tool_calls: Tool calls from LLM\n        completion_text: Raw completion text\n\n    Raises:\n        AdapterParseError: If outputs don't match signature\n    \"\"\"\n\n    # If no tool calls, outputs must match signature exactly\n    output_match_keys = outputs.keys() == signature.get_output_fields().keys()\n    if not native_tool_calls and not output_match_keys:\n        raise AdapterParseError(\n            adapter_name=self.__class__.__name__,\n            signature=signature,\n            lm_response=completion_text,\n            parsed_result=outputs,\n        )\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.StreamingParser","title":"<code>StreamingParser</code>","text":"<p>Parse streaming responses and generate StreamEvent objects.</p> <p>This parser processes streaming chunks from the LLM, handling: - Content deltas (JSON output fields) - Tool call deltas - Reasoning deltas</p> <p>It yields StreamEvent objects as they occur and provides finalized outputs at the end.</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>class StreamingParser:\n    \"\"\"Parse streaming responses and generate StreamEvent objects.\n\n    This parser processes streaming chunks from the LLM, handling:\n    - Content deltas (JSON output fields)\n    - Tool call deltas\n    - Reasoning deltas\n\n    It yields StreamEvent objects as they occur and provides finalized\n    outputs at the end.\n    \"\"\"\n\n    def __init__(\n        self,\n        adapter: \"ChatAdapter\",\n        module: Any,\n        signature: Any,\n    ):\n        \"\"\"Initialize streaming parser.\n\n        Args:\n            adapter: ChatAdapter instance for parsing logic\n            module: Module instance for creating stream events\n            signature: Signature defining expected outputs\n        \"\"\"\n        self.adapter = adapter\n        self.module = module\n        self.signature = signature\n        self.output_fields = signature.get_output_fields()\n\n        # Content parsing state\n        self.accumulated_json = \"\"\n        self.previous_values: dict[str, str] = {}\n        self.completed_fields: set[str] = set()\n\n        # Tool call accumulation\n        self.tool_calls: dict[int, dict[str, Any]] = {}\n\n        # Reasoning tracking\n        self.reasoning_content = \"\"\n        self.reasoning_complete = False\n        self.has_seen_reasoning = False  # Track if we've seen any reasoning\n\n        # Full completion text\n        self.full_completion: list[str] = []\n\n    def reset_content_accumulator(self) -&gt; None:\n        \"\"\"Reset the JSON content accumulator.\n\n        Called when reasoning is completed and we're about to parse actual output fields.\n        \"\"\"\n        self.accumulated_json = \"\"\n        self.previous_values.clear()\n        self.completed_fields.clear()\n\n    async def process_chunk(self, chunk: \"ChatCompletionChunk\") -&gt; Any:\n        \"\"\"Process a streaming chunk and yield StreamEvent objects.\n\n        Args:\n            chunk: ChatCompletionChunk from the LLM\n\n        Yields:\n            StreamEvent objects (ThoughtStreamChunk, OutputStreamChunk, etc.)\n        \"\"\"\n\n        if not chunk.choices:\n            return\n\n        choice = chunk.choices[0]\n\n        # Process tool calls first (mutually exclusive with content/reasoning)\n        if choice.delta.tool_calls:\n            self.adapter.process_tool_call_deltas(self.tool_calls, choice.delta.tool_calls)\n            return\n\n        # Check if the answer contains reasoning content\n        (\n            reasoning_delta,\n            remaining_delta,\n        ) = self.adapter.split_reasoning_and_content_delta(chunk)\n        # If we found reasoning content, track that we're in reasoning mode\n        if reasoning_delta and not self.reasoning_complete:\n            self.has_seen_reasoning = True\n            self.reasoning_content += reasoning_delta\n            self.reasoning_complete = bool(remaining_delta)\n\n            yield ThoughtStreamChunk(\n                self.module,\n                \"thought\",\n                reasoning_delta,\n                self.reasoning_content,\n                is_complete=self.reasoning_complete,\n            )\n            return\n\n        # Process content delta for output fields\n        # Only accumulate if: no reasoning seen OR reasoning is complete\n        if remaining_delta:\n            self.full_completion.append(remaining_delta)\n            async for event in self._process_content_delta(remaining_delta):\n                yield event\n\n    async def _process_content_delta(self, delta: str) -&gt; Any:\n        \"\"\"Process content delta for JSON output fields.\n\n        Args:\n            delta: New content fragment\n\n        Yields:\n            OutputStreamChunk events\n        \"\"\"\n\n        if not delta:\n            return\n\n        self.accumulated_json += delta\n\n        # Try to parse the accumulated JSON\n        try:\n            parsed = jiter.from_json(\n                self.accumulated_json.encode(\"utf-8\"), partial_mode=\"trailing-strings\"\n            )\n        except (TypeError, ValueError):\n            # If we can't parse yet, just accumulate more\n            return\n\n        if not isinstance(parsed, dict):\n            return\n\n        # Process each field in the parsed output\n        for field_name, value in parsed.items():\n            if field_name not in self.output_fields:\n                continue\n\n            value_str = str(value) if not isinstance(value, str) else value\n            previous = self.previous_values.get(field_name, \"\")\n\n            if value_str != previous:\n                delta_content = value_str[len(previous) :]\n                yield OutputStreamChunk(\n                    module=self.module,\n                    field_name=field_name,\n                    delta=delta_content,\n                    content=value_str,\n                    is_complete=False,\n                )\n                self.previous_values[field_name] = value_str\n            elif value_str and field_name not in self.completed_fields:\n                yield OutputStreamChunk(\n                    module=self.module,\n                    field_name=field_name,\n                    delta=\"\",\n                    content=value_str,\n                    is_complete=True,\n                )\n                self.completed_fields.add(field_name)\n\n    async def finalize(self) -&gt; tuple[dict[str, Any], list[Any], str]:\n        \"\"\"Finalize parsing and return outputs, tool calls, and completion text.\n\n        Returns:\n            Tuple of (outputs, native_tool_calls, completion_text)\n\n        Raises:\n            AdapterParseError: If parsing fails\n        \"\"\"\n        # Parse final outputs\n        outputs = self.adapter.parse_outputs(self.signature, self.accumulated_json)\n\n        # Emit completion events for any fields not yet marked complete\n        for field_name in self.output_fields:\n            if field_name in outputs and field_name not in self.completed_fields:\n                value = outputs[field_name]\n                value_str = str(value) if not isinstance(value, str) else value\n                emit_event(\n                    OutputStreamChunk(\n                        module=self.module,\n                        field_name=field_name,\n                        delta=\"\",\n                        content=value_str,\n                        is_complete=True,\n                    )\n                )\n                self.completed_fields.add(field_name)\n\n        # Finalize tool calls\n        native_tool_calls = self.adapter.finalize_tool_calls(self.tool_calls)\n\n        # Get completion text\n        completion_text = \"\".join(self.full_completion)\n\n        return outputs, native_tool_calls, completion_text\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.StreamingParser-functions","title":"Functions","text":""},{"location":"api/adapter/#udspy.adapter.StreamingParser.__init__","title":"<code>__init__(adapter, module, signature)</code>","text":"<p>Initialize streaming parser.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>ChatAdapter</code> <p>ChatAdapter instance for parsing logic</p> required <code>module</code> <code>Any</code> <p>Module instance for creating stream events</p> required <code>signature</code> <code>Any</code> <p>Signature defining expected outputs</p> required Source code in <code>src/udspy/adapter.py</code> <pre><code>def __init__(\n    self,\n    adapter: \"ChatAdapter\",\n    module: Any,\n    signature: Any,\n):\n    \"\"\"Initialize streaming parser.\n\n    Args:\n        adapter: ChatAdapter instance for parsing logic\n        module: Module instance for creating stream events\n        signature: Signature defining expected outputs\n    \"\"\"\n    self.adapter = adapter\n    self.module = module\n    self.signature = signature\n    self.output_fields = signature.get_output_fields()\n\n    # Content parsing state\n    self.accumulated_json = \"\"\n    self.previous_values: dict[str, str] = {}\n    self.completed_fields: set[str] = set()\n\n    # Tool call accumulation\n    self.tool_calls: dict[int, dict[str, Any]] = {}\n\n    # Reasoning tracking\n    self.reasoning_content = \"\"\n    self.reasoning_complete = False\n    self.has_seen_reasoning = False  # Track if we've seen any reasoning\n\n    # Full completion text\n    self.full_completion: list[str] = []\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.StreamingParser.finalize","title":"<code>finalize()</code>  <code>async</code>","text":"<p>Finalize parsing and return outputs, tool calls, and completion text.</p> <p>Returns:</p> Type Description <code>tuple[dict[str, Any], list[Any], str]</code> <p>Tuple of (outputs, native_tool_calls, completion_text)</p> <p>Raises:</p> Type Description <code>AdapterParseError</code> <p>If parsing fails</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>async def finalize(self) -&gt; tuple[dict[str, Any], list[Any], str]:\n    \"\"\"Finalize parsing and return outputs, tool calls, and completion text.\n\n    Returns:\n        Tuple of (outputs, native_tool_calls, completion_text)\n\n    Raises:\n        AdapterParseError: If parsing fails\n    \"\"\"\n    # Parse final outputs\n    outputs = self.adapter.parse_outputs(self.signature, self.accumulated_json)\n\n    # Emit completion events for any fields not yet marked complete\n    for field_name in self.output_fields:\n        if field_name in outputs and field_name not in self.completed_fields:\n            value = outputs[field_name]\n            value_str = str(value) if not isinstance(value, str) else value\n            emit_event(\n                OutputStreamChunk(\n                    module=self.module,\n                    field_name=field_name,\n                    delta=\"\",\n                    content=value_str,\n                    is_complete=True,\n                )\n            )\n            self.completed_fields.add(field_name)\n\n    # Finalize tool calls\n    native_tool_calls = self.adapter.finalize_tool_calls(self.tool_calls)\n\n    # Get completion text\n    completion_text = \"\".join(self.full_completion)\n\n    return outputs, native_tool_calls, completion_text\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.StreamingParser.process_chunk","title":"<code>process_chunk(chunk)</code>  <code>async</code>","text":"<p>Process a streaming chunk and yield StreamEvent objects.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>ChatCompletionChunk</code> <p>ChatCompletionChunk from the LLM</p> required <p>Yields:</p> Type Description <code>Any</code> <p>StreamEvent objects (ThoughtStreamChunk, OutputStreamChunk, etc.)</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>async def process_chunk(self, chunk: \"ChatCompletionChunk\") -&gt; Any:\n    \"\"\"Process a streaming chunk and yield StreamEvent objects.\n\n    Args:\n        chunk: ChatCompletionChunk from the LLM\n\n    Yields:\n        StreamEvent objects (ThoughtStreamChunk, OutputStreamChunk, etc.)\n    \"\"\"\n\n    if not chunk.choices:\n        return\n\n    choice = chunk.choices[0]\n\n    # Process tool calls first (mutually exclusive with content/reasoning)\n    if choice.delta.tool_calls:\n        self.adapter.process_tool_call_deltas(self.tool_calls, choice.delta.tool_calls)\n        return\n\n    # Check if the answer contains reasoning content\n    (\n        reasoning_delta,\n        remaining_delta,\n    ) = self.adapter.split_reasoning_and_content_delta(chunk)\n    # If we found reasoning content, track that we're in reasoning mode\n    if reasoning_delta and not self.reasoning_complete:\n        self.has_seen_reasoning = True\n        self.reasoning_content += reasoning_delta\n        self.reasoning_complete = bool(remaining_delta)\n\n        yield ThoughtStreamChunk(\n            self.module,\n            \"thought\",\n            reasoning_delta,\n            self.reasoning_content,\n            is_complete=self.reasoning_complete,\n        )\n        return\n\n    # Process content delta for output fields\n    # Only accumulate if: no reasoning seen OR reasoning is complete\n    if remaining_delta:\n        self.full_completion.append(remaining_delta)\n        async for event in self._process_content_delta(remaining_delta):\n            yield event\n</code></pre>"},{"location":"api/adapter/#udspy.adapter.StreamingParser.reset_content_accumulator","title":"<code>reset_content_accumulator()</code>","text":"<p>Reset the JSON content accumulator.</p> <p>Called when reasoning is completed and we're about to parse actual output fields.</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def reset_content_accumulator(self) -&gt; None:\n    \"\"\"Reset the JSON content accumulator.\n\n    Called when reasoning is completed and we're about to parse actual output fields.\n    \"\"\"\n    self.accumulated_json = \"\"\n    self.previous_values.clear()\n    self.completed_fields.clear()\n</code></pre>"},{"location":"api/adapter/#udspy.adapter-functions","title":"Functions","text":""},{"location":"api/adapter/#udspy.adapter.translate_field_type","title":"<code>translate_field_type(field_name, field_info)</code>","text":"<p>Translate a field's type annotation into a format hint for the LLM.</p> <p>This function generates a placeholder with optional type constraints that guide the LLM on how to format output values for non-string types.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the field</p> required <code>field_info</code> <code>FieldInfo</code> <p>Pydantic FieldInfo containing annotation</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted string like \"{field_name}\" with optional type constraint comment</p> <p>Examples:</p> <p>For str: \"{answer}\" For int: \"{count}\\n        # note: the value you produce must be a single int value\" For bool: \"{is_valid}\\n        # note: the value you produce must be True or False\" For Literal: \"{status}\\n        # note: the value you produce must exactly match one of: pending; approved\"</p> Source code in <code>src/udspy/adapter.py</code> <pre><code>def translate_field_type(field_name: str, field_info: FieldInfo) -&gt; str:\n    \"\"\"Translate a field's type annotation into a format hint for the LLM.\n\n    This function generates a placeholder with optional type constraints that guide\n    the LLM on how to format output values for non-string types.\n\n    Args:\n        field_name: Name of the field\n        field_info: Pydantic FieldInfo containing annotation\n\n    Returns:\n        Formatted string like \"{field_name}\" with optional type constraint comment\n\n    Examples:\n        For str: \"{answer}\"\n        For int: \"{count}\\\\n        # note: the value you produce must be a single int value\"\n        For bool: \"{is_valid}\\\\n        # note: the value you produce must be True or False\"\n        For Literal: \"{status}\\\\n        # note: the value you produce must exactly match one of: pending; approved\"\n    \"\"\"\n    field_type = field_info.annotation\n\n    # For strings, no special formatting needed\n    if field_type is str:\n        desc = \"\"\n    elif field_type is bool:\n        desc = \"must be True or False\"\n    elif field_type in (int, float):\n        desc = f\"must be a single {field_type.__name__} value\"\n    elif inspect.isclass(field_type) and issubclass(field_type, enum.Enum):\n        enum_vals = \"; \".join(str(member.value) for member in field_type)\n        desc = f\"must be one of: {enum_vals}\"\n    elif get_origin(field_type) is Literal:\n        literal_values = get_args(field_type)\n        desc = f\"must exactly match (no extra characters) one of: {'; '.join([str(x) for x in literal_values])}\"\n    else:\n        # For complex types (lists, dicts, Pydantic models), show JSON schema\n        try:\n            schema = minimize_schema(\n                resolve_json_schema_reference(TypeAdapter(field_type).json_schema())\n            )\n            if schema.get(\"type\") == \"array\":\n                item_schema = schema.get(\"items\", {}).get(\"properties\", {})\n                desc = f\"must be a JSON array where every item adheres to the schema: {json.dumps(item_schema, ensure_ascii=False)}\"\n            else:\n                desc = f\"must adhere to the JSON schema: {json.dumps(schema, ensure_ascii=False)}\"\n        except Exception:\n            # Fallback if we can't generate a schema\n            desc = \"\"\n\n    # Format with indentation for readability\n    desc = (\" \" * 8) + f\"# note: the value you produce {desc}\" if desc else \"\"\n    return f\"{{{field_name}}}{desc}\"\n</code></pre>"},{"location":"api/callback/","title":"Callbacks API","text":""},{"location":"api/callback/#udspy.callback","title":"<code>udspy.callback</code>","text":"<p>Callback system for telemetry and monitoring.</p> <p>This module provides a DSPy-compatible callback system for tracking LLM calls, module executions, and tool invocations. Compatible with Opik, MLflow, and other observability tools that support DSPy callbacks.</p>"},{"location":"api/callback/#udspy.callback-classes","title":"Classes","text":""},{"location":"api/callback/#udspy.callback.BaseCallback","title":"<code>BaseCallback</code>","text":"<p>Base class for callback handlers.</p> <p>Subclass this and implement the desired handlers to track LLM calls, module executions, and tool invocations. Compatible with DSPy callback interface.</p> Example <pre><code>from udspy import settings\nfrom udspy.callback import BaseCallback\n\nclass LoggingCallback(BaseCallback):\n    def on_lm_start(self, call_id, instance, inputs):\n        print(f\"LLM called with: {inputs}\")\n\n    def on_lm_end(self, call_id, outputs, exception):\n        if exception:\n            print(f\"LLM failed: {exception}\")\n        else:\n            print(f\"LLM returned: {outputs}\")\n\n    def on_tool_start(self, call_id, instance, inputs):\n        print(f\"Tool {instance.name} called with: {inputs}\")\n\n    def on_tool_end(self, call_id, outputs, exception):\n        print(f\"Tool returned: {outputs}\")\n\n# Set globally\nsettings.configure(callbacks=[LoggingCallback()])\n</code></pre> Source code in <code>src/udspy/callback.py</code> <pre><code>class BaseCallback:\n    \"\"\"Base class for callback handlers.\n\n    Subclass this and implement the desired handlers to track LLM calls, module\n    executions, and tool invocations. Compatible with DSPy callback interface.\n\n    Example:\n        ```python\n        from udspy import settings\n        from udspy.callback import BaseCallback\n\n        class LoggingCallback(BaseCallback):\n            def on_lm_start(self, call_id, instance, inputs):\n                print(f\"LLM called with: {inputs}\")\n\n            def on_lm_end(self, call_id, outputs, exception):\n                if exception:\n                    print(f\"LLM failed: {exception}\")\n                else:\n                    print(f\"LLM returned: {outputs}\")\n\n            def on_tool_start(self, call_id, instance, inputs):\n                print(f\"Tool {instance.name} called with: {inputs}\")\n\n            def on_tool_end(self, call_id, outputs, exception):\n                print(f\"Tool returned: {outputs}\")\n\n        # Set globally\n        settings.configure(callbacks=[LoggingCallback()])\n        ```\n    \"\"\"\n\n    def on_module_start(\n        self,\n        call_id: str,\n        instance: Any,\n        inputs: dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Called when a module's forward() method starts.\n\n        Args:\n            call_id: Unique identifier for this call\n            instance: The Module instance being called\n            inputs: Input arguments as key-value pairs\n        \"\"\"\n        pass\n\n    def on_module_end(\n        self,\n        call_id: str,\n        outputs: Any | None,\n        exception: Exception | None = None,\n    ) -&gt; None:\n        \"\"\"Called when a module's forward() method completes.\n\n        Args:\n            call_id: Unique identifier for this call\n            outputs: The module's output, or None if exception occurred\n            exception: Exception raised during execution, if any\n        \"\"\"\n        pass\n\n    def on_lm_start(\n        self,\n        call_id: str,\n        instance: Any,\n        inputs: dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Called when an LLM call starts.\n\n        Args:\n            call_id: Unique identifier for this call\n            instance: The LLM client or adapter instance\n            inputs: LLM input parameters (messages, model, etc.)\n        \"\"\"\n        pass\n\n    def on_lm_end(\n        self,\n        call_id: str,\n        outputs: dict[str, Any] | None,\n        exception: Exception | None = None,\n    ) -&gt; None:\n        \"\"\"Called when an LLM call completes.\n\n        Args:\n            call_id: Unique identifier for this call\n            outputs: LLM response, or None if exception occurred\n            exception: Exception raised during execution, if any\n        \"\"\"\n        pass\n\n    def on_tool_start(\n        self,\n        call_id: str,\n        instance: Any,\n        inputs: dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Called when a tool is invoked.\n\n        Args:\n            call_id: Unique identifier for this call\n            instance: The Tool instance being called\n            inputs: Tool input arguments as key-value pairs\n        \"\"\"\n        pass\n\n    def on_tool_end(\n        self,\n        call_id: str,\n        outputs: Any | None,\n        exception: Exception | None = None,\n    ) -&gt; None:\n        \"\"\"Called when a tool invocation completes.\n\n        Args:\n            call_id: Unique identifier for this call\n            outputs: Tool output, or None if exception occurred\n            exception: Exception raised during execution, if any\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/callback/#udspy.callback.BaseCallback-functions","title":"Functions","text":""},{"location":"api/callback/#udspy.callback.BaseCallback.on_lm_end","title":"<code>on_lm_end(call_id, outputs, exception=None)</code>","text":"<p>Called when an LLM call completes.</p> <p>Parameters:</p> Name Type Description Default <code>call_id</code> <code>str</code> <p>Unique identifier for this call</p> required <code>outputs</code> <code>dict[str, Any] | None</code> <p>LLM response, or None if exception occurred</p> required <code>exception</code> <code>Exception | None</code> <p>Exception raised during execution, if any</p> <code>None</code> Source code in <code>src/udspy/callback.py</code> <pre><code>def on_lm_end(\n    self,\n    call_id: str,\n    outputs: dict[str, Any] | None,\n    exception: Exception | None = None,\n) -&gt; None:\n    \"\"\"Called when an LLM call completes.\n\n    Args:\n        call_id: Unique identifier for this call\n        outputs: LLM response, or None if exception occurred\n        exception: Exception raised during execution, if any\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/callback/#udspy.callback.BaseCallback.on_lm_start","title":"<code>on_lm_start(call_id, instance, inputs)</code>","text":"<p>Called when an LLM call starts.</p> <p>Parameters:</p> Name Type Description Default <code>call_id</code> <code>str</code> <p>Unique identifier for this call</p> required <code>instance</code> <code>Any</code> <p>The LLM client or adapter instance</p> required <code>inputs</code> <code>dict[str, Any]</code> <p>LLM input parameters (messages, model, etc.)</p> required Source code in <code>src/udspy/callback.py</code> <pre><code>def on_lm_start(\n    self,\n    call_id: str,\n    instance: Any,\n    inputs: dict[str, Any],\n) -&gt; None:\n    \"\"\"Called when an LLM call starts.\n\n    Args:\n        call_id: Unique identifier for this call\n        instance: The LLM client or adapter instance\n        inputs: LLM input parameters (messages, model, etc.)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/callback/#udspy.callback.BaseCallback.on_module_end","title":"<code>on_module_end(call_id, outputs, exception=None)</code>","text":"<p>Called when a module's forward() method completes.</p> <p>Parameters:</p> Name Type Description Default <code>call_id</code> <code>str</code> <p>Unique identifier for this call</p> required <code>outputs</code> <code>Any | None</code> <p>The module's output, or None if exception occurred</p> required <code>exception</code> <code>Exception | None</code> <p>Exception raised during execution, if any</p> <code>None</code> Source code in <code>src/udspy/callback.py</code> <pre><code>def on_module_end(\n    self,\n    call_id: str,\n    outputs: Any | None,\n    exception: Exception | None = None,\n) -&gt; None:\n    \"\"\"Called when a module's forward() method completes.\n\n    Args:\n        call_id: Unique identifier for this call\n        outputs: The module's output, or None if exception occurred\n        exception: Exception raised during execution, if any\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/callback/#udspy.callback.BaseCallback.on_module_start","title":"<code>on_module_start(call_id, instance, inputs)</code>","text":"<p>Called when a module's forward() method starts.</p> <p>Parameters:</p> Name Type Description Default <code>call_id</code> <code>str</code> <p>Unique identifier for this call</p> required <code>instance</code> <code>Any</code> <p>The Module instance being called</p> required <code>inputs</code> <code>dict[str, Any]</code> <p>Input arguments as key-value pairs</p> required Source code in <code>src/udspy/callback.py</code> <pre><code>def on_module_start(\n    self,\n    call_id: str,\n    instance: Any,\n    inputs: dict[str, Any],\n) -&gt; None:\n    \"\"\"Called when a module's forward() method starts.\n\n    Args:\n        call_id: Unique identifier for this call\n        instance: The Module instance being called\n        inputs: Input arguments as key-value pairs\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/callback/#udspy.callback.BaseCallback.on_tool_end","title":"<code>on_tool_end(call_id, outputs, exception=None)</code>","text":"<p>Called when a tool invocation completes.</p> <p>Parameters:</p> Name Type Description Default <code>call_id</code> <code>str</code> <p>Unique identifier for this call</p> required <code>outputs</code> <code>Any | None</code> <p>Tool output, or None if exception occurred</p> required <code>exception</code> <code>Exception | None</code> <p>Exception raised during execution, if any</p> <code>None</code> Source code in <code>src/udspy/callback.py</code> <pre><code>def on_tool_end(\n    self,\n    call_id: str,\n    outputs: Any | None,\n    exception: Exception | None = None,\n) -&gt; None:\n    \"\"\"Called when a tool invocation completes.\n\n    Args:\n        call_id: Unique identifier for this call\n        outputs: Tool output, or None if exception occurred\n        exception: Exception raised during execution, if any\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/callback/#udspy.callback.BaseCallback.on_tool_start","title":"<code>on_tool_start(call_id, instance, inputs)</code>","text":"<p>Called when a tool is invoked.</p> <p>Parameters:</p> Name Type Description Default <code>call_id</code> <code>str</code> <p>Unique identifier for this call</p> required <code>instance</code> <code>Any</code> <p>The Tool instance being called</p> required <code>inputs</code> <code>dict[str, Any]</code> <p>Tool input arguments as key-value pairs</p> required Source code in <code>src/udspy/callback.py</code> <pre><code>def on_tool_start(\n    self,\n    call_id: str,\n    instance: Any,\n    inputs: dict[str, Any],\n) -&gt; None:\n    \"\"\"Called when a tool is invoked.\n\n    Args:\n        call_id: Unique identifier for this call\n        instance: The Tool instance being called\n        inputs: Tool input arguments as key-value pairs\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/callback/#udspy.callback-functions","title":"Functions","text":""},{"location":"api/callback/#udspy.callback.with_callbacks","title":"<code>with_callbacks(fn)</code>","text":"<p>Decorator to add callback functionality to methods.</p> <p>Automatically calls appropriate callback handlers before and after method execution. Handles both sync and async methods.</p> <p>The decorator determines which callback handlers to call based on the instance type (Module, Tool, etc.) and method name.</p> Source code in <code>src/udspy/callback.py</code> <pre><code>def with_callbacks(fn: Callable) -&gt; Callable:\n    \"\"\"Decorator to add callback functionality to methods.\n\n    Automatically calls appropriate callback handlers before and after\n    method execution. Handles both sync and async methods.\n\n    The decorator determines which callback handlers to call based on the\n    instance type (Module, Tool, etc.) and method name.\n    \"\"\"\n\n    def _execute_start_callbacks(\n        instance: Any,\n        fn: Callable,\n        call_id: str,\n        callbacks: list[BaseCallback],\n        args: tuple,\n        kwargs: dict,\n    ) -&gt; None:\n        \"\"\"Execute all start callbacks.\"\"\"\n        # Get function arguments\n        inputs = {\"kwargs\": kwargs, \"args\": args}\n\n        for callback in callbacks:\n            try:\n                handler = _get_on_start_handler(callback, instance, fn)\n                handler(call_id=call_id, instance=instance, inputs=inputs)\n            except Exception as e:\n                logger.warning(f\"Error in callback {callback.__class__.__name__}.on_*_start: {e}\")\n\n    def _execute_end_callbacks(\n        instance: Any,\n        fn: Callable,\n        call_id: str,\n        results: Any,\n        exception: Exception | None,\n        callbacks: list[BaseCallback],\n    ) -&gt; None:\n        \"\"\"Execute all end callbacks.\"\"\"\n        for callback in callbacks:\n            try:\n                handler = _get_on_end_handler(callback, instance, fn)\n                handler(call_id=call_id, outputs=results, exception=exception)\n            except Exception as e:\n                logger.warning(f\"Error in callback {callback.__class__.__name__}.on_*_end: {e}\")\n\n    def _get_active_callbacks(instance: Any) -&gt; list[BaseCallback]:\n        \"\"\"Get combined global and instance-level callbacks.\"\"\"\n        from udspy.settings import settings\n\n        return settings.callbacks\n\n    # Handle async functions\n    if inspect.iscoroutinefunction(fn):\n\n        @functools.wraps(fn)\n        async def async_wrapper(instance: Any, *args: Any, **kwargs: Any) -&gt; Any:\n            callbacks = _get_active_callbacks(instance)\n            if not callbacks:\n                return await fn(instance, *args, **kwargs)\n\n            call_id = uuid.uuid4().hex\n\n            _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)\n\n            # Set active call ID for nested tracking\n            parent_call_id = ACTIVE_CALL_ID.get()\n            ACTIVE_CALL_ID.set(call_id)\n\n            results = None\n            exception = None\n            try:\n                results = await fn(instance, *args, **kwargs)\n                return results\n            except Exception as e:\n                exception = e\n                raise\n            finally:\n                ACTIVE_CALL_ID.set(parent_call_id)\n                _execute_end_callbacks(instance, fn, call_id, results, exception, callbacks)\n\n        return async_wrapper\n\n    # Handle sync functions\n    else:\n\n        @functools.wraps(fn)\n        def sync_wrapper(instance: Any, *args: Any, **kwargs: Any) -&gt; Any:\n            callbacks = _get_active_callbacks(instance)\n            if not callbacks:\n                return fn(instance, *args, **kwargs)\n\n            call_id = uuid.uuid4().hex\n\n            _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)\n\n            # Set active call ID for nested tracking\n            parent_call_id = ACTIVE_CALL_ID.get()\n            ACTIVE_CALL_ID.set(call_id)\n\n            results = None\n            exception = None\n            try:\n                results = fn(instance, *args, **kwargs)\n                return results\n            except Exception as e:\n                exception = e\n                raise\n            finally:\n                ACTIVE_CALL_ID.set(parent_call_id)\n                _execute_end_callbacks(instance, fn, call_id, results, exception, callbacks)\n\n        return sync_wrapper\n</code></pre>"},{"location":"api/confirmation/","title":"Confirmation API Reference","text":"<p>API documentation for the confirmation system that enables human-in-the-loop workflows.</p>"},{"location":"api/confirmation/#module-udspyconfirmation","title":"Module: <code>udspy.confirmation</code>","text":"<p>The confirmation system provides a general-purpose mechanism for pausing execution to request human input or approval. It's designed to be: - Thread-safe: Works correctly with multi-threaded applications - Task-safe: Compatible with asyncio concurrent tasks - Module-agnostic: Can be used by any module, not just ReAct</p>"},{"location":"api/confirmation/#confirmationrequired","title":"<code>ConfirmationRequired</code>","text":"<pre><code>class ConfirmationRequired(Exception):\n    \"\"\"Raised when human input is needed to proceed.\"\"\"\n</code></pre> <p>Exception that pauses execution and saves state for resumption. This is the core mechanism for implementing human-in-the-loop workflows.</p>"},{"location":"api/confirmation/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    question: str,\n    *,\n    confirmation_id: str | None = None,\n    tool_call: ToolCall | None = None,\n    context: dict[str, Any] | None = None,\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>question</code> (<code>str</code>): The question to ask the user</li> <li>Should be clear and actionable</li> <li>Example: \"Confirm execution of delete_file with args: {'path': '/tmp/test.txt'}?\"</li> <li><code>confirmation_id</code> (<code>str | None</code>, optional): Unique confirmation identifier</li> <li>Auto-generated UUID if not provided</li> <li>Used to track confirmation status</li> <li><code>tool_call</code> (<code>ToolCall | None</code>, optional): Information about the tool call that triggered this confirmation</li> <li>Contains tool name, arguments, and optional call ID</li> <li>Can be <code>None</code> for non-tool confirmations</li> <li><code>context</code> (<code>dict[str, Any] | None</code>, optional): Module-specific state dictionary</li> <li>Used to save execution state for resumption</li> <li>Each module defines its own context structure</li> </ul> <p>Example:</p> <pre><code>from udspy.confirmation import ConfirmationRequired, ToolCall\n\n# Simple confirmation with just a question\nraise ConfirmationRequired(\"Do you want to proceed?\")\n\n# Confirmation with tool call information\nraise ConfirmationRequired(\n    question=\"Confirm deletion?\",\n    tool_call=ToolCall(\n        name=\"delete_file\",\n        args={\"path\": \"/tmp/test.txt\"}\n    )\n)\n\n# Confirmation with module state\nraise ConfirmationRequired(\n    question=\"Need clarification\",\n    context={\n        \"iteration\": 5,\n        \"trajectory\": {...},\n        \"input_args\": {...}\n    }\n)\n</code></pre>"},{"location":"api/confirmation/#attributes","title":"Attributes","text":""},{"location":"api/confirmation/#question","title":"<code>question</code>","text":"<pre><code>question: str\n</code></pre> <p>The question being asked to the user.</p>"},{"location":"api/confirmation/#confirmation_id","title":"<code>confirmation_id</code>","text":"<pre><code>confirmation_id: str\n</code></pre> <p>Unique identifier for this confirmation. Use with <code>get_confirmation_status()</code> and <code>respond_to_confirmation()</code>.</p>"},{"location":"api/confirmation/#tool_call","title":"<code>tool_call</code>","text":"<pre><code>tool_call: ToolCall | None\n</code></pre> <p>Optional tool call information. See <code>ToolCall</code> class below.</p>"},{"location":"api/confirmation/#context","title":"<code>context</code>","text":"<pre><code>context: dict[str, Any]\n</code></pre> <p>Module-specific state dictionary. Structure depends on the module that raised the exception.</p>"},{"location":"api/confirmation/#toolcall","title":"<code>ToolCall</code>","text":"<pre><code>class ToolCall:\n    \"\"\"Information about a tool call that triggered a confirmation.\"\"\"\n</code></pre> <p>Encapsulates information about a tool invocation.</p>"},{"location":"api/confirmation/#constructor_1","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    name: str,\n    args: dict[str, Any],\n    call_id: str | None = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>name</code> (<code>str</code>): Tool name</li> <li><code>args</code> (<code>dict[str, Any]</code>): Tool arguments as keyword arguments</li> <li><code>call_id</code> (<code>str | None</code>, optional): Call ID from the LLM provider (e.g., OpenAI)</li> </ul> <p>Example:</p> <pre><code>from udspy.confirmation import ToolCall\n\ntool_call = ToolCall(\n    name=\"search\",\n    args={\"query\": \"Python tutorials\"},\n    call_id=\"call_abc123\"\n)\n</code></pre>"},{"location":"api/confirmation/#attributes_1","title":"Attributes","text":""},{"location":"api/confirmation/#name","title":"<code>name</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the tool.</p>"},{"location":"api/confirmation/#args","title":"<code>args</code>","text":"<pre><code>args: dict[str, Any]\n</code></pre> <p>The tool arguments as a dictionary.</p>"},{"location":"api/confirmation/#call_id","title":"<code>call_id</code>","text":"<pre><code>call_id: str | None\n</code></pre> <p>Optional call ID from the LLM provider.</p>"},{"location":"api/confirmation/#confirm_first","title":"<code>@confirm_first</code>","text":"<pre><code>def confirm_first(func: Callable) -&gt; Callable:\n    \"\"\"Decorator that makes a function require approval before execution.\"\"\"\n</code></pre> <p>Decorator that wraps a function to require human approval on first call. Subsequent calls with the same arguments proceed normally after approval.</p> <p>How it works:</p> <ol> <li>First call: Raises <code>ConfirmationRequired</code> with tool call information</li> <li>User approves: Call <code>respond_to_confirmation(confirmation_id, approved=True)</code></li> <li>Subsequent calls: Execute normally if approved</li> </ol> <p>Supports: - Sync and async functions - Positional and keyword arguments - Thread-safe and asyncio task-safe execution</p> <p>Example:</p> <pre><code>from udspy.confirmation import confirm_first, ConfirmationRequired, respond_to_confirmation\n\n@confirm_first\ndef delete_file(path: str) -&gt; str:\n    os.remove(path)\n    return f\"Deleted {path}\"\n\n# First call raises\ntry:\n    delete_file(\"/tmp/test.txt\")\nexcept ConfirmationRequired as e:\n    print(e.question)  # \"Confirm execution of delete_file...\"\n    confirmation_id = e.confirmation_id\n\n    # User approves\n    respond_to_confirmation(confirmation_id, approved=True)\n\n    # Second call succeeds\n    result = delete_file(\"/tmp/test.txt\")\n    print(result)  # \"Deleted /tmp/test.txt\"\n</code></pre> <p>With async functions:</p> <pre><code>@confirm_first\nasync def async_delete(path: str) -&gt; str:\n    await asyncio.sleep(0.1)\n    os.remove(path)\n    return f\"Deleted {path}\"\n\ntry:\n    await async_delete(\"/tmp/test.txt\")\nexcept ConfirmationRequired as e:\n    respond_to_confirmation(e.confirmation_id, approved=True)\n    result = await async_delete(\"/tmp/test.txt\")\n</code></pre> <p>Modifying arguments:</p> <pre><code>try:\n    delete_file(\"/tmp/test.txt\")\nexcept ConfirmationRequired as e:\n    # Approve with modified arguments\n    modified_args = {\"path\": \"/tmp/safe.txt\"}\n    respond_to_confirmation(e.confirmation_id, approved=True, data=modified_args)\n\n    # Next call uses modified args\n    result = delete_file(\"/tmp/test.txt\")\n    print(result)  # \"Deleted /tmp/safe.txt\"\n</code></pre>"},{"location":"api/confirmation/#get_confirmation_status","title":"<code>get_confirmation_status()</code>","text":"<pre><code>def get_confirmation_status(confirmation_id: str) -&gt; str | None:\n    \"\"\"Get the status of a confirmation.\"\"\"\n</code></pre> <p>Returns the current status of a confirmation by its ID.</p> <p>Parameters:</p> <ul> <li><code>confirmation_id</code> (<code>str</code>): The confirmation ID to query</li> </ul> <p>Returns:</p> <ul> <li><code>str | None</code>: One of:</li> <li><code>\"pending\"</code>: No decision made yet (or ID not found)</li> <li><code>\"approved\"</code>: User approved the action</li> <li><code>\"rejected\"</code>: User rejected the action</li> <li><code>\"edited\"</code>: User approved with modifications</li> <li><code>\"feedback\"</code>: User provided feedback for the agent</li> </ul> <p>Example:</p> <pre><code>from udspy.confirmation import get_confirmation_status, ConfirmationRequired\n\ntry:\n    agent(question=\"Delete files\")\nexcept ConfirmationRequired as e:\n    status = get_confirmation_status(e.confirmation_id)\n    print(status)  # \"pending\"\n\n    # After user responds\n    agent.resume(\"yes\", e)\n    status = get_confirmation_status(e.confirmation_id)\n    print(status)  # \"approved\"\n</code></pre>"},{"location":"api/confirmation/#respond_to_confirmation","title":"<code>respond_to_confirmation()</code>","text":"<pre><code>def respond_to_confirmation(\n    confirmation_id: str,\n    approved: bool = True,\n    data: Any = None,\n    status: str | None = None\n) -&gt; None:\n    \"\"\"Mark a confirmation as approved or rejected.\"\"\"\n</code></pre> <p>Sets the approval status for a confirmation, optionally providing modified data.</p> <p>Parameters:</p> <ul> <li><code>confirmation_id</code> (<code>str</code>): The confirmation ID to update</li> <li><code>approved</code> (<code>bool</code>, default: <code>True</code>): Whether to approve or reject</li> <li><code>True</code>: Allow execution to proceed</li> <li><code>False</code>: Block execution</li> <li><code>data</code> (<code>Any</code>, optional): Modified arguments or feedback data</li> <li>For <code>@confirm_first</code> functions: Dict with modified arguments</li> <li>For modules: Any data to pass back</li> <li><code>status</code> (<code>str | None</code>, optional): Explicit status to set</li> <li>If not provided, inferred from <code>approved</code> and <code>data</code></li> <li>Can be: \"approved\", \"rejected\", \"edited\", \"feedback\"</li> </ul> <p>Example:</p> <pre><code>from udspy.confirmation import respond_to_confirmation\n\n# Simple approval\nrespond_to_confirmation(\"abc-123\", approved=True)\n\n# Rejection\nrespond_to_confirmation(\"abc-123\", approved=False)\n\n# Approval with modified arguments\nrespond_to_confirmation(\n    \"abc-123\",\n    approved=True,\n    data={\"path\": \"/safe/location.txt\"}\n)\n\n# Explicit status\nrespond_to_confirmation(\n    \"abc-123\",\n    approved=True,\n    status=\"feedback\"\n)\n</code></pre>"},{"location":"api/confirmation/#get_confirmation_context","title":"<code>get_confirmation_context()</code>","text":"<pre><code>def get_confirmation_context() -&gt; dict[str, dict[str, Any]]:\n    \"\"\"Get the current confirmation context dictionary.\"\"\"\n</code></pre> <p>Returns the complete confirmation context for the current thread/task. Mostly used for debugging.</p> <p>Returns:</p> <ul> <li><code>dict[str, dict[str, Any]]</code>: Dictionary mapping confirmation IDs to their state:   <pre><code>{\n    \"confirmation-id-1\": {\n        \"approved\": True,\n        \"data\": {...},\n        \"status\": \"approved\"\n    },\n    \"confirmation-id-2\": {\n        \"approved\": False,\n        \"status\": \"rejected\"\n    }\n}\n</code></pre></li> </ul> <p>Example:</p> <pre><code>from udspy.confirmation import get_confirmation_context\n\ncontext = get_confirmation_context()\nprint(f\"Active confirmations: {len(context)}\")\nfor confirmation_id, state in context.items():\n    print(f\"{confirmation_id}: {state['status']}\")\n</code></pre>"},{"location":"api/confirmation/#clear_confirmation","title":"<code>clear_confirmation()</code>","text":"<pre><code>def clear_confirmation(confirmation_id: str) -&gt; None:\n    \"\"\"Remove an confirmation from the context.\"\"\"\n</code></pre> <p>Clears a specific confirmation from the context. Usually done automatically after successful execution.</p> <p>Parameters:</p> <ul> <li><code>confirmation_id</code> (<code>str</code>): The confirmation ID to clear</li> </ul> <p>Example:</p> <pre><code>from udspy.confirmation import clear_confirmation\n\nclear_confirmation(\"abc-123\")\n</code></pre>"},{"location":"api/confirmation/#clear_all_confirmations","title":"<code>clear_all_confirmations()</code>","text":"<pre><code>def clear_all_confirmations() -&gt; None:\n    \"\"\"Clear all confirmations from the context.\"\"\"\n</code></pre> <p>Removes all confirmations from the current context. Useful for cleanup or testing.</p> <p>Example:</p> <pre><code>from udspy.confirmation import clear_all_confirmations\n\n# Start fresh\nclear_all_confirmations()\n</code></pre>"},{"location":"api/confirmation/#confirmation-status-lifecycle","title":"Confirmation Status Lifecycle","text":"<p>The status of a confirmation follows this lifecycle:</p> <pre><code>pending (initial)\n    \u2193\n    \u251c\u2192 approved (user said \"yes\")\n    \u251c\u2192 rejected (user said \"no\")\n    \u251c\u2192 edited (user modified args)\n    \u2514\u2192 feedback (user provided feedback)\n</code></pre> <p>Status Meanings:</p> <ul> <li>pending: Initial state, no decision made</li> <li>approved: User approved the action as-is</li> <li>rejected: User rejected the action</li> <li>edited: User approved with modifications to arguments</li> <li>feedback: User provided textual feedback (not yes/no)</li> </ul>"},{"location":"api/confirmation/#thread-safety","title":"Thread Safety","text":"<p>The confirmation system uses <code>contextvars.ContextVar</code> for thread-safe and asyncio task-safe storage:</p> <ul> <li>Each thread has its own confirmation context</li> <li>Each asyncio task inherits parent task's context</li> <li>No cross-contamination between threads/tasks</li> </ul> <p>Example:</p> <pre><code>import threading\nfrom udspy.confirmation import confirm_first, ConfirmationRequired, respond_to_confirmation\n\n@confirm_first\ndef thread_func(thread_id: int) -&gt; str:\n    return f\"Thread {thread_id}\"\n\ndef worker(thread_id: int):\n    try:\n        thread_func(thread_id)\n    except ConfirmationRequired as e:\n        # Each thread has its own confirmation context\n        respond_to_confirmation(e.confirmation_id, approved=True)\n        result = thread_func(thread_id)\n        print(result)\n\nthreads = [threading.Thread(target=worker, args=(i,)) for i in range(3)]\nfor t in threads:\n    t.start()\nfor t in threads:\n    t.join()\n</code></pre>"},{"location":"api/confirmation/#integration-with-modules","title":"Integration with Modules","text":"<p>Modules can use the confirmation system by:</p> <ol> <li>Raising <code>ConfirmationRequired</code> when human input is needed</li> <li>Saving state in the <code>context</code> dict</li> <li>Implementing <code>resume()</code> and <code>aresume()</code> methods to restore state</li> </ol> <p>Example Module:</p> <pre><code>from udspy import Module, Prediction\nfrom udspy.confirmation import ConfirmationRequired\n\nclass MyModule(Module):\n    def forward(self, input: str) -&gt; Prediction:\n        # ... some work ...\n\n        if needs_human_input:\n            raise ConfirmationRequired(\n                question=\"Please confirm\",\n                context={\n                    \"current_step\": 5,\n                    \"partial_result\": \"...\",\n                    \"input\": input\n                }\n            )\n\n        # ... continue work ...\n        return Prediction(output=\"result\")\n\n    def resume(self, user_response: str, saved_state: ConfirmationRequired) -&gt; Prediction:\n        # Restore state from context\n        current_step = saved_state.context[\"current_step\"]\n        partial_result = saved_state.context[\"partial_result\"]\n        input = saved_state.context[\"input\"]\n\n        # Process user response\n        if user_response.lower() == \"yes\":\n            # Continue from where we left off\n            pass\n\n        # ... complete work ...\n        return Prediction(output=\"final result\")\n</code></pre>"},{"location":"api/confirmation/#integration-with-tools","title":"Integration with Tools","text":"<p>Tools can use <code>require_confirmation=True</code> parameter to require confirmation:</p> <pre><code>from udspy import tool\nfrom pydantic import Field\n\n@tool(\n    name=\"delete_file\",\n    description=\"Delete a file\",\n    require_confirmation=True  # Wraps function with @confirm_first decorator\n)\ndef delete_file(path: str = Field(...)) -&gt; str:\n    os.remove(path)\n    return f\"Deleted {path}\"\n</code></pre> <p>The <code>@tool</code> decorator automatically wraps the function with <code>@confirm_first</code> when this parameter is set.</p>"},{"location":"api/confirmation/#see-also","title":"See Also","text":"<ul> <li>ReAct API - ReAct agent that uses the confirmation system</li> <li>Tool API - Creating tools with require_confirmation</li> <li>Module API - Base module with suspend/resume methods</li> </ul>"},{"location":"api/history/","title":"History API Reference","text":"<p>The <code>History</code> class manages conversation history for multi-turn interactions.</p>"},{"location":"api/history/#class-history","title":"Class: History","text":"<pre><code>from udspy import History\n</code></pre>"},{"location":"api/history/#constructor","title":"Constructor","text":"<pre><code>History(messages: list[dict[str, Any]] | None = None)\n</code></pre> <p>Create a new History instance.</p> <p>Parameters: - <code>messages</code> (optional): Initial list of messages in OpenAI format</p> <p>Example: <pre><code># Empty history\nhistory = History()\n\n# With initial messages\nhistory = History(messages=[\n    {\"role\": \"system\", \"content\": \"You are helpful\"},\n    {\"role\": \"user\", \"content\": \"Hello\"}\n])\n</code></pre></p>"},{"location":"api/history/#attributes","title":"Attributes","text":""},{"location":"api/history/#messages","title":"messages","text":"<pre><code>history.messages: list[dict[str, Any]]\n</code></pre> <p>List of conversation messages in OpenAI format. Each message is a dictionary with at minimum: - <code>role</code>: One of \"system\", \"user\", \"assistant\", or \"tool\" - <code>content</code>: Message content string</p> <p>Assistant messages with tool calls also include: - <code>tool_calls</code>: List of tool call dictionaries</p> <p>Tool messages also include: - <code>tool_call_id</code>: ID of the tool call this result is for</p> <p>Example: <pre><code>for msg in history.messages:\n    print(f\"{msg['role']}: {msg['content']}\")\n</code></pre></p>"},{"location":"api/history/#methods","title":"Methods","text":""},{"location":"api/history/#add_message","title":"add_message","text":"<pre><code>add_message(\n    role: str,\n    content: str,\n    *,\n    tool_calls: list[dict[str, Any]] | None = None\n) -&gt; None\n</code></pre> <p>Add a message to the history.</p> <p>Parameters: - <code>role</code>: Message role (\"system\", \"user\", \"assistant\", \"tool\") - <code>content</code>: Message content - <code>tool_calls</code> (optional): Tool calls for assistant messages</p> <p>Example: <pre><code>history.add_message(\"user\", \"What is AI?\")\nhistory.add_message(\"assistant\", \"AI is...\", tool_calls=[...])\n</code></pre></p>"},{"location":"api/history/#add_user_message","title":"add_user_message","text":"<pre><code>add_user_message(content: str) -&gt; None\n</code></pre> <p>Add a user message.</p> <p>Parameters: - <code>content</code>: User message content</p> <p>Example: <pre><code>history.add_user_message(\"Tell me about Python\")\n</code></pre></p>"},{"location":"api/history/#add_assistant_message","title":"add_assistant_message","text":"<pre><code>add_assistant_message(\n    content: str,\n    tool_calls: list[dict[str, Any]] | None = None\n) -&gt; None\n</code></pre> <p>Add an assistant message.</p> <p>Parameters: - <code>content</code>: Assistant message content - <code>tool_calls</code> (optional): Tool calls made by assistant</p> <p>Example: <pre><code>history.add_assistant_message(\"Python is a programming language...\")\n</code></pre></p>"},{"location":"api/history/#add_system_message","title":"add_system_message","text":"<pre><code>add_system_message(content: str) -&gt; None\n</code></pre> <p>Add a system message.</p> <p>Parameters: - <code>content</code>: System message content</p> <p>Example: <pre><code>history.add_system_message(\"You are a helpful coding tutor\")\n</code></pre></p>"},{"location":"api/history/#add_tool_result","title":"add_tool_result","text":"<pre><code>add_tool_result(tool_call_id: str, content: str) -&gt; None\n</code></pre> <p>Add a tool result message.</p> <p>Parameters: - <code>tool_call_id</code>: ID of the tool call this result is for - <code>content</code>: Tool result content</p> <p>Example: <pre><code>history.add_tool_result(\"call_123\", \"Result: 42\")\n</code></pre></p>"},{"location":"api/history/#clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear all messages from history.</p> <p>Example: <pre><code>history.clear()\nprint(len(history))  # 0\n</code></pre></p>"},{"location":"api/history/#copy","title":"copy","text":"<pre><code>copy() -&gt; History\n</code></pre> <p>Create a copy of this history.</p> <p>Returns: - New <code>History</code> instance with copied messages</p> <p>Example: <pre><code>branch = history.copy()\n# Modify branch without affecting original\nbranch.add_user_message(\"New question\")\n</code></pre></p>"},{"location":"api/history/#magic-methods","title":"Magic Methods","text":""},{"location":"api/history/#__len__","title":"<code>__len__</code>","text":"<pre><code>len(history) -&gt; int\n</code></pre> <p>Get number of messages in history.</p> <p>Example: <pre><code>print(len(history))  # e.g., 5\n</code></pre></p>"},{"location":"api/history/#__repr__","title":"<code>__repr__</code>","text":"<pre><code>repr(history) -&gt; str\n</code></pre> <p>String representation showing number of messages.</p> <p>Example: <pre><code>print(repr(history))  # \"History(5 messages)\"\n</code></pre></p>"},{"location":"api/history/#__str__","title":"<code>__str__</code>","text":"<pre><code>str(history) -&gt; str\n</code></pre> <p>Human-readable formatted conversation history.</p> <p>Example: <pre><code>print(history)\n# History (3 messages):\n#   1. [user] What is Python?\n#   2. [assistant] Python is a programming language...\n#   3. [user] What are its features?\n</code></pre></p>"},{"location":"api/history/#usage-with-predict","title":"Usage with Predict","text":"<p>History integrates seamlessly with <code>Predict</code>:</p> <pre><code>from udspy import Predict, History\n\npredictor = Predict(QA)\nhistory = History()\n\n# History is automatically updated with each call\nresult = predictor(question=\"First question\", history=history)\nresult = predictor(question=\"Follow-up question\", history=history)\n</code></pre> <p>See History Examples for more usage patterns.</p>"},{"location":"api/module/","title":"API Reference: Modules","text":""},{"location":"api/module/#udspy.module","title":"<code>udspy.module</code>","text":"<p>Module package for composable LLM calls.</p>"},{"location":"api/module/#udspy.module-classes","title":"Classes","text":""},{"location":"api/module/#udspy.module.ChainOfThought","title":"<code>ChainOfThought</code>","text":"<p>               Bases: <code>Module</code></p> <p>Chain of Thought reasoning module.</p> <p>Automatically adds a reasoning step before generating outputs. This encourages the LLM to think step-by-step, improving answer quality.</p> Example <pre><code>class QA(Signature):\n    '''Answer questions.'''\n    question: str = InputField()\n    answer: str = OutputField()\n\n# Creates predictor with automatic reasoning\npredictor = ChainOfThought(QA)\nresult = predictor(question=\"What is 2+2?\")\n\nprint(result.reasoning)  # \"Let's think step by step...\"\nprint(result.answer)     # \"4\"\n</code></pre> Source code in <code>src/udspy/module/chain_of_thought.py</code> <pre><code>class ChainOfThought(Module):\n    \"\"\"Chain of Thought reasoning module.\n\n    Automatically adds a reasoning step before generating outputs.\n    This encourages the LLM to think step-by-step, improving answer quality.\n\n    Example:\n        ```python\n        class QA(Signature):\n            '''Answer questions.'''\n            question: str = InputField()\n            answer: str = OutputField()\n\n        # Creates predictor with automatic reasoning\n        predictor = ChainOfThought(QA)\n        result = predictor(question=\"What is 2+2?\")\n\n        print(result.reasoning)  # \"Let's think step by step...\"\n        print(result.answer)     # \"4\"\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        signature: type[Signature] | str,\n        *,\n        reasoning_description: str = \"Step-by-step reasoning process\",\n        tools: list[Tool] | None = None,\n        model: str | None = None,\n        adapter: ChatAdapter | None = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Initialize a Chain of Thought module.\n\n        Args:\n            signature: Signature defining inputs and final outputs, or a string in\n                      format \"inputs -&gt; outputs\" (e.g., \"question -&gt; answer\")\n            reasoning_description: Description for the reasoning field\n            model: Model name (overrides global default)\n            tools: List of Pydantic tool models\n            adapter: Custom adapter\n            **kwargs: Additional arguments for chat completion (including callbacks)\n        \"\"\"\n        if isinstance(signature, str):\n            signature = Signature.from_string(signature)\n\n        self.original_signature = signature\n        self.reasoning_description = reasoning_description\n        self.model = model\n        self.adapter = adapter\n        self.kwargs = kwargs\n\n        # Initialize module with tools\n        self.init_module(tools=tools)\n\n    def init_module(self, tools: list[Any] | None = None) -&gt; None:\n        \"\"\"Initialize or reinitialize ChainOfThought with new tools.\n\n        Args:\n            tools: New tools to initialize with\n        \"\"\"\n        extended_signature = self._build_extended_signature()\n        self._create_predictor(extended_signature, tools)\n\n    def _build_extended_signature(self) -&gt; type[Signature]:\n        \"\"\"Build extended signature with reasoning field.\n\n        Returns:\n            Signature with reasoning field prepended to outputs\n        \"\"\"\n        signature = self.original_signature\n\n        input_fields = {\n            name: field.annotation for name, field in signature.get_input_fields().items()\n        }\n        output_fields = {\n            name: field.annotation for name, field in signature.get_output_fields().items()\n        }\n\n        extended_outputs = {\"reasoning\": str, **output_fields}\n\n        extended_signature = make_signature(\n            input_fields,  # type: ignore[arg-type]\n            extended_outputs,  # type: ignore[arg-type]\n            signature.get_instructions(),\n        )\n\n        extended_signature.model_fields[\"reasoning\"].description = self.reasoning_description\n\n        return extended_signature\n\n    def _create_predictor(self, signature: type[Signature], tools: list[Any] | None) -&gt; None:\n        \"\"\"Create the internal Predict module.\n\n        Args:\n            signature: Extended signature with reasoning field\n            tools: Tools to pass to Predict\n        \"\"\"\n        self.predict = Predict(\n            signature, tools=tools, model=self.model, adapter=self.adapter, **self.kwargs\n        )\n\n    @with_callbacks\n    async def aexecute(self, *, stream: bool = False, **inputs: Any) -&gt; Prediction:\n        \"\"\"Execute chain of thought prediction.\n\n        Delegates to the wrapped Predict module's aexecute method, which will\n        automatically emit streaming events if a queue is active.\n\n        Args:\n            stream: If True, request streaming from LLM provider\n            **inputs: Input values matching the signature's input fields\n\n        Returns:\n            Prediction with reasoning and other output fields\n        \"\"\"\n        return await self.predict.aexecute(stream=stream, **inputs)\n</code></pre>"},{"location":"api/module/#udspy.module.ChainOfThought-functions","title":"Functions","text":""},{"location":"api/module/#udspy.module.ChainOfThought.__init__","title":"<code>__init__(signature, *, reasoning_description='Step-by-step reasoning process', tools=None, model=None, adapter=None, **kwargs)</code>","text":"<p>Initialize a Chain of Thought module.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature] | str</code> <p>Signature defining inputs and final outputs, or a string in       format \"inputs -&gt; outputs\" (e.g., \"question -&gt; answer\")</p> required <code>reasoning_description</code> <code>str</code> <p>Description for the reasoning field</p> <code>'Step-by-step reasoning process'</code> <code>model</code> <code>str | None</code> <p>Model name (overrides global default)</p> <code>None</code> <code>tools</code> <code>list[Tool] | None</code> <p>List of Pydantic tool models</p> <code>None</code> <code>adapter</code> <code>ChatAdapter | None</code> <p>Custom adapter</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for chat completion (including callbacks)</p> <code>{}</code> Source code in <code>src/udspy/module/chain_of_thought.py</code> <pre><code>def __init__(\n    self,\n    signature: type[Signature] | str,\n    *,\n    reasoning_description: str = \"Step-by-step reasoning process\",\n    tools: list[Tool] | None = None,\n    model: str | None = None,\n    adapter: ChatAdapter | None = None,\n    **kwargs: Any,\n):\n    \"\"\"Initialize a Chain of Thought module.\n\n    Args:\n        signature: Signature defining inputs and final outputs, or a string in\n                  format \"inputs -&gt; outputs\" (e.g., \"question -&gt; answer\")\n        reasoning_description: Description for the reasoning field\n        model: Model name (overrides global default)\n        tools: List of Pydantic tool models\n        adapter: Custom adapter\n        **kwargs: Additional arguments for chat completion (including callbacks)\n    \"\"\"\n    if isinstance(signature, str):\n        signature = Signature.from_string(signature)\n\n    self.original_signature = signature\n    self.reasoning_description = reasoning_description\n    self.model = model\n    self.adapter = adapter\n    self.kwargs = kwargs\n\n    # Initialize module with tools\n    self.init_module(tools=tools)\n</code></pre>"},{"location":"api/module/#udspy.module.ChainOfThought.aexecute","title":"<code>aexecute(*, stream=False, **inputs)</code>  <code>async</code>","text":"<p>Execute chain of thought prediction.</p> <p>Delegates to the wrapped Predict module's aexecute method, which will automatically emit streaming events if a queue is active.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>bool</code> <p>If True, request streaming from LLM provider</p> <code>False</code> <code>**inputs</code> <code>Any</code> <p>Input values matching the signature's input fields</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Prediction with reasoning and other output fields</p> Source code in <code>src/udspy/module/chain_of_thought.py</code> <pre><code>@with_callbacks\nasync def aexecute(self, *, stream: bool = False, **inputs: Any) -&gt; Prediction:\n    \"\"\"Execute chain of thought prediction.\n\n    Delegates to the wrapped Predict module's aexecute method, which will\n    automatically emit streaming events if a queue is active.\n\n    Args:\n        stream: If True, request streaming from LLM provider\n        **inputs: Input values matching the signature's input fields\n\n    Returns:\n        Prediction with reasoning and other output fields\n    \"\"\"\n    return await self.predict.aexecute(stream=stream, **inputs)\n</code></pre>"},{"location":"api/module/#udspy.module.ChainOfThought.init_module","title":"<code>init_module(tools=None)</code>","text":"<p>Initialize or reinitialize ChainOfThought with new tools.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>list[Any] | None</code> <p>New tools to initialize with</p> <code>None</code> Source code in <code>src/udspy/module/chain_of_thought.py</code> <pre><code>def init_module(self, tools: list[Any] | None = None) -&gt; None:\n    \"\"\"Initialize or reinitialize ChainOfThought with new tools.\n\n    Args:\n        tools: New tools to initialize with\n    \"\"\"\n    extended_signature = self._build_extended_signature()\n    self._create_predictor(extended_signature, tools)\n</code></pre>"},{"location":"api/module/#udspy.module.ConfirmationRequired","title":"<code>ConfirmationRequired</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when human input is needed to proceed.</p> <p>This exception pauses execution and allows modules to save state for resumption. It can be raised by: - Tools decorated with @confirm_first - Modules that need user input (e.g., ask_to_user) - Custom code requiring human interaction</p> <p>Attributes:</p> Name Type Description <code>question</code> <p>The question being asked to the user</p> <code>confirmation_id</code> <p>Unique ID for this confirmation request</p> <code>tool_call</code> <p>Optional ToolCall information if raised by a tool</p> <code>context</code> <p>General-purpose context dictionary for module state</p> Source code in <code>src/udspy/confirmation.py</code> <pre><code>class ConfirmationRequired(Exception):\n    \"\"\"Raised when human input is needed to proceed.\n\n    This exception pauses execution and allows modules to save state for resumption.\n    It can be raised by:\n    - Tools decorated with @confirm_first\n    - Modules that need user input (e.g., ask_to_user)\n    - Custom code requiring human interaction\n\n    Attributes:\n        question: The question being asked to the user\n        confirmation_id: Unique ID for this confirmation request\n        tool_call: Optional ToolCall information if raised by a tool\n        context: General-purpose context dictionary for module state\n    \"\"\"\n\n    def __init__(\n        self,\n        question: str,\n        *,\n        confirmation_id: str | None = None,\n        tool_call: Optional[\"ToolCall\"] = None,\n        context: dict[str, Any] | None = None,\n    ):\n        \"\"\"Initialize ConfirmationRequired exception.\n\n        Args:\n            question: Question to ask the user\n            confirmation_id: Unique ID for this confirmation (auto-generated if not provided)\n            tool_call: Optional tool call information\n            context: Optional context dictionary for module-specific state\n        \"\"\"\n        super().__init__(question)\n        self.question = question\n        self.confirmation_id = confirmation_id or str(uuid.uuid4())\n        self.tool_call = tool_call\n        self.context = context or {}\n</code></pre>"},{"location":"api/module/#udspy.module.ConfirmationRequired-functions","title":"Functions","text":""},{"location":"api/module/#udspy.module.ConfirmationRequired.__init__","title":"<code>__init__(question, *, confirmation_id=None, tool_call=None, context=None)</code>","text":"<p>Initialize ConfirmationRequired exception.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>Question to ask the user</p> required <code>confirmation_id</code> <code>str | None</code> <p>Unique ID for this confirmation (auto-generated if not provided)</p> <code>None</code> <code>tool_call</code> <code>Optional[ToolCall]</code> <p>Optional tool call information</p> <code>None</code> <code>context</code> <code>dict[str, Any] | None</code> <p>Optional context dictionary for module-specific state</p> <code>None</code> Source code in <code>src/udspy/confirmation.py</code> <pre><code>def __init__(\n    self,\n    question: str,\n    *,\n    confirmation_id: str | None = None,\n    tool_call: Optional[\"ToolCall\"] = None,\n    context: dict[str, Any] | None = None,\n):\n    \"\"\"Initialize ConfirmationRequired exception.\n\n    Args:\n        question: Question to ask the user\n        confirmation_id: Unique ID for this confirmation (auto-generated if not provided)\n        tool_call: Optional tool call information\n        context: Optional context dictionary for module-specific state\n    \"\"\"\n    super().__init__(question)\n    self.question = question\n    self.confirmation_id = confirmation_id or str(uuid.uuid4())\n    self.tool_call = tool_call\n    self.context = context or {}\n</code></pre>"},{"location":"api/module/#udspy.module.Module","title":"<code>Module</code>","text":"<p>Base class for all udspy modules.</p> <p>Modules are composable async-first units. The core method is <code>aexecute()</code> which handles both streaming and non-streaming execution. Public methods <code>astream()</code> and <code>aforward()</code> are thin wrappers around <code>aexecute()</code>.</p> <p>Subclasses should implement <code>aexecute()</code> to define their behavior.</p> Example <pre><code># Async streaming (real-time)\nasync for event in module.astream(question=\"What is AI?\"):\n    if isinstance(event, OutputStreamChunk):\n        print(event.delta, end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        result = event\n\n# Async non-streaming\nresult = await module.aforward(question=\"What is AI?\")\n\n# Sync (for scripts, notebooks)\nresult = module(question=\"What is AI?\")\nresult = module.forward(question=\"What is AI?\")\n</code></pre> Source code in <code>src/udspy/module/base.py</code> <pre><code>class Module:\n    \"\"\"Base class for all udspy modules.\n\n    Modules are composable async-first units. The core method is `aexecute()`\n    which handles both streaming and non-streaming execution. Public methods\n    `astream()` and `aforward()` are thin wrappers around `aexecute()`.\n\n    Subclasses should implement `aexecute()` to define their behavior.\n\n    Example:\n        ```python\n        # Async streaming (real-time)\n        async for event in module.astream(question=\"What is AI?\"):\n            if isinstance(event, OutputStreamChunk):\n                print(event.delta, end=\"\", flush=True)\n            elif isinstance(event, Prediction):\n                result = event\n\n        # Async non-streaming\n        result = await module.aforward(question=\"What is AI?\")\n\n        # Sync (for scripts, notebooks)\n        result = module(question=\"What is AI?\")\n        result = module.forward(question=\"What is AI?\")\n        ```\n    \"\"\"\n\n    @with_callbacks\n    async def aexecute(\n        self, *, stream: bool = False, history: History | None = None, **inputs: Any\n    ) -&gt; Prediction:\n        \"\"\"Core execution method. Must be implemented by subclasses.\n\n        This is the single implementation point for both streaming and non-streaming\n        execution. It always returns a Prediction, and optionally emits StreamEvent\n        objects to the active queue (if one exists in the context).\n\n        Args:\n            stream: If True, request streaming from LLM provider. If False, use\n                non-streaming API calls.\n            history: Optional History object for maintaining conversation state\n            **inputs: Input values for the module\n\n        Returns:\n            Final Prediction object\n\n        Behavior:\n            - Checks for active stream queue via _stream_queue.get()\n            - If queue exists: emits OutputStreamChunk and Prediction events\n            - Always returns final Prediction (even in streaming mode)\n            - This enables composability: nested modules emit events automatically\n\n        Raises:\n            NotImplementedError: If not implemented by subclass\n        \"\"\"\n        raise NotImplementedError(f\"{self.__class__.__name__} must implement aexecute() method\")\n\n    @abstractmethod\n    def init_module(self, tools: list[Callable[..., Any]] | None = None) -&gt; None:\n        \"\"\"Initialize or reinitialize the module with new tools.\n\n        This method provides a way to completely reinitialize module state,\n        including tools, tool schemas, and signatures. It's designed to be\n        called from module callbacks that need to dynamically modify the\n        module during execution.\n\n        When implementing this method, subclasses should:\n        1. Rebuild the tools dictionary\n        2. Regenerate tool schemas (if applicable)\n        3. Rebuild signatures with new tool descriptions (if applicable)\n        4. Preserve built-in tools (if applicable)\n\n        Args:\n            tools: New tools to initialize with. Format depends on subclass:\n                - Can be functions (will be wrapped in Tool)\n                - Can be Tool instances\n                - None means clear all non-built-in tools\n\n        Example:\n            ```python\n            from udspy import module_callback\n\n            @module_callback\n            def add_tools(context):\n                # Get current tools\n                current = list(context.module.tools.values())\n\n                # Add new tools\n                new_tools = [weather_tool, calendar_tool]\n\n                # Reinitialize module with all tools\n                context.module.init_module(tools=current + new_tools)\n\n                return \"Added weather and calendar tools\"\n            ```\n\n        Note:\n            This method is typically called from within a module callback\n            decorated with @module_callback. The callback receives a context\n            object with access to the module instance.\n        \"\"\"\n        raise NotImplementedError(f\"{self.__class__.__name__} must implement init_module() method\")\n\n    async def astream(\n        self, *, resume_state: Any = None, history: History | None = None, **inputs: Any\n    ) -&gt; AsyncGenerator[StreamEvent]:\n        \"\"\"Async streaming method. Sets up queue and yields events.\n\n        This method sets up the stream queue context, calls aexecute() with\n        streaming enabled, and yields all events from the queue.\n\n        Supports resuming from a ConfirmationRequired exception by providing\n        resume_state. This enables streaming with confirmation handling.\n\n        Args:\n            resume_state: Optional ResumeState containing exception and user response.\n                Can also be a raw ConfirmationRequired exception (will use \"yes\" as response).\n            history: Optional History object for maintaining conversation state.\n            **inputs: Input values for the module\n\n        Yields:\n            StreamEvent objects (OutputStreamChunk, Prediction, and custom events)\n        \"\"\"\n\n        queue: asyncio.Queue[StreamEvent] = asyncio.Queue()\n        token = _stream_queue.set(queue)\n\n        try:\n            task = asyncio.create_task(\n                self.aexecute(stream=True, resume_state=resume_state, history=history, **inputs)\n            )\n\n            while True:\n                # Prioritize consuming events - check queue FIRST\n                try:\n                    event = await asyncio.wait_for(queue.get(), timeout=0.01)\n                    yield event\n                    continue  # Skip task.done() check, keep consuming\n                except TimeoutError:\n                    pass  # Queue empty, proceed to check task status\n\n                # Only check task completion when queue is idle\n                if task.done():\n                    try:\n                        await task\n                    except Exception:\n                        raise\n\n                    # Drain any remaining events\n                    while not queue.empty():\n                        event = queue.get_nowait()\n                        yield event\n                    break\n\n        finally:\n            try:\n                _stream_queue.reset(token)\n            except (ValueError, LookupError):\n                pass\n\n    async def aforward(\n        self, *, resume_state: Any = None, history: History | None = None, **inputs: Any\n    ) -&gt; Prediction:\n        \"\"\"Async non-streaming method. Returns final result directly.\n\n        This method calls aexecute() with streaming disabled. If called from\n        within a streaming context (i.e., another module is streaming), events\n        will still be emitted to the active queue.\n\n        Supports resuming from a ConfirmationRequired exception by providing\n        resume_state. This enables loop-based confirmation handling.\n\n        Args:\n            resume_state: Optional ResumeState containing exception and user response.\n                Can also be a raw ConfirmationRequired exception (will use \"yes\" as response).\n            history: Optional History object for maintaining conversation state.\n            **inputs: Input values for the module\n\n        Returns:\n            Final Prediction object\n\n        Example:\n            ```python\n            from udspy import ResumeState\n\n            # Loop-based confirmation handling\n            resume_state = None\n\n            while True:\n                try:\n                    result = await agent.aforward(\n                        question=\"Delete files\",\n                        resume_state=resume_state\n                    )\n                    break\n                except ConfirmationRequired as e:\n                    user_response = input(f\"{e.question} (yes/no): \")\n                    resume_state = ResumeState(e, user_response)\n            ```\n        \"\"\"\n        return await self.aexecute(\n            stream=False, resume_state=resume_state, history=history, **inputs\n        )\n\n    def forward(\n        self, *, resume_state: Any = None, history: History | None = None, **inputs: Any\n    ) -&gt; Prediction:\n        \"\"\"Sync non-streaming method. Wraps aforward() with async_to_sync.\n\n        This provides sync compatibility for scripts and notebooks. Cannot be\n        called from within an async context (use aforward() instead).\n\n        Supports resuming from a ConfirmationRequired exception by providing\n        resume_state. This enables loop-based confirmation handling.\n\n        Args:\n            resume_state: Optional ResumeState containing exception and user response.\n                Can also be a raw ConfirmationRequired exception (will use \"yes\" as response).\n            history: Optional History object for maintaining conversation state.\n            **inputs: Input values for the module (includes both input fields\n                and any module-specific parameters like auto_execute_tools)\n\n        Returns:\n            Final Prediction object\n\n        Raises:\n            RuntimeError: If called from within an async context\n\n        Example:\n            ```python\n            from udspy import ResumeState\n\n            # Loop-based confirmation handling\n            resume_state = None\n\n            while True:\n                try:\n                    result = agent.forward(\n                        question=\"Delete files\",\n                        resume_state=resume_state\n                    )\n                    break\n                except ConfirmationRequired as e:\n                    user_response = input(f\"{e.question} (yes/no): \")\n                    resume_state = ResumeState(e, user_response)\n            ```\n        \"\"\"\n        ensure_sync_context(f\"{self.__class__.__name__}.forward\")\n\n        return run_async_with_context(\n            self.aforward(resume_state=resume_state, history=history, **inputs)\n        )\n\n    def __call__(\n        self, *, resume_state: Any = None, history: History | None = None, **inputs: Any\n    ) -&gt; Prediction:\n        \"\"\"Sync convenience method. Calls forward().\n\n        Supports resuming from a ConfirmationRequired exception by providing\n        resume_state. This enables loop-based confirmation handling.\n\n        Args:\n            resume_state: Optional ResumeState containing exception and user response.\n                Can also be a raw ConfirmationRequired exception (will use \"yes\" as response).\n            history: Optional History object for maintaining conversation state.\n            **inputs: Input values for the module\n\n        Returns:\n            Final Prediction object\n\n        Example:\n            ```python\n            from udspy import ResumeState\n\n            # Loop-based confirmation handling\n            resume_state = None\n\n            while True:\n                try:\n                    result = agent(\n                        question=\"Delete files\",\n                        resume_state=resume_state\n                    )\n                    break\n                except ConfirmationRequired as e:\n                    user_response = input(f\"{e.question} (yes/no): \")\n                    resume_state = ResumeState(e, user_response)\n            ```\n        \"\"\"\n        return self.forward(resume_state=resume_state, history=history, **inputs)\n\n    async def asuspend(self, exception: ConfirmationRequired) -&gt; Any:\n        \"\"\"Async suspend execution and save state.\n\n        Called when ConfirmationRequired is raised. Subclasses should override\n        to save any module-specific state needed for resumption.\n\n        Args:\n            exception: The ConfirmationRequired exception that was raised\n\n        Returns:\n            Saved state (can be any type, will be passed to aresume)\n        \"\"\"\n        # Default implementation returns the exception itself as state\n        return exception\n\n    def suspend(self, exception: ConfirmationRequired) -&gt; Any:\n        \"\"\"Sync suspend execution and save state.\n\n        Wraps asuspend() with async_to_sync.\n\n        Args:\n            exception: The ConfirmationRequired exception that was raised\n\n        Returns:\n            Saved state (can be any type, will be passed to resume)\n        \"\"\"\n        ensure_sync_context(f\"{self.__class__.__name__}.suspend\")\n        return run_async_with_context(self.asuspend(exception))\n\n    async def aresume(self, user_response: str, saved_state: Any) -&gt; Prediction:\n        \"\"\"Async resume execution after user input.\n\n        Called to resume execution after a ConfirmationRequired exception.\n        Subclasses must override to implement resumption logic.\n\n        Args:\n            user_response: The user's response. Can be:\n                - \"yes\"/\"y\" to approve the action\n                - \"no\"/\"n\" to reject the action\n                - \"feedback\" to provide feedback for LLM re-reasoning\n                - JSON string with \"edit\" to modify tool arguments\n            saved_state: State returned from asuspend()\n\n        Returns:\n            Final Prediction object\n\n        Raises:\n            NotImplementedError: If not implemented by subclass\n        \"\"\"\n        raise NotImplementedError(f\"{self.__class__.__name__} must implement aresume() method\")\n\n    def resume(self, user_response: str, saved_state: Any) -&gt; Prediction:\n        \"\"\"Sync resume execution after user input.\n\n        Wraps aresume() with async_to_sync.\n\n        Args:\n            user_response: The user's response\n            saved_state: State returned from suspend()\n\n        Returns:\n            Final Prediction object\n        \"\"\"\n        ensure_sync_context(f\"{self.__class__.__name__}.resume\")\n        return run_async_with_context(self.aresume(user_response, saved_state))\n</code></pre>"},{"location":"api/module/#udspy.module.Module-functions","title":"Functions","text":""},{"location":"api/module/#udspy.module.Module.__call__","title":"<code>__call__(*, resume_state=None, history=None, **inputs)</code>","text":"<p>Sync convenience method. Calls forward().</p> <p>Supports resuming from a ConfirmationRequired exception by providing resume_state. This enables loop-based confirmation handling.</p> <p>Parameters:</p> Name Type Description Default <code>resume_state</code> <code>Any</code> <p>Optional ResumeState containing exception and user response. Can also be a raw ConfirmationRequired exception (will use \"yes\" as response).</p> <code>None</code> <code>history</code> <code>History | None</code> <p>Optional History object for maintaining conversation state.</p> <code>None</code> <code>**inputs</code> <code>Any</code> <p>Input values for the module</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object</p> Example <pre><code>from udspy import ResumeState\n\n# Loop-based confirmation handling\nresume_state = None\n\nwhile True:\n    try:\n        result = agent(\n            question=\"Delete files\",\n            resume_state=resume_state\n        )\n        break\n    except ConfirmationRequired as e:\n        user_response = input(f\"{e.question} (yes/no): \")\n        resume_state = ResumeState(e, user_response)\n</code></pre> Source code in <code>src/udspy/module/base.py</code> <pre><code>def __call__(\n    self, *, resume_state: Any = None, history: History | None = None, **inputs: Any\n) -&gt; Prediction:\n    \"\"\"Sync convenience method. Calls forward().\n\n    Supports resuming from a ConfirmationRequired exception by providing\n    resume_state. This enables loop-based confirmation handling.\n\n    Args:\n        resume_state: Optional ResumeState containing exception and user response.\n            Can also be a raw ConfirmationRequired exception (will use \"yes\" as response).\n        history: Optional History object for maintaining conversation state.\n        **inputs: Input values for the module\n\n    Returns:\n        Final Prediction object\n\n    Example:\n        ```python\n        from udspy import ResumeState\n\n        # Loop-based confirmation handling\n        resume_state = None\n\n        while True:\n            try:\n                result = agent(\n                    question=\"Delete files\",\n                    resume_state=resume_state\n                )\n                break\n            except ConfirmationRequired as e:\n                user_response = input(f\"{e.question} (yes/no): \")\n                resume_state = ResumeState(e, user_response)\n        ```\n    \"\"\"\n    return self.forward(resume_state=resume_state, history=history, **inputs)\n</code></pre>"},{"location":"api/module/#udspy.module.Module.aexecute","title":"<code>aexecute(*, stream=False, history=None, **inputs)</code>  <code>async</code>","text":"<p>Core execution method. Must be implemented by subclasses.</p> <p>This is the single implementation point for both streaming and non-streaming execution. It always returns a Prediction, and optionally emits StreamEvent objects to the active queue (if one exists in the context).</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>bool</code> <p>If True, request streaming from LLM provider. If False, use non-streaming API calls.</p> <code>False</code> <code>history</code> <code>History | None</code> <p>Optional History object for maintaining conversation state</p> <code>None</code> <code>**inputs</code> <code>Any</code> <p>Input values for the module</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object</p> Behavior <ul> <li>Checks for active stream queue via _stream_queue.get()</li> <li>If queue exists: emits OutputStreamChunk and Prediction events</li> <li>Always returns final Prediction (even in streaming mode)</li> <li>This enables composability: nested modules emit events automatically</li> </ul> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by subclass</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>@with_callbacks\nasync def aexecute(\n    self, *, stream: bool = False, history: History | None = None, **inputs: Any\n) -&gt; Prediction:\n    \"\"\"Core execution method. Must be implemented by subclasses.\n\n    This is the single implementation point for both streaming and non-streaming\n    execution. It always returns a Prediction, and optionally emits StreamEvent\n    objects to the active queue (if one exists in the context).\n\n    Args:\n        stream: If True, request streaming from LLM provider. If False, use\n            non-streaming API calls.\n        history: Optional History object for maintaining conversation state\n        **inputs: Input values for the module\n\n    Returns:\n        Final Prediction object\n\n    Behavior:\n        - Checks for active stream queue via _stream_queue.get()\n        - If queue exists: emits OutputStreamChunk and Prediction events\n        - Always returns final Prediction (even in streaming mode)\n        - This enables composability: nested modules emit events automatically\n\n    Raises:\n        NotImplementedError: If not implemented by subclass\n    \"\"\"\n    raise NotImplementedError(f\"{self.__class__.__name__} must implement aexecute() method\")\n</code></pre>"},{"location":"api/module/#udspy.module.Module.aforward","title":"<code>aforward(*, resume_state=None, history=None, **inputs)</code>  <code>async</code>","text":"<p>Async non-streaming method. Returns final result directly.</p> <p>This method calls aexecute() with streaming disabled. If called from within a streaming context (i.e., another module is streaming), events will still be emitted to the active queue.</p> <p>Supports resuming from a ConfirmationRequired exception by providing resume_state. This enables loop-based confirmation handling.</p> <p>Parameters:</p> Name Type Description Default <code>resume_state</code> <code>Any</code> <p>Optional ResumeState containing exception and user response. Can also be a raw ConfirmationRequired exception (will use \"yes\" as response).</p> <code>None</code> <code>history</code> <code>History | None</code> <p>Optional History object for maintaining conversation state.</p> <code>None</code> <code>**inputs</code> <code>Any</code> <p>Input values for the module</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object</p> Example <pre><code>from udspy import ResumeState\n\n# Loop-based confirmation handling\nresume_state = None\n\nwhile True:\n    try:\n        result = await agent.aforward(\n            question=\"Delete files\",\n            resume_state=resume_state\n        )\n        break\n    except ConfirmationRequired as e:\n        user_response = input(f\"{e.question} (yes/no): \")\n        resume_state = ResumeState(e, user_response)\n</code></pre> Source code in <code>src/udspy/module/base.py</code> <pre><code>async def aforward(\n    self, *, resume_state: Any = None, history: History | None = None, **inputs: Any\n) -&gt; Prediction:\n    \"\"\"Async non-streaming method. Returns final result directly.\n\n    This method calls aexecute() with streaming disabled. If called from\n    within a streaming context (i.e., another module is streaming), events\n    will still be emitted to the active queue.\n\n    Supports resuming from a ConfirmationRequired exception by providing\n    resume_state. This enables loop-based confirmation handling.\n\n    Args:\n        resume_state: Optional ResumeState containing exception and user response.\n            Can also be a raw ConfirmationRequired exception (will use \"yes\" as response).\n        history: Optional History object for maintaining conversation state.\n        **inputs: Input values for the module\n\n    Returns:\n        Final Prediction object\n\n    Example:\n        ```python\n        from udspy import ResumeState\n\n        # Loop-based confirmation handling\n        resume_state = None\n\n        while True:\n            try:\n                result = await agent.aforward(\n                    question=\"Delete files\",\n                    resume_state=resume_state\n                )\n                break\n            except ConfirmationRequired as e:\n                user_response = input(f\"{e.question} (yes/no): \")\n                resume_state = ResumeState(e, user_response)\n        ```\n    \"\"\"\n    return await self.aexecute(\n        stream=False, resume_state=resume_state, history=history, **inputs\n    )\n</code></pre>"},{"location":"api/module/#udspy.module.Module.aresume","title":"<code>aresume(user_response, saved_state)</code>  <code>async</code>","text":"<p>Async resume execution after user input.</p> <p>Called to resume execution after a ConfirmationRequired exception. Subclasses must override to implement resumption logic.</p> <p>Parameters:</p> Name Type Description Default <code>user_response</code> <code>str</code> <p>The user's response. Can be: - \"yes\"/\"y\" to approve the action - \"no\"/\"n\" to reject the action - \"feedback\" to provide feedback for LLM re-reasoning - JSON string with \"edit\" to modify tool arguments</p> required <code>saved_state</code> <code>Any</code> <p>State returned from asuspend()</p> required <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by subclass</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>async def aresume(self, user_response: str, saved_state: Any) -&gt; Prediction:\n    \"\"\"Async resume execution after user input.\n\n    Called to resume execution after a ConfirmationRequired exception.\n    Subclasses must override to implement resumption logic.\n\n    Args:\n        user_response: The user's response. Can be:\n            - \"yes\"/\"y\" to approve the action\n            - \"no\"/\"n\" to reject the action\n            - \"feedback\" to provide feedback for LLM re-reasoning\n            - JSON string with \"edit\" to modify tool arguments\n        saved_state: State returned from asuspend()\n\n    Returns:\n        Final Prediction object\n\n    Raises:\n        NotImplementedError: If not implemented by subclass\n    \"\"\"\n    raise NotImplementedError(f\"{self.__class__.__name__} must implement aresume() method\")\n</code></pre>"},{"location":"api/module/#udspy.module.Module.astream","title":"<code>astream(*, resume_state=None, history=None, **inputs)</code>  <code>async</code>","text":"<p>Async streaming method. Sets up queue and yields events.</p> <p>This method sets up the stream queue context, calls aexecute() with streaming enabled, and yields all events from the queue.</p> <p>Supports resuming from a ConfirmationRequired exception by providing resume_state. This enables streaming with confirmation handling.</p> <p>Parameters:</p> Name Type Description Default <code>resume_state</code> <code>Any</code> <p>Optional ResumeState containing exception and user response. Can also be a raw ConfirmationRequired exception (will use \"yes\" as response).</p> <code>None</code> <code>history</code> <code>History | None</code> <p>Optional History object for maintaining conversation state.</p> <code>None</code> <code>**inputs</code> <code>Any</code> <p>Input values for the module</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[StreamEvent]</code> <p>StreamEvent objects (OutputStreamChunk, Prediction, and custom events)</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>async def astream(\n    self, *, resume_state: Any = None, history: History | None = None, **inputs: Any\n) -&gt; AsyncGenerator[StreamEvent]:\n    \"\"\"Async streaming method. Sets up queue and yields events.\n\n    This method sets up the stream queue context, calls aexecute() with\n    streaming enabled, and yields all events from the queue.\n\n    Supports resuming from a ConfirmationRequired exception by providing\n    resume_state. This enables streaming with confirmation handling.\n\n    Args:\n        resume_state: Optional ResumeState containing exception and user response.\n            Can also be a raw ConfirmationRequired exception (will use \"yes\" as response).\n        history: Optional History object for maintaining conversation state.\n        **inputs: Input values for the module\n\n    Yields:\n        StreamEvent objects (OutputStreamChunk, Prediction, and custom events)\n    \"\"\"\n\n    queue: asyncio.Queue[StreamEvent] = asyncio.Queue()\n    token = _stream_queue.set(queue)\n\n    try:\n        task = asyncio.create_task(\n            self.aexecute(stream=True, resume_state=resume_state, history=history, **inputs)\n        )\n\n        while True:\n            # Prioritize consuming events - check queue FIRST\n            try:\n                event = await asyncio.wait_for(queue.get(), timeout=0.01)\n                yield event\n                continue  # Skip task.done() check, keep consuming\n            except TimeoutError:\n                pass  # Queue empty, proceed to check task status\n\n            # Only check task completion when queue is idle\n            if task.done():\n                try:\n                    await task\n                except Exception:\n                    raise\n\n                # Drain any remaining events\n                while not queue.empty():\n                    event = queue.get_nowait()\n                    yield event\n                break\n\n    finally:\n        try:\n            _stream_queue.reset(token)\n        except (ValueError, LookupError):\n            pass\n</code></pre>"},{"location":"api/module/#udspy.module.Module.asuspend","title":"<code>asuspend(exception)</code>  <code>async</code>","text":"<p>Async suspend execution and save state.</p> <p>Called when ConfirmationRequired is raised. Subclasses should override to save any module-specific state needed for resumption.</p> <p>Parameters:</p> Name Type Description Default <code>exception</code> <code>ConfirmationRequired</code> <p>The ConfirmationRequired exception that was raised</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Saved state (can be any type, will be passed to aresume)</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>async def asuspend(self, exception: ConfirmationRequired) -&gt; Any:\n    \"\"\"Async suspend execution and save state.\n\n    Called when ConfirmationRequired is raised. Subclasses should override\n    to save any module-specific state needed for resumption.\n\n    Args:\n        exception: The ConfirmationRequired exception that was raised\n\n    Returns:\n        Saved state (can be any type, will be passed to aresume)\n    \"\"\"\n    # Default implementation returns the exception itself as state\n    return exception\n</code></pre>"},{"location":"api/module/#udspy.module.Module.forward","title":"<code>forward(*, resume_state=None, history=None, **inputs)</code>","text":"<p>Sync non-streaming method. Wraps aforward() with async_to_sync.</p> <p>This provides sync compatibility for scripts and notebooks. Cannot be called from within an async context (use aforward() instead).</p> <p>Supports resuming from a ConfirmationRequired exception by providing resume_state. This enables loop-based confirmation handling.</p> <p>Parameters:</p> Name Type Description Default <code>resume_state</code> <code>Any</code> <p>Optional ResumeState containing exception and user response. Can also be a raw ConfirmationRequired exception (will use \"yes\" as response).</p> <code>None</code> <code>history</code> <code>History | None</code> <p>Optional History object for maintaining conversation state.</p> <code>None</code> <code>**inputs</code> <code>Any</code> <p>Input values for the module (includes both input fields and any module-specific parameters like auto_execute_tools)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called from within an async context</p> Example <pre><code>from udspy import ResumeState\n\n# Loop-based confirmation handling\nresume_state = None\n\nwhile True:\n    try:\n        result = agent.forward(\n            question=\"Delete files\",\n            resume_state=resume_state\n        )\n        break\n    except ConfirmationRequired as e:\n        user_response = input(f\"{e.question} (yes/no): \")\n        resume_state = ResumeState(e, user_response)\n</code></pre> Source code in <code>src/udspy/module/base.py</code> <pre><code>def forward(\n    self, *, resume_state: Any = None, history: History | None = None, **inputs: Any\n) -&gt; Prediction:\n    \"\"\"Sync non-streaming method. Wraps aforward() with async_to_sync.\n\n    This provides sync compatibility for scripts and notebooks. Cannot be\n    called from within an async context (use aforward() instead).\n\n    Supports resuming from a ConfirmationRequired exception by providing\n    resume_state. This enables loop-based confirmation handling.\n\n    Args:\n        resume_state: Optional ResumeState containing exception and user response.\n            Can also be a raw ConfirmationRequired exception (will use \"yes\" as response).\n        history: Optional History object for maintaining conversation state.\n        **inputs: Input values for the module (includes both input fields\n            and any module-specific parameters like auto_execute_tools)\n\n    Returns:\n        Final Prediction object\n\n    Raises:\n        RuntimeError: If called from within an async context\n\n    Example:\n        ```python\n        from udspy import ResumeState\n\n        # Loop-based confirmation handling\n        resume_state = None\n\n        while True:\n            try:\n                result = agent.forward(\n                    question=\"Delete files\",\n                    resume_state=resume_state\n                )\n                break\n            except ConfirmationRequired as e:\n                user_response = input(f\"{e.question} (yes/no): \")\n                resume_state = ResumeState(e, user_response)\n        ```\n    \"\"\"\n    ensure_sync_context(f\"{self.__class__.__name__}.forward\")\n\n    return run_async_with_context(\n        self.aforward(resume_state=resume_state, history=history, **inputs)\n    )\n</code></pre>"},{"location":"api/module/#udspy.module.Module.init_module","title":"<code>init_module(tools=None)</code>  <code>abstractmethod</code>","text":"<p>Initialize or reinitialize the module with new tools.</p> <p>This method provides a way to completely reinitialize module state, including tools, tool schemas, and signatures. It's designed to be called from module callbacks that need to dynamically modify the module during execution.</p> <p>When implementing this method, subclasses should: 1. Rebuild the tools dictionary 2. Regenerate tool schemas (if applicable) 3. Rebuild signatures with new tool descriptions (if applicable) 4. Preserve built-in tools (if applicable)</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>list[Callable[..., Any]] | None</code> <p>New tools to initialize with. Format depends on subclass: - Can be functions (will be wrapped in Tool) - Can be Tool instances - None means clear all non-built-in tools</p> <code>None</code> Example <pre><code>from udspy import module_callback\n\n@module_callback\ndef add_tools(context):\n    # Get current tools\n    current = list(context.module.tools.values())\n\n    # Add new tools\n    new_tools = [weather_tool, calendar_tool]\n\n    # Reinitialize module with all tools\n    context.module.init_module(tools=current + new_tools)\n\n    return \"Added weather and calendar tools\"\n</code></pre> Note <p>This method is typically called from within a module callback decorated with @module_callback. The callback receives a context object with access to the module instance.</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>@abstractmethod\ndef init_module(self, tools: list[Callable[..., Any]] | None = None) -&gt; None:\n    \"\"\"Initialize or reinitialize the module with new tools.\n\n    This method provides a way to completely reinitialize module state,\n    including tools, tool schemas, and signatures. It's designed to be\n    called from module callbacks that need to dynamically modify the\n    module during execution.\n\n    When implementing this method, subclasses should:\n    1. Rebuild the tools dictionary\n    2. Regenerate tool schemas (if applicable)\n    3. Rebuild signatures with new tool descriptions (if applicable)\n    4. Preserve built-in tools (if applicable)\n\n    Args:\n        tools: New tools to initialize with. Format depends on subclass:\n            - Can be functions (will be wrapped in Tool)\n            - Can be Tool instances\n            - None means clear all non-built-in tools\n\n    Example:\n        ```python\n        from udspy import module_callback\n\n        @module_callback\n        def add_tools(context):\n            # Get current tools\n            current = list(context.module.tools.values())\n\n            # Add new tools\n            new_tools = [weather_tool, calendar_tool]\n\n            # Reinitialize module with all tools\n            context.module.init_module(tools=current + new_tools)\n\n            return \"Added weather and calendar tools\"\n        ```\n\n    Note:\n        This method is typically called from within a module callback\n        decorated with @module_callback. The callback receives a context\n        object with access to the module instance.\n    \"\"\"\n    raise NotImplementedError(f\"{self.__class__.__name__} must implement init_module() method\")\n</code></pre>"},{"location":"api/module/#udspy.module.Module.resume","title":"<code>resume(user_response, saved_state)</code>","text":"<p>Sync resume execution after user input.</p> <p>Wraps aresume() with async_to_sync.</p> <p>Parameters:</p> Name Type Description Default <code>user_response</code> <code>str</code> <p>The user's response</p> required <code>saved_state</code> <code>Any</code> <p>State returned from suspend()</p> required <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>def resume(self, user_response: str, saved_state: Any) -&gt; Prediction:\n    \"\"\"Sync resume execution after user input.\n\n    Wraps aresume() with async_to_sync.\n\n    Args:\n        user_response: The user's response\n        saved_state: State returned from suspend()\n\n    Returns:\n        Final Prediction object\n    \"\"\"\n    ensure_sync_context(f\"{self.__class__.__name__}.resume\")\n    return run_async_with_context(self.aresume(user_response, saved_state))\n</code></pre>"},{"location":"api/module/#udspy.module.Module.suspend","title":"<code>suspend(exception)</code>","text":"<p>Sync suspend execution and save state.</p> <p>Wraps asuspend() with async_to_sync.</p> <p>Parameters:</p> Name Type Description Default <code>exception</code> <code>ConfirmationRequired</code> <p>The ConfirmationRequired exception that was raised</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Saved state (can be any type, will be passed to resume)</p> Source code in <code>src/udspy/module/base.py</code> <pre><code>def suspend(self, exception: ConfirmationRequired) -&gt; Any:\n    \"\"\"Sync suspend execution and save state.\n\n    Wraps asuspend() with async_to_sync.\n\n    Args:\n        exception: The ConfirmationRequired exception that was raised\n\n    Returns:\n        Saved state (can be any type, will be passed to resume)\n    \"\"\"\n    ensure_sync_context(f\"{self.__class__.__name__}.suspend\")\n    return run_async_with_context(self.asuspend(exception))\n</code></pre>"},{"location":"api/module/#udspy.module.Predict","title":"<code>Predict</code>","text":"<p>               Bases: <code>Module</code></p> <p>Module for making LLM predictions based on a signature.</p> <p>This is an async-first module. The core method is <code>astream()</code> which yields StreamEvent objects. Use <code>aforward()</code> for async non-streaming, or <code>forward()</code> for sync usage.</p> Example <pre><code>from udspy import Predict, Signature, InputField, OutputField\n\nclass QA(Signature):\n    '''Answer questions.'''\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\n\n# Sync usage\nresult = predictor(question=\"What is 2+2?\")\nprint(result.answer)\n\n# Async non-streaming\nresult = await predictor.aforward(question=\"What is 2+2?\")\n\n# Async streaming\nfrom udspy.streaming import OutputStreamChunk\nasync for event in predictor.astream(question=\"What is 2+2?\"):\n    if isinstance(event, OutputStreamChunk):\n        print(event.delta, end=\"\", flush=True)\n</code></pre> Source code in <code>src/udspy/module/predict.py</code> <pre><code>class Predict(Module):\n    \"\"\"Module for making LLM predictions based on a signature.\n\n    This is an async-first module. The core method is `astream()` which yields\n    StreamEvent objects. Use `aforward()` for async non-streaming, or `forward()`\n    for sync usage.\n\n    Example:\n        ```python\n        from udspy import Predict, Signature, InputField, OutputField\n\n        class QA(Signature):\n            '''Answer questions.'''\n            question: str = InputField()\n            answer: str = OutputField()\n\n        predictor = Predict(QA)\n\n        # Sync usage\n        result = predictor(question=\"What is 2+2?\")\n        print(result.answer)\n\n        # Async non-streaming\n        result = await predictor.aforward(question=\"What is 2+2?\")\n\n        # Async streaming\n        from udspy.streaming import OutputStreamChunk\n        async for event in predictor.astream(question=\"What is 2+2?\"):\n            if isinstance(event, OutputStreamChunk):\n                print(event.delta, end=\"\", flush=True)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        signature: type[Signature] | str,\n        *,\n        tools: list[Tool] | None = None,\n        max_turns: int = 10,\n        adapter: ChatAdapter | None = None,\n        model: str | None = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Initialize a Predict module.\n\n        Args:\n            signature: Signature defining inputs and outputs, or a string in\n                      format \"inputs -&gt; outputs\" (e.g., \"question -&gt; answer\")\n            model: Model name (overrides global default)\n            tools: List of tool functions (decorated with @tool) or Pydantic models\n            max_turns: Maximum number of LLM calls for tool execution loop (default: 10)\n            adapter: Custom adapter (defaults to ChatAdapter)\n            **kwargs: Additional arguments for chat completion (temperature, callbacks, etc.)\n        \"\"\"\n        if isinstance(signature, str):\n            signature = Signature.from_string(signature)\n\n        self.signature = signature\n        self._model = model\n        self._kwargs = kwargs\n        self.max_turns = max_turns\n        if max_turns &lt; 1:\n            raise ValueError(\"max_turns must be at least 1\")\n        self.adapter = adapter or ChatAdapter()\n\n        self.init_module(tools=tools)\n\n    @property\n    def model(self) -&gt; str | None:\n        \"\"\"Get the model name override, or None to use LM's default.\"\"\"\n        return self._model\n\n    @property\n    def kwargs(self) -&gt; dict[str, Any]:\n        return {**settings.default_kwargs, **self._kwargs}\n\n    def init_module(self, tools: list[Any] | None = None) -&gt; None:\n        \"\"\"Initialize or reinitialize Predict with new tools.\n\n        This method rebuilds the tools dictionary and regenerates tool schemas.\n        It's designed to be called from module callbacks to dynamically modify\n        available tools during execution.\n\n        Args:\n            tools: New tools to initialize with. Can be:\n                - Functions decorated with @tool\n                - Tool instances\n                - None to clear all tools\n\n        Example:\n            ```python\n            from udspy import module_callback\n\n            @module_callback\n            def add_specialized_tools(context):\n                # Get current tools\n                current_tools = list(context.module.tools.values())\n\n                # Add new tools\n                new_tools = [weather_tool, calendar_tool]\n\n                # Reinitialize with all tools\n                context.module.init_module(tools=current_tools + new_tools)\n\n                return \"Added weather and calendar tools\"\n            ```\n        \"\"\"\n        self._init_tools(tools or [])\n\n    def _init_tools(self, tools: list[Any]) -&gt; None:\n        \"\"\"Initialize tools dictionary with provided tools.\n\n        Args:\n            tools: List of tools (functions or Tool instances)\n        \"\"\"\n        tool_list = [t if isinstance(t, Tool) else Tool(t) for t in tools]\n        self.tools = {tool.name: tool for tool in tool_list if tool.name}\n        self._build_tool_schemas()\n\n    def _build_tool_schemas(self) -&gt; None:\n        \"\"\"Build OpenAI tool schemas from current tools.\"\"\"\n        self.tool_schemas = [self.adapter.format_tool_schema(tool) for tool in self.tools.values()]\n\n    @suspendable\n    @with_callbacks\n    async def aexecute(\n        self,\n        *,\n        stream: bool = False,\n        auto_execute_tools: bool = True,\n        history: History | None = None,\n        **inputs: Any,\n    ) -&gt; Prediction:\n        \"\"\"Core execution method - handles both streaming and non-streaming.\n\n        This is the single implementation point for LLM interaction. It always\n        returns a Prediction, and emits events to the queue if one is active.\n\n        Args:\n            stream: If True, request streaming from OpenAI. If False, use regular API.\n            auto_execute_tools: If True, automatically execute tools and continue.\n                If False, return Prediction with tool_calls for manual handling.\n            history: Optional History object for multi-turn conversations.\n            **inputs: Input values matching the signature's input fields\n\n        Returns:\n            Final Prediction object (after all tool executions if auto_execute_tools=True)\n        \"\"\"\n        if history is None:\n            history = History()\n\n        self._validate_inputs(inputs)\n        self._build_initial_messages(inputs, history)\n\n        return await self._aexecute(\n            stream=stream,\n            auto_execute_tools=auto_execute_tools,\n            history=history,\n        )\n\n    def _validate_inputs(self, inputs: dict[str, Any]) -&gt; None:\n        \"\"\"Validate that all required inputs are provided.\"\"\"\n        input_fields = self.signature.get_input_fields()\n        for field_name in input_fields:\n            if field_name not in inputs:\n                raise ValueError(f\"Missing required input field: {field_name}\")\n\n    def _build_initial_messages(self, inputs: dict[str, Any], history: History) -&gt; None:\n        \"\"\"Build initial messages from inputs and optional history.\n\n        Args:\n            inputs: Input values from user\n            history: History object with existing conversation\n        \"\"\"\n        history.set_system_message(self.adapter.format_instructions(self.signature))\n        history.add_user_message(self.adapter.format_user_request(self.signature, inputs))\n\n    async def _aexecute(\n        self,\n        stream: bool,\n        auto_execute_tools: bool,\n        history: History,\n    ) -&gt; Prediction:\n        \"\"\"Execute multi-turn conversation with optional automatic tool execution.\n\n        This is the core execution loop that handles both streaming and non-streaming.\n\n        Args:\n            stream: If True, request streaming from OpenAI\n            auto_execute_tools: If True, automatically execute tools. If False,\n                return after first tool call.\n            history: Optional History object to update with conversation\n\n        Returns:\n            Final Prediction object\n        \"\"\"\n        prediction: Prediction | None = None\n\n        for turn in range(self.max_turns):\n            prediction = await self._aexecute_one_turn(history.messages, turn, stream=stream)\n\n            if not auto_execute_tools or not prediction.native_tool_calls:\n                break\n\n            await self._aexecute_tool_calls(prediction.native_tool_calls, history)\n        else:\n            if prediction is not None and not prediction.is_final:\n                raise RuntimeError(f\"Max turns ({self.max_turns}) reached without final answer\")\n\n        if prediction is None:\n            raise RuntimeError(\"No prediction generated\")\n\n        self._update_history_with_prediction(history, prediction)\n        return prediction\n\n    async def _aexecute_one_turn(\n        self, messages: list[dict[str, Any]], turn: int, stream: bool\n    ) -&gt; Prediction:\n        \"\"\"Execute one LLM turn (streaming or non-streaming).\n\n        Args:\n            messages: Conversation messages\n            turn: Current turn number (0-indexed)\n            stream: If True, request streaming from OpenAI\n\n        Returns:\n            Prediction object for this turn\n        \"\"\"\n        completion_kwargs: dict[str, Any] = {\n            \"messages\": messages,\n            \"stream\": stream,\n            \"tools\": self.tool_schemas,\n            **self.kwargs,\n        }\n\n        # Only pass model if explicitly set (otherwise LM uses its default)\n        if self.model is not None:\n            completion_kwargs[\"model\"] = self.model\n\n        func = self._astream if stream else self._aforward\n        return await func(completion_kwargs)\n\n    async def _aexecute_tool_calls(\n        self,\n        native_tool_calls: list[ToolCall],\n        history: History,\n    ) -&gt; None:\n        \"\"\"Execute tool calls and add results to messages.\n\n        Args:\n            tool_calls: List of tool calls to execute\n            history: History object to update\n        \"\"\"\n        history.add_assistant_message(\n            tool_calls=[\n                {\n                    \"id\": tc.call_id,\n                    \"type\": \"function\",\n                    \"function\": {\"name\": tc.name, \"arguments\": json.dumps(tc.args)},\n                }\n                for tc in native_tool_calls\n            ]\n        )\n\n        for tool_call in native_tool_calls:\n            call_id = tool_call.call_id\n            tool_name = tool_call.name\n            tool_args = tool_call.args\n\n            content: str = \"\"\n            if tool_name in self.tools:\n                try:\n                    result = await self.tools[tool_name](**tool_args)\n\n                    if is_module_callback(result):\n                        context = PredictContext(module=self, history=history)\n                        content = result(context)\n                    elif isinstance(result, BaseModel):\n                        content = result.model_dump_json()\n                    elif not isinstance(result, str):\n                        content = json.dumps(result)\n                    else:\n                        content = result\n                except Exception as e:\n                    content = f\"Error executing tool: {e}\"\n            else:\n                content = f\"Error: Tool `{tool_name}` not found.\"\n                available_tools = \", \".join(f\"`{tool}`\" for tool in self.tools.keys())\n                if available_tools:\n                    content += f\" Available tools are: {available_tools}.\"\n                else:\n                    content += \" No tools are currently available.\"\n\n            history.add_tool_result(str(call_id), content)\n\n    def _update_history_with_prediction(self, history: History, prediction: Prediction) -&gt; None:\n        \"\"\"Update history with assistant's prediction.\n\n        Args:\n            history: History object to update\n            prediction: Prediction from assistant\n        \"\"\"\n        output_fields = self.signature.get_output_fields()\n        content_parts = []\n\n        for field_name in output_fields:\n            if hasattr(prediction, field_name):\n                value = getattr(prediction, field_name)\n                if value:\n                    content_parts.append(f\"[[ ## {field_name} ## ]]\\n{value}\")\n\n        content = \"\\n\".join(content_parts) if content_parts else \"\"\n        history.add_assistant_message(content)\n\n    @retry(\n        retry=retry_if_exception_type(AdapterParseError),\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=0.1, max=3),\n    )\n    async def _aforward(self, completion_kwargs: dict[str, Any]) -&gt; Prediction:\n        \"\"\"Process non-streaming LLM call with automatic retry on parse errors.\n\n        Retries up to 2 times (3 total attempts) with exponential backoff (0.1-3s)\n        when AdapterParseError occurs, giving the LLM multiple chances to format\n        the response correctly.\n\n        Args:\n            completion_kwargs: Arguments for the completion API call\n\n        Returns:\n            Prediction object\n        \"\"\"\n\n        response = await settings.lm.acomplete(**completion_kwargs)\n\n        message = response.choices[0].message  # type: ignore[union-attr]\n        native_tool_calls: list[ToolCall] = []\n        for tc in message.tool_calls or []:\n            try:\n                arguments = (\n                    json.loads(tc.function.arguments)  # type: ignore[union-attr]\n                    if isinstance(tc.function.arguments, str)  # type: ignore[union-attr]\n                    else tc.function.arguments  # type: ignore[union-attr]\n                )\n            except json.JSONDecodeError as exc:\n                raise AdapterParseError(\n                    adapter_name=self.adapter.__class__.__name__,\n                    signature=self.signature,\n                    lm_response=tc.function.arguments,  # type: ignore[union-attr]\n                    parsed_result={\n                        \"error\": f\"Failed to parse tool call {tc.id} arguments as JSON.\"\n                    },\n                ) from exc\n\n            else:\n                native_tool_calls.append(\n                    ToolCall(call_id=tc.id, name=tc.function.name, args=arguments)  # type: ignore[union-attr]\n                )\n\n        _, completion_text = self.adapter.split_reasoning_and_content_delta(response)  # type: ignore[arg-type]\n        outputs = self.adapter.parse_outputs(self.signature, completion_text)\n\n        self.adapter.validate_outputs(self.signature, outputs, native_tool_calls, completion_text)\n\n        prediction = Prediction(module=self, native_tool_calls=native_tool_calls, **outputs)\n        emit_event(prediction)  # If a stream is active, emit the final prediction\n\n        return prediction\n\n    @retry(\n        retry=retry_if_exception_type(AdapterParseError),\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=0.1, max=3),\n    )\n    async def _astream(self, completion_kwargs: dict[str, Any]) -&gt; Prediction:\n        \"\"\"Process streaming LLM call with automatic retry on parse errors.\n\n        Retries up to 2 times (3 total attempts) with exponential backoff (0.1-3s)\n        when AdapterParseError occurs, giving the LLM multiple chances to format\n        the response correctly.\n\n        Args:\n            completion_kwargs: Arguments for the completion API call\n\n        Returns:\n            Prediction object\n        \"\"\"\n\n        try:\n            # Reset parser for this attempt (important for retries)\n            self.adapter.reset_parser()\n\n            stream = await settings.lm.acomplete(**completion_kwargs)\n\n            # Process each chunk through adapter, which yields events\n            async for chunk in stream:  # type: ignore[union-attr]\n                async for event in self.adapter.process_chunk(chunk, self, self.signature):\n                    emit_event(event)\n\n            # Finalize and get validated outputs\n            outputs, native_tool_calls, _ = await self.adapter.finalize(self.signature)\n            prediction = Prediction(\n                module=self,\n                native_tool_calls=native_tool_calls,\n                **outputs,\n            )\n            emit_event(prediction)\n\n            return prediction\n\n        except Exception as exc:\n            import traceback\n\n            error_event = type(\n                \"StreamError\",\n                (StreamEvent,),\n                {\n                    \"error\": str(exc),\n                    \"traceback\": traceback.format_exc(),\n                    \"module\": self,\n                },\n            )()\n            emit_event(error_event)\n            raise\n</code></pre>"},{"location":"api/module/#udspy.module.Predict-attributes","title":"Attributes","text":""},{"location":"api/module/#udspy.module.Predict.model","title":"<code>model</code>  <code>property</code>","text":"<p>Get the model name override, or None to use LM's default.</p>"},{"location":"api/module/#udspy.module.Predict-functions","title":"Functions","text":""},{"location":"api/module/#udspy.module.Predict.__init__","title":"<code>__init__(signature, *, tools=None, max_turns=10, adapter=None, model=None, **kwargs)</code>","text":"<p>Initialize a Predict module.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature] | str</code> <p>Signature defining inputs and outputs, or a string in       format \"inputs -&gt; outputs\" (e.g., \"question -&gt; answer\")</p> required <code>model</code> <code>str | None</code> <p>Model name (overrides global default)</p> <code>None</code> <code>tools</code> <code>list[Tool] | None</code> <p>List of tool functions (decorated with @tool) or Pydantic models</p> <code>None</code> <code>max_turns</code> <code>int</code> <p>Maximum number of LLM calls for tool execution loop (default: 10)</p> <code>10</code> <code>adapter</code> <code>ChatAdapter | None</code> <p>Custom adapter (defaults to ChatAdapter)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for chat completion (temperature, callbacks, etc.)</p> <code>{}</code> Source code in <code>src/udspy/module/predict.py</code> <pre><code>def __init__(\n    self,\n    signature: type[Signature] | str,\n    *,\n    tools: list[Tool] | None = None,\n    max_turns: int = 10,\n    adapter: ChatAdapter | None = None,\n    model: str | None = None,\n    **kwargs: Any,\n):\n    \"\"\"Initialize a Predict module.\n\n    Args:\n        signature: Signature defining inputs and outputs, or a string in\n                  format \"inputs -&gt; outputs\" (e.g., \"question -&gt; answer\")\n        model: Model name (overrides global default)\n        tools: List of tool functions (decorated with @tool) or Pydantic models\n        max_turns: Maximum number of LLM calls for tool execution loop (default: 10)\n        adapter: Custom adapter (defaults to ChatAdapter)\n        **kwargs: Additional arguments for chat completion (temperature, callbacks, etc.)\n    \"\"\"\n    if isinstance(signature, str):\n        signature = Signature.from_string(signature)\n\n    self.signature = signature\n    self._model = model\n    self._kwargs = kwargs\n    self.max_turns = max_turns\n    if max_turns &lt; 1:\n        raise ValueError(\"max_turns must be at least 1\")\n    self.adapter = adapter or ChatAdapter()\n\n    self.init_module(tools=tools)\n</code></pre>"},{"location":"api/module/#udspy.module.Predict.aexecute","title":"<code>aexecute(*, stream=False, auto_execute_tools=True, history=None, **inputs)</code>  <code>async</code>","text":"<p>Core execution method - handles both streaming and non-streaming.</p> <p>This is the single implementation point for LLM interaction. It always returns a Prediction, and emits events to the queue if one is active.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>bool</code> <p>If True, request streaming from OpenAI. If False, use regular API.</p> <code>False</code> <code>auto_execute_tools</code> <code>bool</code> <p>If True, automatically execute tools and continue. If False, return Prediction with tool_calls for manual handling.</p> <code>True</code> <code>history</code> <code>History | None</code> <p>Optional History object for multi-turn conversations.</p> <code>None</code> <code>**inputs</code> <code>Any</code> <p>Input values matching the signature's input fields</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Final Prediction object (after all tool executions if auto_execute_tools=True)</p> Source code in <code>src/udspy/module/predict.py</code> <pre><code>@suspendable\n@with_callbacks\nasync def aexecute(\n    self,\n    *,\n    stream: bool = False,\n    auto_execute_tools: bool = True,\n    history: History | None = None,\n    **inputs: Any,\n) -&gt; Prediction:\n    \"\"\"Core execution method - handles both streaming and non-streaming.\n\n    This is the single implementation point for LLM interaction. It always\n    returns a Prediction, and emits events to the queue if one is active.\n\n    Args:\n        stream: If True, request streaming from OpenAI. If False, use regular API.\n        auto_execute_tools: If True, automatically execute tools and continue.\n            If False, return Prediction with tool_calls for manual handling.\n        history: Optional History object for multi-turn conversations.\n        **inputs: Input values matching the signature's input fields\n\n    Returns:\n        Final Prediction object (after all tool executions if auto_execute_tools=True)\n    \"\"\"\n    if history is None:\n        history = History()\n\n    self._validate_inputs(inputs)\n    self._build_initial_messages(inputs, history)\n\n    return await self._aexecute(\n        stream=stream,\n        auto_execute_tools=auto_execute_tools,\n        history=history,\n    )\n</code></pre>"},{"location":"api/module/#udspy.module.Predict.init_module","title":"<code>init_module(tools=None)</code>","text":"<p>Initialize or reinitialize Predict with new tools.</p> <p>This method rebuilds the tools dictionary and regenerates tool schemas. It's designed to be called from module callbacks to dynamically modify available tools during execution.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>list[Any] | None</code> <p>New tools to initialize with. Can be: - Functions decorated with @tool - Tool instances - None to clear all tools</p> <code>None</code> Example <pre><code>from udspy import module_callback\n\n@module_callback\ndef add_specialized_tools(context):\n    # Get current tools\n    current_tools = list(context.module.tools.values())\n\n    # Add new tools\n    new_tools = [weather_tool, calendar_tool]\n\n    # Reinitialize with all tools\n    context.module.init_module(tools=current_tools + new_tools)\n\n    return \"Added weather and calendar tools\"\n</code></pre> Source code in <code>src/udspy/module/predict.py</code> <pre><code>def init_module(self, tools: list[Any] | None = None) -&gt; None:\n    \"\"\"Initialize or reinitialize Predict with new tools.\n\n    This method rebuilds the tools dictionary and regenerates tool schemas.\n    It's designed to be called from module callbacks to dynamically modify\n    available tools during execution.\n\n    Args:\n        tools: New tools to initialize with. Can be:\n            - Functions decorated with @tool\n            - Tool instances\n            - None to clear all tools\n\n    Example:\n        ```python\n        from udspy import module_callback\n\n        @module_callback\n        def add_specialized_tools(context):\n            # Get current tools\n            current_tools = list(context.module.tools.values())\n\n            # Add new tools\n            new_tools = [weather_tool, calendar_tool]\n\n            # Reinitialize with all tools\n            context.module.init_module(tools=current_tools + new_tools)\n\n            return \"Added weather and calendar tools\"\n        ```\n    \"\"\"\n    self._init_tools(tools or [])\n</code></pre>"},{"location":"api/module/#udspy.module.PredictContext","title":"<code>PredictContext</code>","text":"<p>               Bases: <code>ModuleContext</code></p> <p>Context for Predict module callbacks.</p> <p>Provides access to both the module and the conversation history, allowing callbacks to inspect past interactions.</p> <p>Attributes:</p> Name Type Description <code>module</code> <p>The Predict module instance</p> <code>history</code> <p>Conversation history (if provided)</p> Source code in <code>src/udspy/module/callbacks.py</code> <pre><code>class PredictContext(ModuleContext):\n    \"\"\"Context for Predict module callbacks.\n\n    Provides access to both the module and the conversation history,\n    allowing callbacks to inspect past interactions.\n\n    Attributes:\n        module: The Predict module instance\n        history: Conversation history (if provided)\n    \"\"\"\n\n    def __init__(self, module: \"Predict\", history: Optional[\"History\"] = None):\n        \"\"\"Initialize Predict context.\n\n        Args:\n            module: The Predict module instance\n            history: Conversation history (if any)\n        \"\"\"\n        super().__init__(module)\n        self.history = history\n</code></pre>"},{"location":"api/module/#udspy.module.PredictContext-functions","title":"Functions","text":""},{"location":"api/module/#udspy.module.PredictContext.__init__","title":"<code>__init__(module, history=None)</code>","text":"<p>Initialize Predict context.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Predict</code> <p>The Predict module instance</p> required <code>history</code> <code>Optional[History]</code> <p>Conversation history (if any)</p> <code>None</code> Source code in <code>src/udspy/module/callbacks.py</code> <pre><code>def __init__(self, module: \"Predict\", history: Optional[\"History\"] = None):\n    \"\"\"Initialize Predict context.\n\n    Args:\n        module: The Predict module instance\n        history: Conversation history (if any)\n    \"\"\"\n    super().__init__(module)\n    self.history = history\n</code></pre>"},{"location":"api/module/#udspy.module.Prediction","title":"<code>Prediction</code>","text":"<p>               Bases: <code>StreamEvent</code>, <code>dict[str, Any]</code></p> <p>Final prediction result with attribute access.</p> <p>This is both a StreamEvent (can be yielded from astream) and a dict (for convenient attribute access to outputs).</p> <p>Attributes:</p> Name Type Description <code>module</code> <p>The module that produced this prediction</p> <code>native_tool_calls</code> <p>Tool calls from native LLM response (if any)</p> Example <pre><code>pred = Prediction(answer=\"Paris\", reasoning=\"France's capital\")\nprint(pred.answer)  # \"Paris\"\nprint(pred[\"answer\"])  # \"Paris\"\nprint(pred.is_final)  # True for top-level result\nprint(pred.module)  # Module instance that produced this\n</code></pre> Source code in <code>src/udspy/streaming.py</code> <pre><code>class Prediction(StreamEvent, dict[str, Any]):\n    \"\"\"Final prediction result with attribute access.\n\n    This is both a StreamEvent (can be yielded from astream) and a dict\n    (for convenient attribute access to outputs).\n\n    Attributes:\n        module: The module that produced this prediction\n        native_tool_calls: Tool calls from native LLM response (if any)\n\n    Example:\n        ```python\n        pred = Prediction(answer=\"Paris\", reasoning=\"France's capital\")\n        print(pred.answer)  # \"Paris\"\n        print(pred[\"answer\"])  # \"Paris\"\n        print(pred.is_final)  # True for top-level result\n        print(pred.module)  # Module instance that produced this\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        /,\n        module: \"Module | None\" = None,\n        native_tool_calls: list[\"ToolCall\"] | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(**kwargs)\n        self.module = module\n        self.native_tool_calls = native_tool_calls\n\n    @property\n    def is_final(self) -&gt; bool:\n        \"\"\"Whether this is the final prediction (no pending tool calls).\"\"\"\n        return bool(len(self.keys()) and not self.native_tool_calls)\n\n    def __getattr__(self, name: str) -&gt; Any:\n        try:\n            return self[name]\n        except KeyError:\n            raise AttributeError(f\"Prediction has no attribute '{name}'\") from None\n\n    def __setattr__(self, name: str, value: Any) -&gt; None:\n        self[name] = value\n</code></pre>"},{"location":"api/module/#udspy.module.Prediction-attributes","title":"Attributes","text":""},{"location":"api/module/#udspy.module.Prediction.is_final","title":"<code>is_final</code>  <code>property</code>","text":"<p>Whether this is the final prediction (no pending tool calls).</p>"},{"location":"api/module/#udspy.module.ReAct","title":"<code>ReAct</code>","text":"<p>               Bases: <code>Module</code></p> <p>ReAct (Reasoning and Acting) module for tool-using agents.</p> <p>ReAct iteratively reasons about the current situation and decides whether to call a tool or finish the task. Key features:</p> <ul> <li>Iterative reasoning with tool execution</li> <li>Tool confirmation support for sensitive operations</li> <li>Real-time streaming of reasoning and tool usage</li> </ul> <p>Example (Basic Usage):     <pre><code>from udspy import ReAct, Signature, InputField, OutputField, tool\nfrom pydantic import Field\n\n@tool(name=\"search\", description=\"Search for information\")\ndef search(query: str = Field(...)) -&gt; str:\n    return f\"Results for: {query}\"\n\nclass QA(Signature):\n    '''Answer questions using available tools.'''\n    question: str = InputField()\n    answer: str = OutputField()\n\nreact = ReAct(QA, tools=[search])\nresult = react(question=\"What is the weather in Tokyo?\")\n</code></pre></p> <p>Example (Streaming):     <pre><code># Stream the agent's reasoning process in real-time\nasync for event in react.astream(question=\"What is Python?\"):\n    if isinstance(event, OutputStreamChunk):\n        print(event.delta, end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        print(f\"Answer: {event.answer}\")\n</code></pre></p> <pre><code>See examples/react_streaming.py for a complete streaming example.\n</code></pre> <p>Example (Tools with Confirmation):     <pre><code>from udspy import ConfirmationRequired, ConfirmationRejected\n\n@tool(name=\"delete_file\", require_confirmation=True)\ndef delete_file(path: str = Field(...)) -&gt; str:\n    return f\"Deleted {path}\"\n\nreact = ReAct(QA, tools=[delete_file])\n\ntry:\n    result = await react.aforward(question=\"Delete /tmp/test.txt\")\nexcept ConfirmationRequired as e:\n    # User is asked for confirmation\n    print(f\"Confirm: {e.question}\")\n    # Approve: respond_to_confirmation(e.confirmation_id, approved=True)\n    # Reject: respond_to_confirmation(e.confirmation_id, approved=False, status=\"rejected\")\n    result = await react.aresume(\"yes\", e)\n</code></pre></p> Source code in <code>src/udspy/module/react.py</code> <pre><code>class ReAct(Module):\n    \"\"\"ReAct (Reasoning and Acting) module for tool-using agents.\n\n    ReAct iteratively reasons about the current situation and decides whether\n    to call a tool or finish the task. Key features:\n\n    - Iterative reasoning with tool execution\n    - Tool confirmation support for sensitive operations\n    - Real-time streaming of reasoning and tool usage\n\n    Example (Basic Usage):\n        ```python\n        from udspy import ReAct, Signature, InputField, OutputField, tool\n        from pydantic import Field\n\n        @tool(name=\"search\", description=\"Search for information\")\n        def search(query: str = Field(...)) -&gt; str:\n            return f\"Results for: {query}\"\n\n        class QA(Signature):\n            '''Answer questions using available tools.'''\n            question: str = InputField()\n            answer: str = OutputField()\n\n        react = ReAct(QA, tools=[search])\n        result = react(question=\"What is the weather in Tokyo?\")\n        ```\n\n    Example (Streaming):\n        ```python\n        # Stream the agent's reasoning process in real-time\n        async for event in react.astream(question=\"What is Python?\"):\n            if isinstance(event, OutputStreamChunk):\n                print(event.delta, end=\"\", flush=True)\n            elif isinstance(event, Prediction):\n                print(f\"Answer: {event.answer}\")\n        ```\n\n        See examples/react_streaming.py for a complete streaming example.\n\n    Example (Tools with Confirmation):\n        ```python\n        from udspy import ConfirmationRequired, ConfirmationRejected\n\n        @tool(name=\"delete_file\", require_confirmation=True)\n        def delete_file(path: str = Field(...)) -&gt; str:\n            return f\"Deleted {path}\"\n\n        react = ReAct(QA, tools=[delete_file])\n\n        try:\n            result = await react.aforward(question=\"Delete /tmp/test.txt\")\n        except ConfirmationRequired as e:\n            # User is asked for confirmation\n            print(f\"Confirm: {e.question}\")\n            # Approve: respond_to_confirmation(e.confirmation_id, approved=True)\n            # Reject: respond_to_confirmation(e.confirmation_id, approved=False, status=\"rejected\")\n            result = await react.aresume(\"yes\", e)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        signature: type[Signature] | str,\n        tools: list[Callable | Tool],\n        *,\n        max_iters: int = 10,\n        **kwargs: Any,\n    ):\n        \"\"\"Initialize ReAct module.\n\n        Args:\n            signature: Signature defining inputs and outputs, or signature string\n            tools: List of tool functions (decorated with @tool) or Tool objects\n            max_iters: Maximum number of reasoning iterations (default: 10)\n        \"\"\"\n        if isinstance(signature, str):\n            signature = Signature.from_string(signature)\n\n        self.signature = signature\n        self.user_signature = signature\n        self.max_iters = max_iters\n        self._kwargs = kwargs\n        self._context: ReactContext | None = None  # Current execution context\n\n        self.init_module(tools=tools)\n\n    def _init_tools(self) -&gt; None:\n        \"\"\"Initialize tools dictionary with user-provided tools.\"\"\"\n        tool_list = [t if isinstance(t, Tool) else Tool(t) for t in self._tools]\n        self.tools: dict[str, Tool] = {tool.name: tool for tool in tool_list if tool.name}\n        self._add_builtin_tools()\n\n    def _add_builtin_tools(self) -&gt; None:\n        \"\"\"Add built-in finish tool.\"\"\"\n        outputs = \", \".join([f\"`{k}`\" for k in self.signature.get_output_fields().keys()])\n\n        def finish_tool() -&gt; str:  # pyright: ignore[reportUnusedParameter]\n            \"\"\"Finish tool that accepts and ignores any arguments.\"\"\"\n            return \"Task completed\"\n\n        self.tools[\"finish\"] = Tool(\n            func=finish_tool,\n            name=\"finish\",\n            description=f\"Call this when you have all information needed to produce {outputs}\",\n        )\n\n    def _rebuild_signatures(self) -&gt; None:\n        \"\"\"Rebuild react and extract signatures with current tools.\n\n        This method reconstructs the signatures used by the ReAct module,\n        incorporating the current set of tools. It's called during initialization\n        and when tools are dynamically updated via init_module().\n        \"\"\"\n        self.react_signature = self._build_react_signature()\n        self.extract_signature = self._build_extract_signature()\n        self.react_module = Predict(self.react_signature, **self._kwargs)\n        self.extract_module = ChainOfThought(self.extract_signature, **self._kwargs)\n\n    def _build_react_signature(self) -&gt; type[Signature]:\n        \"\"\"Build ReAct signature with tool descriptions in instructions.\"\"\"\n        inputs = \", \".join([f\"`{k}`\" for k in self.user_signature.get_input_fields().keys()])\n        outputs = \", \".join([f\"`{k}`\" for k in self.user_signature.get_output_fields().keys()])\n\n        base_instructions = getattr(self.user_signature, \"__doc__\", \"\")\n        instr = [f\"{base_instructions}\\n\"] if base_instructions else []\n\n        instr.extend(\n            [\n                f\"You are an Agent. In each episode, you will be given the fields {inputs} as input. And you can see your past trajectory so far.\",\n                f\"Your goal is to use one or more of the supplied tools to collect any necessary information for producing {outputs}.\\n\",\n                \"To do this, you will interleave next_thought, next_tool_name, and next_tool_args in each turn, and also when finishing the task.\",\n                \"After each step, you receive a resulting observation, which gets appended to your trajectory.\\n\",\n                \"When writing next_thought, you may reason about the current situation and plan for future steps.\",\n                \"When selecting the next_tool_name and its next_tool_args, the tool must be one of:\\n\",\n            ]\n        )\n\n        instr.append(Tools(tools=list(self.tools.values())).format())\n        instr.extend(\n            [\n                \"IMPORTANT: You must respond with a JSON object in your message content containing the fields: \"\n                '{\"next_thought\": \"...\", \"next_tool_name\": \"...\", \"next_tool_args\": {...}}.',\n                \"NEVER use function calling or tool calling syntax - return the JSON as plain text in your response.\",\n            ]\n        )\n\n        react_input_fields: dict[str, type] = {\n            \"trajectory\": str,\n        }\n        for name, field_info in self.user_signature.get_input_fields().items():\n            react_input_fields[name] = field_info.annotation or str\n\n        react_output_fields: dict[str, type] = {\n            \"next_thought\": str,\n            \"next_tool_name\": Literal[*self.tools.keys()],  # type: ignore[dict-item]\n            \"next_tool_args\": dict[str, Any],\n        }\n\n        return make_signature(\n            react_input_fields,\n            react_output_fields,\n            \"\\n\".join(instr),\n        )\n\n    def _build_extract_signature(self) -&gt; type[Signature]:\n        \"\"\"Build extract signature for final answer extraction from trajectory.\"\"\"\n        extract_input_fields: dict[str, type] = {}\n        extract_output_fields: dict[str, type] = {}\n\n        for name, field_info in self.user_signature.get_input_fields().items():\n            extract_input_fields[name] = field_info.annotation or str\n\n        for name, field_info in self.user_signature.get_output_fields().items():\n            extract_output_fields[name] = field_info.annotation or str\n\n        extract_input_fields[\"trajectory\"] = str\n\n        return make_signature(\n            extract_input_fields,\n            extract_output_fields,\n            \"Extract the final answer from the trajectory\",\n        )\n\n    def init_module(self, tools: list[Any] | None = None) -&gt; None:\n        \"\"\"Initialize or reinitialize ReAct with new tools.\n\n        This method rebuilds the tools dictionary and regenerates the react signature\n        with new tool descriptions. Built-in tools are automatically preserved.\n\n        Args:\n            tools: New tools to initialize with. Can be:\n                - Functions decorated with @tool\n                - Tool instances\n                - None to clear all non-built-in tools\n\n        Example:\n            ```python from udspy import module_callback\n\n            @module_callback\n            def load_specialized_tools(context):\n                # Get current non-built-in tools\n                current_tools = [\n                    t for t in context.module.tools.values()\n                    if t.name not in builtin_tool_names\n                ]\n\n                # Add new tools\n                new_tools = [weather_tool, calendar_tool]\n\n                # Reinitialize with all tools\n                context.module.init_module(tools=current_tools + new_tools)\n\n                return f\"Added {len(new_tools)} specialized tools\"\n            ```\n        \"\"\"\n\n        self._tools = tools or []\n        self._init_tools()\n        self._rebuild_signatures()\n\n    def _format_trajectory(self, trajectory: list[Episode]) -&gt; str:\n        \"\"\"Format trajectory as a string for the LLM.\n\n        Args:\n            trajectory: List of episodes\n\n        Returns:\n            Formatted string representation\n        \"\"\"\n        if not trajectory:\n            return \"No actions taken yet.\"\n\n        lines = []\n        for step, episode in enumerate(trajectory, start=1):\n            lines.append(json.dumps({\"step\": step, **episode}))\n\n        return \"\\n\".join(lines)\n\n    async def _execute_tool_call(self, tool_name: str, tool_args: dict[str, Any]) -&gt; str:\n        \"\"\"Execute a single tool call and return observation.\n\n        Uses self._context for accessing trajectory, input_args, etc.\n\n        Args:\n            tool_name: Name of tool to execute\n            tool_args: Arguments for the tool\n\n        Returns:\n            Observation string from tool execution\n\n        Raises:\n            ConfirmationRequired: When human input is needed\n        \"\"\"\n        logger.debug(f\"Tool call - name: {tool_name}, args: {tool_args}\")\n        tool = None\n        try:\n            tool = self.tools[tool_name]\n            result = await tool.acall(**tool_args)\n\n            if is_module_callback(result):\n                # Pass module's context to callback\n                if self._context is None:\n                    raise RuntimeError(\"Module callback called outside execution context\")\n                observation = await execute_function_async(result, {\"context\": self._context})\n            else:\n                observation = str(result)\n\n            return observation\n        except ConfirmationRequired as e:\n            # Store context for resumption\n            if self._context is not None:\n                e.context = {\n                    \"trajectory\": self._context.trajectory.copy(),\n                    \"input_args\": self._context.input_args.copy(),\n                    \"stream\": self._context.stream,\n                }\n            raise\n        except Exception as e:\n            parts = [\n                f\"Traceback '{tool_name}': {format_tool_exception(e)}.\",\n            ]\n            if tool is not None:\n                parts.append(f\"Expected tool args schema: {tool.parameters}.\")\n            logger.warning(f\"Tool execution failed: {e}\")\n            return \" \".join(parts)\n\n    async def _execute_iteration(\n        self,\n        *,\n        stream: bool = False,\n    ) -&gt; bool:\n        \"\"\"\n        Execute a single ReAct iteration (create one episode).\n        Uses self._context for trajectory and input_args.\n\n        Args:\n            stream: Whether to stream sub-module execution\n\n        Returns:\n            should_stop: Whether to stop the ReAct loop\n\n        Raises:\n            ConfirmationRequired: When human input is needed\n        \"\"\"\n        # Get context from instance\n        if self._context is None:\n            raise RuntimeError(\"_execute_iteration called outside execution context\")\n\n        trajectory = self._context.trajectory\n        input_args = self._context.input_args\n\n        # Normal flow: get next thought and tool calls from LLM\n        formatted_trajectory = self._format_trajectory(trajectory)\n        pred = await self.react_module.aexecute(\n            stream=stream,\n            **input_args,\n            trajectory=formatted_trajectory,\n        )\n\n        thought = pred.get(\"next_thought\", \"\").strip()\n        tool_name = pred.get(\"next_tool_name\", None)\n        if tool_name not in self.tools:\n            raise ValueError(\n                \"Invalid tool name selected by agent. Available tools: , \".join(\n                    f\"`{name}`\" for name in self.tools.keys()\n                )\n            )\n\n        tool_args = pred.get(\"next_tool_args\", None)\n        observation = await self._execute_tool_call(tool_name, tool_args)\n\n        episode: Episode = {\n            \"thought\": thought,\n            \"tool_name\": tool_name,\n            \"tool_args\": tool_args,\n            \"observation\": observation,\n        }\n        trajectory.append(episode)\n\n        should_stop = tool_name == \"finish\"\n        return should_stop\n\n    @with_callbacks\n    async def aexecute(\n        self,\n        *,\n        stream: bool = False,\n        _trajectory: list[Episode] | None = None,\n        history: History | None = None,\n        **input_args: Any,\n    ) -&gt; Prediction:\n        \"\"\"Execute ReAct loop.\n\n        Args:\n            stream: Passed to sub-modules\n            _trajectory: Internal - restored trajectory for resumption (list of completed episodes)\n            history: History object for streaming (not used currently)\n            **input_args: Input values matching signature's input fields\n\n        Returns:\n            Prediction with trajectory and output fields\n\n        Raises:\n            ConfirmationRequired: When human input is needed\n        \"\"\"\n        max_iters = input_args.pop(\"max_iters\", self.max_iters)\n        trajectory: list[Episode] = _trajectory if _trajectory is not None else []\n        if history is None:\n            history = History()\n\n        # Set up React context for this execution\n        self._context = ReactContext(\n            module=self, trajectory=trajectory, input_args=input_args, stream=stream\n        )\n\n        try:\n            # Continue with normal iteration loop\n            while len(trajectory) &lt; max_iters:\n                try:\n                    should_stop = await self._execute_iteration(stream=stream)\n                    if should_stop:\n                        break\n\n                except ValueError as e:\n                    logger.warning(f\"Agent failed to select valid tool: {e}\")\n                    error_episode: Episode = {\n                        \"thought\": \"\",\n                        \"tool_name\": None,\n                        \"tool_args\": None,\n                        \"observation\": f\"Error: {e}\",\n                    }\n                    trajectory.append(error_episode)\n                    break\n\n            formatted_trajectory = self._format_trajectory(trajectory)\n            extract = await self.extract_module.aexecute(\n                stream=stream,\n                **input_args,\n                trajectory=formatted_trajectory,\n            )\n            result_dict = {\n                key: value\n                for key, value in extract.items()\n                if key in self.signature.get_output_fields()\n            }\n            history.add_assistant_message(json.dumps(result_dict))\n\n            prediction = Prediction(\n                **result_dict,\n                reasoning=extract[\"reasoning\"],\n                trajectory=trajectory,\n                module=self,\n            )\n            emit_event(prediction)\n            return prediction\n        finally:\n            # Clean up context\n            self._context = None\n</code></pre>"},{"location":"api/module/#udspy.module.ReAct-functions","title":"Functions","text":""},{"location":"api/module/#udspy.module.ReAct.__init__","title":"<code>__init__(signature, tools, *, max_iters=10, **kwargs)</code>","text":"<p>Initialize ReAct module.</p> <p>Parameters:</p> Name Type Description Default <code>signature</code> <code>type[Signature] | str</code> <p>Signature defining inputs and outputs, or signature string</p> required <code>tools</code> <code>list[Callable | Tool]</code> <p>List of tool functions (decorated with @tool) or Tool objects</p> required <code>max_iters</code> <code>int</code> <p>Maximum number of reasoning iterations (default: 10)</p> <code>10</code> Source code in <code>src/udspy/module/react.py</code> <pre><code>def __init__(\n    self,\n    signature: type[Signature] | str,\n    tools: list[Callable | Tool],\n    *,\n    max_iters: int = 10,\n    **kwargs: Any,\n):\n    \"\"\"Initialize ReAct module.\n\n    Args:\n        signature: Signature defining inputs and outputs, or signature string\n        tools: List of tool functions (decorated with @tool) or Tool objects\n        max_iters: Maximum number of reasoning iterations (default: 10)\n    \"\"\"\n    if isinstance(signature, str):\n        signature = Signature.from_string(signature)\n\n    self.signature = signature\n    self.user_signature = signature\n    self.max_iters = max_iters\n    self._kwargs = kwargs\n    self._context: ReactContext | None = None  # Current execution context\n\n    self.init_module(tools=tools)\n</code></pre>"},{"location":"api/module/#udspy.module.ReAct.aexecute","title":"<code>aexecute(*, stream=False, _trajectory=None, history=None, **input_args)</code>  <code>async</code>","text":"<p>Execute ReAct loop.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>bool</code> <p>Passed to sub-modules</p> <code>False</code> <code>_trajectory</code> <code>list[Episode] | None</code> <p>Internal - restored trajectory for resumption (list of completed episodes)</p> <code>None</code> <code>history</code> <code>History | None</code> <p>History object for streaming (not used currently)</p> <code>None</code> <code>**input_args</code> <code>Any</code> <p>Input values matching signature's input fields</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>Prediction with trajectory and output fields</p> <p>Raises:</p> Type Description <code>ConfirmationRequired</code> <p>When human input is needed</p> Source code in <code>src/udspy/module/react.py</code> <pre><code>@with_callbacks\nasync def aexecute(\n    self,\n    *,\n    stream: bool = False,\n    _trajectory: list[Episode] | None = None,\n    history: History | None = None,\n    **input_args: Any,\n) -&gt; Prediction:\n    \"\"\"Execute ReAct loop.\n\n    Args:\n        stream: Passed to sub-modules\n        _trajectory: Internal - restored trajectory for resumption (list of completed episodes)\n        history: History object for streaming (not used currently)\n        **input_args: Input values matching signature's input fields\n\n    Returns:\n        Prediction with trajectory and output fields\n\n    Raises:\n        ConfirmationRequired: When human input is needed\n    \"\"\"\n    max_iters = input_args.pop(\"max_iters\", self.max_iters)\n    trajectory: list[Episode] = _trajectory if _trajectory is not None else []\n    if history is None:\n        history = History()\n\n    # Set up React context for this execution\n    self._context = ReactContext(\n        module=self, trajectory=trajectory, input_args=input_args, stream=stream\n    )\n\n    try:\n        # Continue with normal iteration loop\n        while len(trajectory) &lt; max_iters:\n            try:\n                should_stop = await self._execute_iteration(stream=stream)\n                if should_stop:\n                    break\n\n            except ValueError as e:\n                logger.warning(f\"Agent failed to select valid tool: {e}\")\n                error_episode: Episode = {\n                    \"thought\": \"\",\n                    \"tool_name\": None,\n                    \"tool_args\": None,\n                    \"observation\": f\"Error: {e}\",\n                }\n                trajectory.append(error_episode)\n                break\n\n        formatted_trajectory = self._format_trajectory(trajectory)\n        extract = await self.extract_module.aexecute(\n            stream=stream,\n            **input_args,\n            trajectory=formatted_trajectory,\n        )\n        result_dict = {\n            key: value\n            for key, value in extract.items()\n            if key in self.signature.get_output_fields()\n        }\n        history.add_assistant_message(json.dumps(result_dict))\n\n        prediction = Prediction(\n            **result_dict,\n            reasoning=extract[\"reasoning\"],\n            trajectory=trajectory,\n            module=self,\n        )\n        emit_event(prediction)\n        return prediction\n    finally:\n        # Clean up context\n        self._context = None\n</code></pre>"},{"location":"api/module/#udspy.module.ReAct.init_module","title":"<code>init_module(tools=None)</code>","text":"<p>Initialize or reinitialize ReAct with new tools.</p> <p>This method rebuilds the tools dictionary and regenerates the react signature with new tool descriptions. Built-in tools are automatically preserved.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>list[Any] | None</code> <p>New tools to initialize with. Can be: - Functions decorated with @tool - Tool instances - None to clear all non-built-in tools</p> <code>None</code> Example <p>```python from udspy import module_callback</p> <p>@module_callback def load_specialized_tools(context):     # Get current non-built-in tools     current_tools = [         t for t in context.module.tools.values()         if t.name not in builtin_tool_names     ]</p> <pre><code># Add new tools\nnew_tools = [weather_tool, calendar_tool]\n\n# Reinitialize with all tools\ncontext.module.init_module(tools=current_tools + new_tools)\n\nreturn f\"Added {len(new_tools)} specialized tools\"\n</code></pre> <p>```</p> Source code in <code>src/udspy/module/react.py</code> <pre><code>def init_module(self, tools: list[Any] | None = None) -&gt; None:\n    \"\"\"Initialize or reinitialize ReAct with new tools.\n\n    This method rebuilds the tools dictionary and regenerates the react signature\n    with new tool descriptions. Built-in tools are automatically preserved.\n\n    Args:\n        tools: New tools to initialize with. Can be:\n            - Functions decorated with @tool\n            - Tool instances\n            - None to clear all non-built-in tools\n\n    Example:\n        ```python from udspy import module_callback\n\n        @module_callback\n        def load_specialized_tools(context):\n            # Get current non-built-in tools\n            current_tools = [\n                t for t in context.module.tools.values()\n                if t.name not in builtin_tool_names\n            ]\n\n            # Add new tools\n            new_tools = [weather_tool, calendar_tool]\n\n            # Reinitialize with all tools\n            context.module.init_module(tools=current_tools + new_tools)\n\n            return f\"Added {len(new_tools)} specialized tools\"\n        ```\n    \"\"\"\n\n    self._tools = tools or []\n    self._init_tools()\n    self._rebuild_signatures()\n</code></pre>"},{"location":"api/module/#udspy.module-functions","title":"Functions","text":""},{"location":"api/module/#udspy.module.is_module_callback","title":"<code>is_module_callback(obj)</code>","text":"<p>Check if an object is a module callback.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Object to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if obj is a ModuleCallback instance</p> Source code in <code>src/udspy/module/callbacks.py</code> <pre><code>def is_module_callback(obj: Any) -&gt; bool:\n    \"\"\"Check if an object is a module callback.\n\n    Args:\n        obj: Object to check\n\n    Returns:\n        True if obj is a ModuleCallback instance\n    \"\"\"\n    return isinstance(obj, ModuleCallback)\n</code></pre>"},{"location":"api/module_callback/","title":"Module Callbacks API","text":"<p>API reference for module callbacks - the mechanism that enables dynamic tool management during execution.</p>"},{"location":"api/module_callback/#overview","title":"Overview","text":"<p>Module callbacks are special callables that tools can return to modify the module during execution. They receive context about the current execution state and can add/remove tools, modify configuration, or perform other operations on the module.</p>"},{"location":"api/module_callback/#module_callback-decorator","title":"<code>@module_callback</code> Decorator","text":"<pre><code>@module_callback\ndef callback(context: ModuleContext) -&gt; str:\n    # Modify module\n    context.module.init_module(tools=[...])\n    return \"Observation string\"\n</code></pre> <p>Decorator that marks a function as a module callback. The decorated function: - Receives a <code>ModuleContext</code> (or subclass) as its only parameter - Must return a string that becomes the observation in the trajectory - Can modify the module by calling <code>context.module.init_module()</code></p> <p>Parameters:</p> <ul> <li><code>func</code> (<code>Callable[[ModuleContext], str]</code>): Function that takes context and returns observation string</li> </ul> <p>Returns:</p> <ul> <li><code>ModuleCallback</code>: Wrapped callback object</li> </ul> <p>Example:</p> <pre><code>from udspy import module_callback\n\n@module_callback\ndef add_tools(context):\n    # Access the module\n    current_tools = list(context.module.tools.values())\n\n    # Add new tools\n    context.module.init_module(tools=current_tools + [new_tool])\n\n    # Return observation\n    return \"Added new tool\"\n</code></pre>"},{"location":"api/module_callback/#modulecallback-class","title":"<code>ModuleCallback</code> Class","text":"<pre><code>class ModuleCallback:\n    \"\"\"Wrapper for module callback functions.\"\"\"\n\n    func: Callable[[ModuleContext], str]\n\n    def __call__(self, context: ModuleContext) -&gt; str: ...\n</code></pre> <p>The <code>ModuleCallback</code> class wraps a callback function. You typically don't instantiate this directly; use the <code>@module_callback</code> decorator instead.</p>"},{"location":"api/module_callback/#attributes","title":"Attributes","text":""},{"location":"api/module_callback/#func","title":"<code>func</code>","text":"<pre><code>func: Callable[[ModuleContext], str]\n</code></pre> <p>The underlying callback function.</p>"},{"location":"api/module_callback/#methods","title":"Methods","text":""},{"location":"api/module_callback/#__call__context-modulecontext-str","title":"<code>__call__(context: ModuleContext) -&gt; str</code>","text":"<p>Execute the callback with the provided context.</p> <p>Parameters:</p> <ul> <li><code>context</code> (<code>ModuleContext</code>): Execution context with module and state</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Observation string to add to trajectory/history</li> </ul>"},{"location":"api/module_callback/#context-classes","title":"Context Classes","text":""},{"location":"api/module_callback/#modulecontext","title":"<code>ModuleContext</code>","text":"<pre><code>class ModuleContext:\n    \"\"\"Base context for module callbacks.\"\"\"\n\n    module: Module\n</code></pre> <p>Base context class that provides access to the module instance.</p> <p>Attributes:</p> <ul> <li><code>module</code> (<code>Module</code>): The module instance (Predict or ReAct)</li> </ul> <p>Example:</p> <pre><code>@module_callback\ndef callback(context: ModuleContext):\n    # Access module\n    module = context.module\n\n    # Access current tools\n    tools = context.module.tools\n\n    # Modify module\n    context.module.init_module(tools=[...])\n\n    return \"Modified module\"\n</code></pre>"},{"location":"api/module_callback/#predictcontext","title":"<code>PredictContext</code>","text":"<pre><code>class PredictContext(ModuleContext):\n    \"\"\"Context for Predict module callbacks.\"\"\"\n\n    module: Predict\n    history: Optional[History]\n</code></pre> <p>Context for callbacks in Predict modules. Provides access to conversation history in addition to the module.</p> <p>Attributes:</p> <ul> <li><code>module</code> (<code>Predict</code>): The Predict module instance</li> <li><code>history</code> (<code>Optional[History]</code>): Conversation history (if provided)</li> </ul> <p>Example:</p> <pre><code>from udspy.module.callbacks import PredictContext\n\n@module_callback\ndef callback(context: PredictContext):\n    # Access Predict-specific features\n    module = context.module  # Type: Predict\n\n    # Access conversation history\n    if context.history:\n        messages = context.history.messages\n        print(f\"Conversation has {len(messages)} messages\")\n\n    # Modify tools\n    context.module.init_module(tools=[...])\n\n    return \"Updated tools based on conversation history\"\n</code></pre>"},{"location":"api/module_callback/#reactcontext","title":"<code>ReactContext</code>","text":"<pre><code>class ReactContext(ModuleContext):\n    \"\"\"Context for ReAct module callbacks.\"\"\"\n\n    module: ReAct\n    trajectory: dict[str, Any]\n</code></pre> <p>Context for callbacks in ReAct modules. Provides access to the agent's trajectory in addition to the module.</p> <p>Attributes:</p> <ul> <li><code>module</code> (<code>ReAct</code>): The ReAct module instance</li> <li><code>trajectory</code> (<code>dict[str, Any]</code>): Current trajectory with thought/action/observation history</li> </ul> <p>Example:</p> <pre><code>from udspy.module.callbacks import ReactContext\n\n@module_callback\ndef callback(context: ReactContext):\n    # Access ReAct-specific features\n    module = context.module  # Type: ReAct\n\n    # Access trajectory\n    trajectory = context.trajectory\n\n    # Analyze agent's thoughts\n    thoughts = [v for k, v in trajectory.items() if k.startswith(\"thought_\")]\n    print(f\"Agent has had {len(thoughts)} thoughts\")\n\n    # Modify tools based on trajectory\n    if len(thoughts) &gt; 3:\n        # Agent is struggling, add more tools\n        context.module.init_module(tools=[...])\n\n    return \"Adapted tools based on trajectory\"\n</code></pre> <p>Trajectory Structure:</p> <p>The trajectory dictionary contains entries like:</p> <pre><code>{\n    \"thought_0\": \"I need to search for information\",\n    \"tool_calls_0\": [{\"name\": \"search\", \"args\": {...}}],\n    \"observation_0\": \"Search results: ...\",\n    \"thought_1\": \"Now I need to calculate\",\n    \"tool_calls_1\": [{\"name\": \"calculator\", \"args\": {...}}],\n    \"observation_1\": \"Result: 42\",\n    # ...\n}\n</code></pre>"},{"location":"api/module_callback/#helper-functions","title":"Helper Functions","text":""},{"location":"api/module_callback/#is_module_callbackobj-any-bool","title":"<code>is_module_callback(obj: Any) -&gt; bool</code>","text":"<pre><code>def is_module_callback(obj: Any) -&gt; bool:\n    \"\"\"Check if an object is a module callback.\"\"\"\n</code></pre> <p>Check if an object is a <code>ModuleCallback</code> instance.</p> <p>Parameters:</p> <ul> <li><code>obj</code> (<code>Any</code>): Object to check</li> </ul> <p>Returns:</p> <ul> <li><code>bool</code>: True if obj is a ModuleCallback instance</li> </ul> <p>Example:</p> <pre><code>from udspy import module_callback, is_module_callback\n\n@module_callback\ndef my_callback(context):\n    return \"Done\"\n\ndef regular_function():\n    return \"Done\"\n\nprint(is_module_callback(my_callback))      # True\nprint(is_module_callback(regular_function))  # False\nprint(is_module_callback(\"not a function\"))  # False\n</code></pre>"},{"location":"api/module_callback/#usage-with-tools","title":"Usage with Tools","text":"<p>Module callbacks are typically returned by tools to enable dynamic tool loading:</p> <pre><code>from udspy import tool, module_callback\nfrom pydantic import Field\n\n@tool(name=\"load_calculator\", description=\"Load calculator tool\")\ndef load_calculator() -&gt; callable:\n    \"\"\"Tool that returns a module callback.\"\"\"\n\n    @module_callback\n    def add_calculator(context):\n        # Get current tools\n        current = [\n            t for t in context.module.tools.values()\n            if t.name not in (\"finish\", \"user_clarification\")\n        ]\n\n        # Add calculator\n        from my_tools import calculator\n        context.module.init_module(tools=current + [calculator])\n\n        return \"Calculator loaded and ready to use\"\n\n    return add_calculator\n</code></pre> <p>Execution Flow:</p> <ol> <li>LLM calls <code>load_calculator()</code> tool</li> <li>Tool returns a <code>ModuleCallback</code> instance</li> <li>Module detects the callback with <code>is_module_callback()</code></li> <li>Module creates appropriate context (<code>PredictContext</code> or <code>ReactContext</code>)</li> <li>Module executes callback: <code>callback(context)</code></li> <li>Callback modifies module via <code>context.module.init_module()</code></li> <li>Callback returns observation string</li> <li>Observation is added to history/trajectory</li> <li>Execution continues with modified tool set</li> </ol>"},{"location":"api/module_callback/#common-patterns","title":"Common Patterns","text":""},{"location":"api/module_callback/#loading-tools-on-demand","title":"Loading Tools on Demand","text":"<pre><code>@tool(name=\"load_tools\", description=\"Load specialized tools\")\ndef load_tools(category: str = Field(...)) -&gt; callable:\n    @module_callback\n    def add_tools(context):\n        current = list(context.module.tools.values())\n\n        if category == \"math\":\n            new_tools = [calculator, statistics]\n        elif category == \"web\":\n            new_tools = [search, scrape]\n\n        context.module.init_module(tools=current + new_tools)\n        return f\"Loaded {len(new_tools)} {category} tools\"\n\n    return add_tools\n</code></pre>"},{"location":"api/module_callback/#conditional-loading-based-on-history","title":"Conditional Loading Based on History","text":"<pre><code>@tool(name=\"smart_load\", description=\"Intelligently load tools\")\ndef smart_load() -&gt; callable:\n    @module_callback\n    def analyze_and_load(context: PredictContext):\n        # Analyze conversation history\n        if context.history:\n            messages = context.history.messages\n            # Determine what tools are needed\n            # ...\n\n        context.module.init_module(tools=[...])\n        return \"Loaded appropriate tools\"\n\n    return analyze_and_load\n</code></pre>"},{"location":"api/module_callback/#progressive-tool-discovery","title":"Progressive Tool Discovery","text":"<pre><code>@tool(name=\"discover_tools\", description=\"Discover needed tools\")\ndef discover_tools(task: str = Field(...)) -&gt; callable:\n    @module_callback\n    def discover(context: ReactContext):\n        # Analyze trajectory to see what agent has tried\n        thoughts = [v for k, v in context.trajectory.items()\n                   if k.startswith(\"thought_\")]\n\n        # Load tools the agent seems to need\n        # ...\n\n        context.module.init_module(tools=[...])\n        return \"Discovered and loaded needed tools\"\n\n    return discover\n</code></pre>"},{"location":"api/module_callback/#type-annotations","title":"Type Annotations","text":"<pre><code>from typing import Callable, Any\nfrom udspy import Module, History\n\n# Decorator\ndef module_callback(\n    func: Callable[[ModuleContext], str]\n) -&gt; ModuleCallback: ...\n\n# Callback class\nclass ModuleCallback:\n    func: Callable[[ModuleContext], str]\n    def __call__(self, context: ModuleContext) -&gt; str: ...\n\n# Context classes\nclass ModuleContext:\n    module: Module\n\nclass PredictContext(ModuleContext):\n    module: Predict\n    history: Optional[History]\n\nclass ReactContext(ModuleContext):\n    module: ReAct\n    trajectory: dict[str, Any]\n\n# Helper\ndef is_module_callback(obj: Any) -&gt; bool: ...\n</code></pre>"},{"location":"api/module_callback/#important-notes","title":"Important Notes","text":"<ol> <li> <p>Return String Required: Callbacks MUST return a string - this becomes the observation</p> </li> <li> <p>Thread Safety: Callbacks execute synchronously during tool execution</p> </li> <li> <p>Built-in Preservation: ReAct's <code>finish</code> and user clarification tools are automatically preserved</p> </li> <li> <p>Tool Persistence: Tools added via callbacks remain available for the entire execution</p> </li> <li> <p>Error Handling: Wrap callback logic in try/except to handle errors gracefully:    <pre><code>@module_callback\ndef safe_callback(context):\n    try:\n        context.module.init_module(tools=[...])\n        return \"Success\"\n    except Exception as e:\n        return f\"Failed: {e}\"\n</code></pre></p> </li> </ol>"},{"location":"api/module_callback/#see-also","title":"See Also","text":"<ul> <li>Dynamic Tools Guide - Complete guide with examples</li> <li>Tool API - Tool creation and usage</li> <li>ReAct API - ReAct module documentation</li> <li>Predict API - Predict module documentation</li> </ul> <p>Example Code: See <code>examples/dynamic_calculator.py</code> and <code>examples/dynamic_tools.py</code> in the GitHub repository</p>"},{"location":"api/react/","title":"ReAct API Reference","text":"<p>API documentation for the ReAct (Reasoning and Acting) module.</p>"},{"location":"api/react/#module-udspymodulereact","title":"Module: <code>udspy.module.react</code>","text":""},{"location":"api/react/#react","title":"<code>ReAct</code>","text":"<pre><code>class ReAct(Module):\n    \"\"\"ReAct (Reasoning and Acting) module for tool-using agents.\"\"\"\n</code></pre> <p>Agent module that iteratively reasons about the current situation and decides whether to call a tool or finish the task.</p>"},{"location":"api/react/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    signature: type[Signature] | str,\n    tools: list[Callable | Tool],\n    *,\n    max_iters: int = 10,\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>signature</code> (<code>type[Signature] | str</code>): Task signature defining inputs and outputs</li> <li>Can be a <code>Signature</code> class or a string like <code>\"input1, input2 -&gt; output1, output2\"</code></li> <li><code>tools</code> (<code>list[Callable | Tool]</code>): List of tool functions or <code>Tool</code> objects</li> <li>Tools can be decorated functions (<code>@tool</code>) or <code>Tool</code> instances</li> <li><code>max_iters</code> (<code>int</code>, default: <code>10</code>): Maximum number of reasoning iterations</li> <li>Agent will stop after this many steps even if not finished</li> </ul> <p>Example:</p> <pre><code>from udspy import ReAct, Signature, InputField, OutputField, tool\nfrom pydantic import Field\n\n@tool(name=\"search\", description=\"Search for information\")\ndef search(query: str = Field(...)) -&gt; str:\n    return f\"Results for: {query}\"\n\nclass QA(Signature):\n    \"\"\"Answer questions using available tools.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\nagent = ReAct(QA, tools=[search], max_iters=10)\n</code></pre>"},{"location":"api/react/#methods","title":"Methods","text":""},{"location":"api/react/#forwardinput_args-prediction","title":"<code>forward(**input_args) -&gt; Prediction</code>","text":"<p>Synchronous forward pass through the ReAct loop.</p> <p>Parameters:</p> <ul> <li><code>**input_args</code>: Input values matching the signature's input fields</li> <li>Keys must match input field names</li> <li>Can include <code>max_iters</code> to override default</li> </ul> <p>Returns:</p> <ul> <li><code>Prediction</code>: Contains output fields and trajectory</li> <li>Trajectory tracks all reasoning steps</li> <li>Output fields match the signature</li> </ul> <p>Raises:</p> <ul> <li><code>ConfirmationRequired</code>: When user input is needed</li> <li>Raised when user clarification is called</li> <li>Raised when tool requires confirmation</li> <li>Contains saved state for resumption</li> </ul> <p>Example:</p> <pre><code>result = agent(question=\"What is Python?\")\nprint(result.answer)\nprint(result.trajectory)  # All reasoning steps\n</code></pre>"},{"location":"api/react/#aforwardinput_args-prediction","title":"<code>aforward(**input_args) -&gt; Prediction</code>","text":"<p>Async forward pass through the ReAct loop.</p> <p>Parameters:</p> <ul> <li>Same as <code>forward()</code></li> </ul> <p>Returns:</p> <ul> <li><code>Prediction</code>: Same as <code>forward()</code></li> </ul> <p>Raises:</p> <ul> <li>Same as <code>forward()</code></li> </ul> <p>Example:</p> <pre><code>import asyncio\n\nasync def main():\n    result = await agent.aforward(question=\"What is Python?\")\n    print(result.answer)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/react/#resumeuser_response-saved_state-prediction","title":"<code>resume(user_response, saved_state) -&gt; Prediction</code>","text":"<p>\u26a0\ufe0f Not Yet Implemented: This method is planned but not yet available in ReAct. Use <code>respond_to_confirmation()</code> with <code>forward()</code> instead.</p> <p>Resume execution after user provides input (synchronous).</p> <p>Parameters:</p> <ul> <li><code>user_response</code> (<code>str</code>): The user's response to the question</li> <li><code>\"yes\"</code> or <code>\"y\"</code>: Approve the pending tool call</li> <li><code>\"no\"</code> or <code>\"n\"</code>: Reject and let agent decide next action</li> <li>Free text: Treated as feedback for the agent</li> <li>JSON with <code>\"edit\"</code> key: Modify tool name/args (e.g., <code>{\"edit\": {\"name\": \"new_tool\", \"args\": {...}}}</code>)</li> <li><code>saved_state</code> (<code>ConfirmationRequired</code>): The exception that was raised</li> <li>Contains <code>context</code> dict with: <code>trajectory</code>, <code>iteration</code>, <code>input_args</code></li> <li>Contains <code>tool_call</code> with pending tool information</li> <li>Contains <code>confirmation_id</code> for tracking</li> </ul> <p>Returns:</p> <ul> <li><code>Prediction</code>: Final result after resuming execution</li> </ul> <p>Example:</p> <pre><code>from udspy import ConfirmationRequired\n\ntry:\n    result = agent(question=\"Delete my files\")\nexcept ConfirmationRequired as e:\n    print(f\"Agent asks: {e.question}\")\n    if e.tool_call:\n        print(f\"Tool: {e.tool_call.name}\")\n        print(f\"Args: {e.tool_call.args}\")\n    response = input(\"Your response (yes/no/feedback): \")\n    result = agent.resume(response, e)\n</code></pre>"},{"location":"api/react/#aresumeuser_response-saved_state-prediction","title":"<code>aresume(user_response, saved_state) -&gt; Prediction</code>","text":"<p>\u26a0\ufe0f Not Yet Implemented: This method is planned but not yet available in ReAct. Use <code>respond_to_confirmation()</code> with <code>aforward()</code> instead.</p> <p>Resume execution after user provides input (async).</p> <p>Parameters:</p> <ul> <li>Same as <code>resume()</code></li> </ul> <p>Returns:</p> <ul> <li>Same as <code>resume()</code></li> </ul> <p>Example:</p> <pre><code>try:\n    result = await agent.aforward(question=\"Delete my files\")\nexcept ConfirmationRequired as e:\n    print(f\"Agent asks: {e.question}\")\n    response = get_user_input(e.question)\n    result = await agent.aresume(response, e)\n</code></pre>"},{"location":"api/react/#properties","title":"Properties","text":""},{"location":"api/react/#signature","title":"<code>signature</code>","text":"<pre><code>signature: type[Signature]\n</code></pre> <p>The task signature used by this agent.</p>"},{"location":"api/react/#tools","title":"<code>tools</code>","text":"<pre><code>tools: dict[str, Tool]\n</code></pre> <p>Dictionary mapping tool names to Tool objects. Includes: - User-provided tools - Built-in <code>finish</code> tool</p>"},{"location":"api/react/#react_signature","title":"<code>react_signature</code>","text":"<pre><code>react_signature: type[Signature]\n</code></pre> <p>Internal signature for the reasoning loop. With native tool calling: - Outputs <code>reasoning</code>: The agent's reasoning about what to do next - Tools are called natively via OpenAI's tool calling API - The LLM both produces reasoning text and selects tools simultaneously</p>"},{"location":"api/react/#extract_signature","title":"<code>extract_signature</code>","text":"<pre><code>extract_signature: type[Signature]\n</code></pre> <p>Internal signature for extracting the final answer from the trajectory.</p>"},{"location":"api/react/#confirmationrequired","title":"<code>ConfirmationRequired</code>","text":"<pre><code>class ConfirmationRequired(Exception):\n    \"\"\"Raised when human input is needed to proceed.\"\"\"\n</code></pre> <p>Note: This exception has been moved to the <code>confirmation</code> module. See the Confirmation API for full documentation.</p> <p>Exception that pauses ReAct execution and saves state for resumption. This exception can be raised by: - The user clarification tool when the agent needs clarification - Tools with <code>require_confirmation=True</code> before execution - Custom tools that need human input</p>"},{"location":"api/react/#constructor_1","title":"Constructor","text":"<pre><code>from udspy.confirmation import ConfirmationRequired, ToolCall\n\ndef __init__(\n    self,\n    question: str,\n    *,\n    confirmation_id: str | None = None,\n    tool_call: ToolCall | None = None,\n    context: dict[str, Any] | None = None,\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>question</code> (<code>str</code>): Question to ask the user</li> <li><code>confirmation_id</code> (<code>str | None</code>): Unique confirmation ID (auto-generated if not provided)</li> <li><code>tool_call</code> (<code>ToolCall | None</code>): Optional tool call information</li> <li>Has attributes: <code>.name</code> (tool name), <code>.args</code> (arguments dict), <code>.call_id</code> (optional ID)</li> <li><code>context</code> (<code>dict[str, Any] | None</code>): Module-specific state dictionary</li> <li>ReAct stores: <code>trajectory</code>, <code>iteration</code>, <code>input_args</code> here</li> </ul>"},{"location":"api/react/#attributes","title":"Attributes","text":""},{"location":"api/react/#question","title":"<code>question</code>","text":"<pre><code>question: str\n</code></pre> <p>The question being asked to the user.</p> <p>Example:</p> <pre><code>try:\n    result = agent(question=\"Delete files\")\nexcept ConfirmationRequired as e:\n    print(e.question)  # \"Confirm execution of delete_file...?\"\n</code></pre>"},{"location":"api/react/#confirmation_id","title":"<code>confirmation_id</code>","text":"<pre><code>confirmation_id: str\n</code></pre> <p>Unique identifier for this confirmation. Used with <code>get_confirmation_status()</code> to check approval status.</p> <p>Example:</p> <pre><code>from udspy import get_confirmation_status\n\ntry:\n    result = agent(question=\"Delete files\")\nexcept ConfirmationRequired as e:\n    print(e.confirmation_id)  # \"abc-123-def-456\"\n    status = get_confirmation_status(e.confirmation_id)\n    print(status)  # \"pending\"\n</code></pre>"},{"location":"api/react/#tool_call","title":"<code>tool_call</code>","text":"<pre><code>tool_call: ToolCall | None\n</code></pre> <p>Information about the tool call that triggered this confirmation (if applicable).</p> <p>Attributes: - <code>name</code> (<code>str</code>): Tool name - <code>args</code> (<code>dict[str, Any]</code>): Tool arguments - <code>call_id</code> (<code>str | None</code>): Optional call ID from OpenAI</p> <p>Example:</p> <pre><code>try:\n    result = agent(question=\"Delete my files\")\nexcept ConfirmationRequired as e:\n    if e.tool_call:\n        print(f\"Tool: {e.tool_call.name}\")  # \"delete_file\"\n        print(f\"Args: {e.tool_call.args}\")  # {\"path\": \"/tmp/test.txt\"}\n        print(f\"Call ID: {e.tool_call.call_id}\")  # \"call_abc123\"\n</code></pre>"},{"location":"api/react/#context","title":"<code>context</code>","text":"<pre><code>context: dict[str, Any]\n</code></pre> <p>Module-specific state dictionary. For ReAct agents, contains: - <code>trajectory</code> (<code>dict[str, Any]</code>): Current execution trajectory with reasoning, tool calls, and observations - <code>iteration</code> (<code>int</code>): Current iteration number (0-indexed) - <code>input_args</code> (<code>dict[str, Any]</code>): Original input arguments</p> <p>Example:</p> <pre><code>try:\n    result = agent(question=\"What is 2+2?\")\nexcept ConfirmationRequired as e:\n    # Access ReAct-specific state\n    trajectory = e.context[\"trajectory\"]\n    print(trajectory)\n    # {\n    #   'reasoning_0': 'I should use the calculator',\n    #   'tool_name_0': 'calculator',\n    #   'tool_args_0': {'expression': '2+2'},\n    #   'observation_0': '4'\n    # }\n\n    iteration = e.context[\"iteration\"]\n    print(f\"Paused at step {iteration + 1}\")\n\n    input_args = e.context[\"input_args\"]\n    print(f\"Original question: {input_args['question']}\")\n</code></pre>"},{"location":"api/react/#built-in-tools","title":"Built-in Tools","text":"<p>Every ReAct agent automatically includes these tools:</p>"},{"location":"api/react/#finish","title":"<code>finish</code>","text":"<p>Tool that signals task completion.</p> <p>Name: <code>finish</code></p> <p>Description: \"Call this when you have all information needed to produce {outputs}\"</p> <p>Arguments: None</p> <p>Usage:</p> <p>The agent automatically selects this tool when it has enough information to answer. You don't call it directly.</p>"},{"location":"api/react/#trajectory-format","title":"Trajectory Format","text":"<p>The trajectory is a dictionary with the following keys:</p> <pre><code>{\n    \"reasoning_0\": str,    # Agent's reasoning for step 0\n    \"tool_name_0\": str,    # Tool name selected for step 0\n    \"tool_args_0\": dict,   # Arguments for step 0\n    \"observation_0\": str,  # Tool result for step 0\n\n    \"reasoning_1\": str,    # Agent's reasoning for step 1\n    \"tool_name_1\": str,    # Tool name selected for step 1\n    \"tool_args_1\": dict,   # Arguments for step 1\n    \"observation_1\": str,  # Tool result for step 1\n\n    # ... continues for all iterations\n}\n</code></pre> <p>Example:</p> <pre><code>result = agent(question=\"Calculate 2+2\")\n\n# Access trajectory\nprint(result.trajectory)\n# {\n#     'reasoning_0': 'I need to calculate 2+2',\n#     'tool_name_0': 'calculator',\n#     'tool_args_0': {'expression': '2+2'},\n#     'observation_0': '4',\n#     'reasoning_1': 'I have the answer',\n#     'tool_name_1': 'finish',\n#     'tool_args_1': {},\n#     'observation_1': 'Task completed'\n# }\n\n# Iterate through steps\ni = 0\nwhile f\"observation_{i}\" in result.trajectory:\n    print(f\"Step {i}:\")\n    print(f\"  Reasoning: {result.trajectory.get(f'reasoning_{i}', '')}\")\n    print(f\"  Tool: {result.trajectory[f'tool_name_{i}']}\")\n    print(f\"  Args: {result.trajectory[f'tool_args_{i}']}\")\n    print(f\"  Result: {result.trajectory[f'observation_{i}']}\")\n    i += 1\n</code></pre>"},{"location":"api/react/#string-signature-format","title":"String Signature Format","text":"<p>For quick prototyping, you can use string signatures:</p> <p>Format: <code>\"input1, input2 -&gt; output1, output2\"</code></p> <p>Examples:</p> <pre><code># Single input, single output\nagent = ReAct(\"query -&gt; result\", tools=[search])\n\n# Multiple inputs\nagent = ReAct(\"context, question -&gt; answer\", tools=[search])\n\n# Multiple outputs\nagent = ReAct(\"topic -&gt; summary, sources\", tools=[search])\n</code></pre> <p>The string signature is parsed into: - Input fields: All fields before <code>-&gt;</code> (type: <code>str</code>) - Output fields: All fields after <code>-&gt;</code> (type: <code>str</code>)</p>"},{"location":"api/react/#tool-confirmation","title":"Tool Confirmation","text":"<p>Tools can require user confirmation before execution using the <code>require_confirmation</code> parameter:</p> <pre><code>from udspy import tool\nfrom pydantic import Field\n\n@tool(\n    name=\"delete_file\",\n    description=\"Delete a file\",\n    require_confirmation=True  # Require confirmation before execution\n)\ndef delete_file(path: str = Field(...)) -&gt; str:\n    os.remove(path)\n    return f\"Deleted {path}\"\n</code></pre> <p>When the agent tries to call this tool, it raises <code>ConfirmationRequired</code> with a confirmation question on the first call. After the user approves, the tool executes normally.</p> <p>Confirmation Message Format:</p> <pre><code>\"Confirm execution of {tool_name} with args: {args}? (yes/no)\"\n</code></pre> <p>Response Options: - <code>\"yes\"</code> or <code>\"y\"</code>: Approve and execute the tool - <code>\"no\"</code> or <code>\"n\"</code>: Reject and let agent choose a different action - JSON with <code>\"edit\"</code>: Modify tool arguments before execution</p> <p>Example:</p> <pre><code>try:\n    result = agent(question=\"Delete all temporary files\")\nexcept ConfirmationRequired as e:\n    print(e.question)  # \"Confirm execution of delete_file...\"\n    # User approves\n    result = agent.resume(\"yes\", e)\n</code></pre>"},{"location":"api/react/#error-handling","title":"Error Handling","text":""},{"location":"api/react/#tool-execution-errors","title":"Tool Execution Errors","text":"<p>If a tool raises an exception, the error is captured as an observation:</p> <pre><code>@tool(name=\"api_call\", description=\"Call API\")\ndef api_call(endpoint: str = Field(...)) -&gt; str:\n    if endpoint == \"invalid\":\n        raise ValueError(\"Invalid endpoint\")\n    return \"Success\"\n\n# Agent will see observation:\n# \"Error executing api_call: Invalid endpoint\"\n</code></pre> <p>The agent can then: 1. Try a different tool 2. Retry with different arguments 3. Ask the user for help (using the user clarification tool)</p>"},{"location":"api/react/#maximum-iterations","title":"Maximum Iterations","text":"<p>If the agent reaches <code>max_iters</code>, it stops and extracts an answer from the current trajectory:</p> <pre><code>agent = ReAct(signature, tools=tools, max_iters=5)\nresult = agent(question=\"Complex task\")\n# Will stop after 5 iterations even if not finished\n</code></pre>"},{"location":"api/react/#type-annotations","title":"Type Annotations","text":"<pre><code>from typing import Callable, Any\nfrom udspy import ReAct, Signature, Tool, Prediction, ConfirmationRequired\n\n# Constructor types\nsignature: type[Signature] | str\ntools: list[Callable | Tool]\nmax_iters: int\nenable_user_clarification: bool\n\n# Method types\ndef forward(**input_args: Any) -&gt; Prediction: ...\nasync def aforward(**input_args: Any) -&gt; Prediction: ...\ndef resume(\n    user_response: str,\n    saved_state: ConfirmationRequired\n) -&gt; Prediction: ...\nasync def aresume(\n    user_response: str,\n    saved_state: ConfirmationRequired\n) -&gt; Prediction: ...\n</code></pre>"},{"location":"api/react/#see-also","title":"See Also","text":"<ul> <li>ReAct Examples - Usage guide and examples</li> <li>Confirmation API - Confirmation system and <code>ConfirmationRequired</code> documentation</li> <li>Tool API - Creating and configuring tools</li> <li>Module API - Base module documentation</li> <li>Signature API - Signature documentation</li> </ul>"},{"location":"api/settings/","title":"API Reference: Settings","text":""},{"location":"api/settings/#udspy.settings","title":"<code>udspy.settings</code>","text":"<p>Global settings and configuration.</p>"},{"location":"api/settings/#udspy.settings-classes","title":"Classes","text":""},{"location":"api/settings/#udspy.settings.Settings","title":"<code>Settings</code>","text":"<p>Global settings for udspy.</p> <p>udspy uses a single LM (Language Model) instance to handle all provider interactions. Create an LM using the factory function and configure it globally or per-context.</p> Source code in <code>src/udspy/settings.py</code> <pre><code>class Settings:\n    \"\"\"Global settings for udspy.\n\n    udspy uses a single LM (Language Model) instance to handle all provider interactions.\n    Create an LM using the factory function and configure it globally or per-context.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self._lm: BaseLM | None = None\n        self._default_kwargs: dict[str, Any] = {}\n        self._callbacks: list[Any] = []\n\n        self._context_lm: ContextVar[BaseLM | None] = ContextVar(\"context_lm\", default=None)\n        self._context_kwargs: ContextVar[dict[str, Any] | None] = ContextVar(\n            \"context_kwargs\", default=None\n        )\n        self._context_callbacks: ContextVar[list[Any] | None] = ContextVar(\n            \"context_callbacks\", default=None\n        )\n\n    def configure(\n        self,\n        lm: BaseLM | None = None,\n        callbacks: list[Any] | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Configure global language model and defaults.\n\n        Args:\n            lm: Language model instance. If not provided, creates from environment variables\n            callbacks: List of callback handlers for telemetry/monitoring\n            **kwargs: Default kwargs for all completions (temperature, etc.)\n\n        Examples:\n            # From environment variables\n            # Set: UDSPY_LM_MODEL=gpt-4o, UDSPY_LM_API_KEY=sk-...\n            udspy.settings.configure()\n\n            # With custom LM instance\n            from udspy import LM\n            lm = LM(model=\"gpt-4o\", api_key=\"sk-...\")\n            udspy.settings.configure(lm=lm)\n\n            # With Ollama (local)\n            lm = LM(model=\"ollama/llama2\")\n            udspy.settings.configure(lm=lm)\n\n            # With callbacks\n            from udspy import LM, BaseCallback\n\n            class LoggingCallback(BaseCallback):\n                def on_lm_start(self, call_id, instance, inputs):\n                    print(f\"LLM called: {inputs}\")\n\n            lm = LM(model=\"gpt-4o\", api_key=\"sk-...\")\n            udspy.settings.configure(lm=lm, callbacks=[LoggingCallback()])\n        \"\"\"\n        if lm:\n            self._lm = lm\n\n        if callbacks is not None:\n            self._callbacks = callbacks\n\n        self._default_kwargs.update(kwargs)\n\n    @property\n    def lm(self) -&gt; BaseLM:\n        \"\"\"Get the language model instance (context-aware).\n\n        This is the standard way to access the LM for predictions.\n\n        Returns:\n            LM instance for making predictions\n\n        Raises:\n            RuntimeError: If LM not configured\n        \"\"\"\n        context_lm = self._context_lm.get()\n        if context_lm is not None:\n            return context_lm\n\n        if model := os.getenv(\"USDPY_LM_MODEL\"):\n            self._lm = LM(model)\n\n        if self._lm is None:\n            raise RuntimeError(\n                \"LM not configured. Call udspy.settings.configure() first.\\n\"\n                \"Example: udspy.settings.configure(lm=LM(model='gpt-4o', api_key='sk-...'))\"\n            )\n        return self._lm\n\n    @property\n    def callbacks(self) -&gt; list[Any]:\n        \"\"\"Get the default callbacks (context-aware).\"\"\"\n        context_callbacks = self._context_callbacks.get()\n        if context_callbacks is not None:\n            return context_callbacks\n\n        return self._callbacks\n\n    @property\n    def default_kwargs(self) -&gt; dict[str, Any]:\n        \"\"\"Get the default kwargs for completions (context-aware).\"\"\"\n        result = self._default_kwargs.copy()\n\n        context_kwargs = self._context_kwargs.get()\n        if context_kwargs is not None:\n            result.update(context_kwargs)\n\n        return result\n\n    def get(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"Get a setting value by key (for callback compatibility).\n\n        Args:\n            key: Setting key to retrieve\n            default: Default value if key not found\n\n        Returns:\n            Setting value or default\n        \"\"\"\n        if key == \"callbacks\":\n            context_callbacks = self._context_callbacks.get()\n            if context_callbacks is not None:\n                return context_callbacks\n            return self._callbacks\n        return default\n\n    @contextmanager\n    def context(\n        self,\n        lm: BaseLM | None = None,\n        callbacks: list[Any] | None = None,\n        **kwargs: Any,\n    ) -&gt; Iterator[None]:\n        \"\"\"Context manager for temporary settings overrides.\n\n        This is thread-safe and allows you to use different LMs or settings\n        within a specific context. Useful for multi-tenant applications.\n\n        Args:\n            lm: Temporary LM instance\n            callbacks: Temporary callback handlers\n            **kwargs: Temporary kwargs for completions\n\n        Examples:\n            # Global settings\n            from udspy import LM\n            lm = LM(model=\"gpt-4o-mini\", api_key=\"global-key\")\n            udspy.settings.configure(lm=lm)\n\n            class QA(Signature):\n                question: str = InputField()\n                answer: str = OutputField()\n\n            predictor = Predict(QA)\n\n            # Temporary override for specific context\n            tenant_lm = LM(model=\"gpt-4\", api_key=\"tenant-key\")\n            with udspy.settings.context(lm=tenant_lm):\n                result = predictor(question=\"...\")  # Uses gpt-4 with tenant-key\n\n            # Back to global settings\n            result = predictor(question=\"...\")  # Uses gpt-4o-mini with global-key\n\n            # With Ollama\n            ollama_lm = LM(model=\"ollama/llama2\")\n            with udspy.settings.context(lm=ollama_lm):\n                result = predictor(question=\"...\")  # Uses Ollama\n        \"\"\"\n        prev_lm = self._context_lm.get()\n        prev_kwargs = self._context_kwargs.get()\n        prev_callbacks = self._context_callbacks.get()\n\n        try:\n            if lm:\n                self._context_lm.set(lm)\n\n            if callbacks is not None:\n                self._context_callbacks.set(callbacks)\n\n            if kwargs:\n                merged_kwargs = (prev_kwargs or {}).copy()\n                merged_kwargs.update(kwargs)\n                self._context_kwargs.set(merged_kwargs)\n\n            yield\n\n        finally:\n            self._context_lm.set(prev_lm)\n            self._context_kwargs.set(prev_kwargs)\n            self._context_callbacks.set(prev_callbacks)\n</code></pre>"},{"location":"api/settings/#udspy.settings.Settings-attributes","title":"Attributes","text":""},{"location":"api/settings/#udspy.settings.Settings.callbacks","title":"<code>callbacks</code>  <code>property</code>","text":"<p>Get the default callbacks (context-aware).</p>"},{"location":"api/settings/#udspy.settings.Settings.default_kwargs","title":"<code>default_kwargs</code>  <code>property</code>","text":"<p>Get the default kwargs for completions (context-aware).</p>"},{"location":"api/settings/#udspy.settings.Settings.lm","title":"<code>lm</code>  <code>property</code>","text":"<p>Get the language model instance (context-aware).</p> <p>This is the standard way to access the LM for predictions.</p> <p>Returns:</p> Type Description <code>BaseLM</code> <p>LM instance for making predictions</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If LM not configured</p>"},{"location":"api/settings/#udspy.settings.Settings-functions","title":"Functions","text":""},{"location":"api/settings/#udspy.settings.Settings.configure","title":"<code>configure(lm=None, callbacks=None, **kwargs)</code>","text":"<p>Configure global language model and defaults.</p> <p>Parameters:</p> Name Type Description Default <code>lm</code> <code>BaseLM | None</code> <p>Language model instance. If not provided, creates from environment variables</p> <code>None</code> <code>callbacks</code> <code>list[Any] | None</code> <p>List of callback handlers for telemetry/monitoring</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Default kwargs for all completions (temperature, etc.)</p> <code>{}</code> <p>Examples:</p>"},{"location":"api/settings/#udspy.settings.Settings.configure--from-environment-variables","title":"From environment variables","text":""},{"location":"api/settings/#udspy.settings.Settings.configure--set-udspy_lm_modelgpt-4o-udspy_lm_api_keysk-","title":"Set: UDSPY_LM_MODEL=gpt-4o, UDSPY_LM_API_KEY=sk-...","text":"<p>udspy.settings.configure()</p>"},{"location":"api/settings/#udspy.settings.Settings.configure--with-custom-lm-instance","title":"With custom LM instance","text":"<p>from udspy import LM lm = LM(model=\"gpt-4o\", api_key=\"sk-...\") udspy.settings.configure(lm=lm)</p>"},{"location":"api/settings/#udspy.settings.Settings.configure--with-ollama-local","title":"With Ollama (local)","text":"<p>lm = LM(model=\"ollama/llama2\") udspy.settings.configure(lm=lm)</p>"},{"location":"api/settings/#udspy.settings.Settings.configure--with-callbacks","title":"With callbacks","text":"<p>from udspy import LM, BaseCallback</p> <p>class LoggingCallback(BaseCallback):     def on_lm_start(self, call_id, instance, inputs):         print(f\"LLM called: {inputs}\")</p> <p>lm = LM(model=\"gpt-4o\", api_key=\"sk-...\") udspy.settings.configure(lm=lm, callbacks=[LoggingCallback()])</p> Source code in <code>src/udspy/settings.py</code> <pre><code>def configure(\n    self,\n    lm: BaseLM | None = None,\n    callbacks: list[Any] | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Configure global language model and defaults.\n\n    Args:\n        lm: Language model instance. If not provided, creates from environment variables\n        callbacks: List of callback handlers for telemetry/monitoring\n        **kwargs: Default kwargs for all completions (temperature, etc.)\n\n    Examples:\n        # From environment variables\n        # Set: UDSPY_LM_MODEL=gpt-4o, UDSPY_LM_API_KEY=sk-...\n        udspy.settings.configure()\n\n        # With custom LM instance\n        from udspy import LM\n        lm = LM(model=\"gpt-4o\", api_key=\"sk-...\")\n        udspy.settings.configure(lm=lm)\n\n        # With Ollama (local)\n        lm = LM(model=\"ollama/llama2\")\n        udspy.settings.configure(lm=lm)\n\n        # With callbacks\n        from udspy import LM, BaseCallback\n\n        class LoggingCallback(BaseCallback):\n            def on_lm_start(self, call_id, instance, inputs):\n                print(f\"LLM called: {inputs}\")\n\n        lm = LM(model=\"gpt-4o\", api_key=\"sk-...\")\n        udspy.settings.configure(lm=lm, callbacks=[LoggingCallback()])\n    \"\"\"\n    if lm:\n        self._lm = lm\n\n    if callbacks is not None:\n        self._callbacks = callbacks\n\n    self._default_kwargs.update(kwargs)\n</code></pre>"},{"location":"api/settings/#udspy.settings.Settings.context","title":"<code>context(lm=None, callbacks=None, **kwargs)</code>","text":"<p>Context manager for temporary settings overrides.</p> <p>This is thread-safe and allows you to use different LMs or settings within a specific context. Useful for multi-tenant applications.</p> <p>Parameters:</p> Name Type Description Default <code>lm</code> <code>BaseLM | None</code> <p>Temporary LM instance</p> <code>None</code> <code>callbacks</code> <code>list[Any] | None</code> <p>Temporary callback handlers</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Temporary kwargs for completions</p> <code>{}</code> <p>Examples:</p>"},{"location":"api/settings/#udspy.settings.Settings.context--global-settings","title":"Global settings","text":"<p>from udspy import LM lm = LM(model=\"gpt-4o-mini\", api_key=\"global-key\") udspy.settings.configure(lm=lm)</p> <p>class QA(Signature):     question: str = InputField()     answer: str = OutputField()</p> <p>predictor = Predict(QA)</p>"},{"location":"api/settings/#udspy.settings.Settings.context--temporary-override-for-specific-context","title":"Temporary override for specific context","text":"<p>tenant_lm = LM(model=\"gpt-4\", api_key=\"tenant-key\") with udspy.settings.context(lm=tenant_lm):     result = predictor(question=\"...\")  # Uses gpt-4 with tenant-key</p>"},{"location":"api/settings/#udspy.settings.Settings.context--back-to-global-settings","title":"Back to global settings","text":"<p>result = predictor(question=\"...\")  # Uses gpt-4o-mini with global-key</p>"},{"location":"api/settings/#udspy.settings.Settings.context--with-ollama","title":"With Ollama","text":"<p>ollama_lm = LM(model=\"ollama/llama2\") with udspy.settings.context(lm=ollama_lm):     result = predictor(question=\"...\")  # Uses Ollama</p> Source code in <code>src/udspy/settings.py</code> <pre><code>@contextmanager\ndef context(\n    self,\n    lm: BaseLM | None = None,\n    callbacks: list[Any] | None = None,\n    **kwargs: Any,\n) -&gt; Iterator[None]:\n    \"\"\"Context manager for temporary settings overrides.\n\n    This is thread-safe and allows you to use different LMs or settings\n    within a specific context. Useful for multi-tenant applications.\n\n    Args:\n        lm: Temporary LM instance\n        callbacks: Temporary callback handlers\n        **kwargs: Temporary kwargs for completions\n\n    Examples:\n        # Global settings\n        from udspy import LM\n        lm = LM(model=\"gpt-4o-mini\", api_key=\"global-key\")\n        udspy.settings.configure(lm=lm)\n\n        class QA(Signature):\n            question: str = InputField()\n            answer: str = OutputField()\n\n        predictor = Predict(QA)\n\n        # Temporary override for specific context\n        tenant_lm = LM(model=\"gpt-4\", api_key=\"tenant-key\")\n        with udspy.settings.context(lm=tenant_lm):\n            result = predictor(question=\"...\")  # Uses gpt-4 with tenant-key\n\n        # Back to global settings\n        result = predictor(question=\"...\")  # Uses gpt-4o-mini with global-key\n\n        # With Ollama\n        ollama_lm = LM(model=\"ollama/llama2\")\n        with udspy.settings.context(lm=ollama_lm):\n            result = predictor(question=\"...\")  # Uses Ollama\n    \"\"\"\n    prev_lm = self._context_lm.get()\n    prev_kwargs = self._context_kwargs.get()\n    prev_callbacks = self._context_callbacks.get()\n\n    try:\n        if lm:\n            self._context_lm.set(lm)\n\n        if callbacks is not None:\n            self._context_callbacks.set(callbacks)\n\n        if kwargs:\n            merged_kwargs = (prev_kwargs or {}).copy()\n            merged_kwargs.update(kwargs)\n            self._context_kwargs.set(merged_kwargs)\n\n        yield\n\n    finally:\n        self._context_lm.set(prev_lm)\n        self._context_kwargs.set(prev_kwargs)\n        self._context_callbacks.set(prev_callbacks)\n</code></pre>"},{"location":"api/settings/#udspy.settings.Settings.get","title":"<code>get(key, default=None)</code>","text":"<p>Get a setting value by key (for callback compatibility).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Setting key to retrieve</p> required <code>default</code> <code>Any</code> <p>Default value if key not found</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Setting value or default</p> Source code in <code>src/udspy/settings.py</code> <pre><code>def get(self, key: str, default: Any = None) -&gt; Any:\n    \"\"\"Get a setting value by key (for callback compatibility).\n\n    Args:\n        key: Setting key to retrieve\n        default: Default value if key not found\n\n    Returns:\n        Setting value or default\n    \"\"\"\n    if key == \"callbacks\":\n        context_callbacks = self._context_callbacks.get()\n        if context_callbacks is not None:\n            return context_callbacks\n        return self._callbacks\n    return default\n</code></pre>"},{"location":"api/settings/#udspy.settings-functions","title":"Functions","text":""},{"location":"api/signature/","title":"API Reference: Signatures","text":"<p>Signatures define the inputs and outputs for modules. They provide type safety and clear contracts for LLM interactions.</p>"},{"location":"api/signature/#creating-signatures","title":"Creating Signatures","text":""},{"location":"api/signature/#class-based-signatures","title":"Class-based Signatures","text":"<p>The traditional way to define signatures using Python classes:</p> <pre><code>from udspy import Signature, InputField, OutputField\n\nclass QA(Signature):\n    \"\"\"Answer questions concisely.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n</code></pre>"},{"location":"api/signature/#string-signatures-dspy-style","title":"String Signatures (DSPy-style)","text":"<p>For quick prototyping, use the string format <code>\"inputs -&gt; outputs\"</code>:</p> <pre><code>from udspy import Signature\n\n# Simple signature\nQA = Signature.from_string(\"question -&gt; answer\")\n\n# Multiple inputs and outputs\nAnalyze = Signature.from_string(\n    \"context, question -&gt; summary, answer\",\n    \"Analyze text and answer questions\"\n)\n</code></pre> <p>Format: <code>\"input1, input2 -&gt; output1, output2\"</code></p> <ul> <li>Inputs and outputs are comma-separated</li> <li>Whitespace is trimmed automatically</li> <li>All fields default to <code>str</code> type</li> <li>Optional second argument for instructions</li> </ul>"},{"location":"api/signature/#direct-module-usage","title":"Direct Module Usage","text":"<p>All modules automatically recognize string signatures:</p> <pre><code>from udspy import Predict, ChainOfThought\n\n# These are equivalent\npredictor1 = Predict(\"question -&gt; answer\")\npredictor2 = Predict(Signature.from_string(\"question -&gt; answer\"))\n</code></pre>"},{"location":"api/signature/#when-to-use-each-format","title":"When to Use Each Format","text":"<p>Use string signatures (<code>from_string</code>) when: - Prototyping quickly - All fields are strings - You don't need field descriptions - The signature is simple</p> <p>Use class-based signatures when: - You need custom types (int, list, custom Pydantic models) - You want field descriptions for better LLM guidance - The signature is complex - You want IDE autocomplete and type checking</p>"},{"location":"api/signature/#examples","title":"Examples","text":""},{"location":"api/signature/#basic-string-signature","title":"Basic String Signature","text":"<pre><code>from udspy import Predict\n\npredictor = Predict(\"question -&gt; answer\")\nresult = predictor(question=\"What is Python?\")\nprint(result.answer)\n</code></pre>"},{"location":"api/signature/#multiple-fields","title":"Multiple Fields","text":"<pre><code>from udspy import ChainOfThought\n\ncot = ChainOfThought(\"context, question -&gt; summary, answer\")\nresult = cot(\n    context=\"Python is a programming language\",\n    question=\"What is Python?\"\n)\nprint(result.reasoning)\nprint(result.summary)\nprint(result.answer)\n</code></pre>"},{"location":"api/signature/#with-instructions","title":"With Instructions","text":"<pre><code>QA = Signature.from_string(\n    \"question -&gt; answer\",\n    \"Answer questions concisely and accurately\"\n)\npredictor = Predict(QA)\n</code></pre>"},{"location":"api/signature/#comparison","title":"Comparison","text":"<pre><code># String format - quick and simple\nQA_String = Signature.from_string(\"question -&gt; answer\")\n\n# Class format - more control\nclass QA_Class(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField(description=\"Question to answer\")\n    answer: str = OutputField(description=\"Concise answer\")\n</code></pre>"},{"location":"api/signature/#api-reference","title":"API Reference","text":""},{"location":"api/signature/#udspy.signature","title":"<code>udspy.signature</code>","text":"<p>Signature definitions for structured LLM inputs and outputs.</p>"},{"location":"api/signature/#udspy.signature-classes","title":"Classes","text":""},{"location":"api/signature/#udspy.signature.Signature","title":"<code>Signature</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for defining LLM task signatures.</p> <p>A Signature specifies the input and output fields for an LLM task, along with an optional instruction describing the task.</p> Example <pre><code>class QA(Signature):\n    '''Answer questions concisely.'''\n    question: str = InputField(description=\"Question to answer\")\n    answer: str = OutputField(description=\"Concise answer\")\n</code></pre> Source code in <code>src/udspy/signature.py</code> <pre><code>class Signature(BaseModel, metaclass=SignatureMeta):\n    \"\"\"Base class for defining LLM task signatures.\n\n    A Signature specifies the input and output fields for an LLM task,\n    along with an optional instruction describing the task.\n\n    Example:\n        ```python\n        class QA(Signature):\n            '''Answer questions concisely.'''\n            question: str = InputField(description=\"Question to answer\")\n            answer: str = OutputField(description=\"Concise answer\")\n        ```\n    \"\"\"\n\n    @classmethod\n    def get_input_fields(cls) -&gt; dict[str, FieldInfo]:\n        \"\"\"Get all input fields defined in this signature.\"\"\"\n        return {\n            name: field_info\n            for name, field_info in cls.model_fields.items()\n            if (field_info.json_schema_extra or {}).get(\"__udspy_field_type\") == \"input\"  # type: ignore[union-attr]\n        }\n\n    @classmethod\n    def get_output_fields(cls) -&gt; dict[str, FieldInfo]:\n        \"\"\"Get all output fields defined in this signature.\"\"\"\n        return {\n            name: field_info\n            for name, field_info in cls.model_fields.items()\n            if (field_info.json_schema_extra or {}).get(\"__udspy_field_type\") == \"output\"  # type: ignore[union-attr]\n        }\n\n    @classmethod\n    def get_instructions(cls) -&gt; str:\n        \"\"\"Get the task instructions from the docstring.\"\"\"\n        return (cls.__doc__ or \"\").strip()\n\n    @classmethod\n    def from_string(cls, spec: str, instructions: str = \"\") -&gt; type[\"Signature\"]:\n        \"\"\"Create a Signature from DSPy-style string format.\n\n        This is a convenience method for creating simple signatures using\n        the DSPy string format \"input1, input2 -&gt; output1, output2\".\n        All fields default to type `str`.\n\n        For more control over field types, descriptions, and defaults,\n        use the class-based Signature definition or `make_signature()`.\n\n        Args:\n            spec: Signature specification in format \"inputs -&gt; outputs\"\n                  Examples: \"question -&gt; answer\"\n                           \"context, question -&gt; answer\"\n                           \"text -&gt; summary, keywords\"\n            instructions: Optional task instructions (docstring)\n\n        Returns:\n            A new Signature class with all fields as type `str`\n\n        Raises:\n            ValueError: If spec is not in valid format\n\n        Example:\n            ```python\n            # Simple signature\n            QA = Signature.from_string(\"question -&gt; answer\", \"Answer questions\")\n            predictor = Predict(QA)\n\n            # Multiple inputs and outputs\n            Summarize = Signature.from_string(\n                \"document, style -&gt; summary, keywords\",\n                \"Summarize documents in specified style\"\n            )\n            ```\n\n        Note:\n            This is equivalent to DSPy's string-based signature creation.\n            All fields default to `str` type. For custom types, use the\n            class-based approach with InputField() and OutputField().\n        \"\"\"\n        if \"-&gt;\" not in spec:\n            raise ValueError(\n                f\"Invalid signature format: '{spec}'. \"\n                \"Must be in format 'inputs -&gt; outputs' (e.g., 'question -&gt; answer')\"\n            )\n\n        parts = spec.split(\"-&gt;\")\n        if len(parts) != 2:\n            raise ValueError(\n                f\"Invalid signature format: '{spec}'. Must have exactly one '-&gt;' separator\"\n            )\n\n        input_str = parts[0].strip()\n        if not input_str:\n            raise ValueError(\"Signature must have at least one input field\")\n\n        input_names = [name.strip() for name in input_str.split(\",\")]\n        input_fields: dict[str, type] = {name: str for name in input_names if name}\n\n        output_str = parts[1].strip()\n        if not output_str:\n            raise ValueError(\"Signature must have at least one output field\")\n\n        output_names = [name.strip() for name in output_str.split(\",\")]\n        output_fields: dict[str, type] = {name: str for name in output_names if name}\n\n        return make_signature(input_fields, output_fields, instructions)\n</code></pre>"},{"location":"api/signature/#udspy.signature.Signature-functions","title":"Functions","text":""},{"location":"api/signature/#udspy.signature.Signature.from_string","title":"<code>from_string(spec, instructions='')</code>  <code>classmethod</code>","text":"<p>Create a Signature from DSPy-style string format.</p> <p>This is a convenience method for creating simple signatures using the DSPy string format \"input1, input2 -&gt; output1, output2\". All fields default to type <code>str</code>.</p> <p>For more control over field types, descriptions, and defaults, use the class-based Signature definition or <code>make_signature()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>str</code> <p>Signature specification in format \"inputs -&gt; outputs\"   Examples: \"question -&gt; answer\"            \"context, question -&gt; answer\"            \"text -&gt; summary, keywords\"</p> required <code>instructions</code> <code>str</code> <p>Optional task instructions (docstring)</p> <code>''</code> <p>Returns:</p> Type Description <code>type[Signature]</code> <p>A new Signature class with all fields as type <code>str</code></p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If spec is not in valid format</p> Example <pre><code># Simple signature\nQA = Signature.from_string(\"question -&gt; answer\", \"Answer questions\")\npredictor = Predict(QA)\n\n# Multiple inputs and outputs\nSummarize = Signature.from_string(\n    \"document, style -&gt; summary, keywords\",\n    \"Summarize documents in specified style\"\n)\n</code></pre> Note <p>This is equivalent to DSPy's string-based signature creation. All fields default to <code>str</code> type. For custom types, use the class-based approach with InputField() and OutputField().</p> Source code in <code>src/udspy/signature.py</code> <pre><code>@classmethod\ndef from_string(cls, spec: str, instructions: str = \"\") -&gt; type[\"Signature\"]:\n    \"\"\"Create a Signature from DSPy-style string format.\n\n    This is a convenience method for creating simple signatures using\n    the DSPy string format \"input1, input2 -&gt; output1, output2\".\n    All fields default to type `str`.\n\n    For more control over field types, descriptions, and defaults,\n    use the class-based Signature definition or `make_signature()`.\n\n    Args:\n        spec: Signature specification in format \"inputs -&gt; outputs\"\n              Examples: \"question -&gt; answer\"\n                       \"context, question -&gt; answer\"\n                       \"text -&gt; summary, keywords\"\n        instructions: Optional task instructions (docstring)\n\n    Returns:\n        A new Signature class with all fields as type `str`\n\n    Raises:\n        ValueError: If spec is not in valid format\n\n    Example:\n        ```python\n        # Simple signature\n        QA = Signature.from_string(\"question -&gt; answer\", \"Answer questions\")\n        predictor = Predict(QA)\n\n        # Multiple inputs and outputs\n        Summarize = Signature.from_string(\n            \"document, style -&gt; summary, keywords\",\n            \"Summarize documents in specified style\"\n        )\n        ```\n\n    Note:\n        This is equivalent to DSPy's string-based signature creation.\n        All fields default to `str` type. For custom types, use the\n        class-based approach with InputField() and OutputField().\n    \"\"\"\n    if \"-&gt;\" not in spec:\n        raise ValueError(\n            f\"Invalid signature format: '{spec}'. \"\n            \"Must be in format 'inputs -&gt; outputs' (e.g., 'question -&gt; answer')\"\n        )\n\n    parts = spec.split(\"-&gt;\")\n    if len(parts) != 2:\n        raise ValueError(\n            f\"Invalid signature format: '{spec}'. Must have exactly one '-&gt;' separator\"\n        )\n\n    input_str = parts[0].strip()\n    if not input_str:\n        raise ValueError(\"Signature must have at least one input field\")\n\n    input_names = [name.strip() for name in input_str.split(\",\")]\n    input_fields: dict[str, type] = {name: str for name in input_names if name}\n\n    output_str = parts[1].strip()\n    if not output_str:\n        raise ValueError(\"Signature must have at least one output field\")\n\n    output_names = [name.strip() for name in output_str.split(\",\")]\n    output_fields: dict[str, type] = {name: str for name in output_names if name}\n\n    return make_signature(input_fields, output_fields, instructions)\n</code></pre>"},{"location":"api/signature/#udspy.signature.Signature.get_input_fields","title":"<code>get_input_fields()</code>  <code>classmethod</code>","text":"<p>Get all input fields defined in this signature.</p> Source code in <code>src/udspy/signature.py</code> <pre><code>@classmethod\ndef get_input_fields(cls) -&gt; dict[str, FieldInfo]:\n    \"\"\"Get all input fields defined in this signature.\"\"\"\n    return {\n        name: field_info\n        for name, field_info in cls.model_fields.items()\n        if (field_info.json_schema_extra or {}).get(\"__udspy_field_type\") == \"input\"  # type: ignore[union-attr]\n    }\n</code></pre>"},{"location":"api/signature/#udspy.signature.Signature.get_instructions","title":"<code>get_instructions()</code>  <code>classmethod</code>","text":"<p>Get the task instructions from the docstring.</p> Source code in <code>src/udspy/signature.py</code> <pre><code>@classmethod\ndef get_instructions(cls) -&gt; str:\n    \"\"\"Get the task instructions from the docstring.\"\"\"\n    return (cls.__doc__ or \"\").strip()\n</code></pre>"},{"location":"api/signature/#udspy.signature.Signature.get_output_fields","title":"<code>get_output_fields()</code>  <code>classmethod</code>","text":"<p>Get all output fields defined in this signature.</p> Source code in <code>src/udspy/signature.py</code> <pre><code>@classmethod\ndef get_output_fields(cls) -&gt; dict[str, FieldInfo]:\n    \"\"\"Get all output fields defined in this signature.\"\"\"\n    return {\n        name: field_info\n        for name, field_info in cls.model_fields.items()\n        if (field_info.json_schema_extra or {}).get(\"__udspy_field_type\") == \"output\"  # type: ignore[union-attr]\n    }\n</code></pre>"},{"location":"api/signature/#udspy.signature.SignatureMeta","title":"<code>SignatureMeta</code>","text":"<p>               Bases: <code>type(BaseModel)</code></p> <p>Metaclass for Signature that validates field types.</p> Source code in <code>src/udspy/signature.py</code> <pre><code>class SignatureMeta(type(BaseModel)):  # type: ignore[misc]\n    \"\"\"Metaclass for Signature that validates field types.\"\"\"\n\n    def __new__(\n        mcs,\n        name: str,\n        bases: tuple[type, ...],\n        namespace: dict[str, Any],\n        **kwargs: Any,\n    ) -&gt; type:\n        cls = super().__new__(mcs, name, bases, namespace, **kwargs)\n\n        # Skip validation for the base Signature class\n        if name == \"Signature\":\n            return cls\n\n        for field_name, field_info in cls.model_fields.items():\n            if not isinstance(field_info, FieldInfo):\n                continue\n\n            json_schema_extra = field_info.json_schema_extra or {}\n            field_type = json_schema_extra.get(\"__udspy_field_type\")  # type: ignore[union-attr]\n\n            if field_type not in (\"input\", \"output\"):\n                raise TypeError(\n                    f\"Field '{field_name}' in {name} must be declared with \"\n                    f\"InputField() or OutputField()\"\n                )\n\n        return cls\n</code></pre>"},{"location":"api/signature/#udspy.signature-functions","title":"Functions","text":""},{"location":"api/signature/#udspy.signature.InputField","title":"<code>InputField(default=..., *, description=None, **kwargs)</code>","text":"<p>Define an input field for a Signature.</p> <p>Parameters:</p> Name Type Description Default <code>default</code> <code>Any</code> <p>Default value for the field</p> <code>...</code> <code>description</code> <code>str | None</code> <p>Human-readable description of the field's purpose</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional Pydantic field arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic FieldInfo with input metadata</p> Source code in <code>src/udspy/signature.py</code> <pre><code>def InputField(\n    default: Any = ...,\n    *,\n    description: str | None = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Define an input field for a Signature.\n\n    Args:\n        default: Default value for the field\n        description: Human-readable description of the field's purpose\n        **kwargs: Additional Pydantic field arguments\n\n    Returns:\n        A Pydantic FieldInfo with input metadata\n    \"\"\"\n    json_schema_extra = kwargs.pop(\"json_schema_extra\", {})\n    json_schema_extra[\"__udspy_field_type\"] = \"input\"\n\n    return Field(\n        default=default,\n        description=description,\n        json_schema_extra=json_schema_extra,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/signature/#udspy.signature.OutputField","title":"<code>OutputField(default=..., *, description=None, **kwargs)</code>","text":"<p>Define an output field for a Signature.</p> <p>Parameters:</p> Name Type Description Default <code>default</code> <code>Any</code> <p>Default value for the field</p> <code>...</code> <code>description</code> <code>str | None</code> <p>Human-readable description of the field's purpose</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional Pydantic field arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic FieldInfo with output metadata</p> Source code in <code>src/udspy/signature.py</code> <pre><code>def OutputField(\n    default: Any = ...,\n    *,\n    description: str | None = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Define an output field for a Signature.\n\n    Args:\n        default: Default value for the field\n        description: Human-readable description of the field's purpose\n        **kwargs: Additional Pydantic field arguments\n\n    Returns:\n        A Pydantic FieldInfo with output metadata\n    \"\"\"\n    json_schema_extra = kwargs.pop(\"json_schema_extra\", {})\n    json_schema_extra[\"__udspy_field_type\"] = \"output\"\n\n    return Field(\n        default=default,\n        description=description,\n        json_schema_extra=json_schema_extra,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/signature/#udspy.signature.make_signature","title":"<code>make_signature(input_fields, output_fields, instructions='')</code>","text":"<p>Dynamically create a Signature class.</p> <p>Parameters:</p> Name Type Description Default <code>input_fields</code> <code>dict[str, type]</code> <p>Dictionary mapping field names to types for inputs</p> required <code>output_fields</code> <code>dict[str, type]</code> <p>Dictionary mapping field names to types for outputs</p> required <code>instructions</code> <code>str</code> <p>Task instructions</p> <code>''</code> <p>Returns:</p> Type Description <code>type[Signature]</code> <p>A new Signature class</p> Example <pre><code>QA = make_signature(\n    {\"question\": str},\n    {\"answer\": str},\n    \"Answer questions concisely\"\n)\n</code></pre> Source code in <code>src/udspy/signature.py</code> <pre><code>def make_signature(\n    input_fields: dict[str, type],\n    output_fields: dict[str, type],\n    instructions: str = \"\",\n) -&gt; type[Signature]:\n    \"\"\"Dynamically create a Signature class.\n\n    Args:\n        input_fields: Dictionary mapping field names to types for inputs\n        output_fields: Dictionary mapping field names to types for outputs\n        instructions: Task instructions\n\n    Returns:\n        A new Signature class\n\n    Example:\n        ```python\n        QA = make_signature(\n            {\"question\": str},\n            {\"answer\": str},\n            \"Answer questions concisely\"\n        )\n        ```\n    \"\"\"\n    fields = {}\n\n    for name, type_ in input_fields.items():\n        fields[name] = (type_, InputField())\n\n    for name, type_ in output_fields.items():\n        fields[name] = (type_, OutputField())\n\n    sig = create_model(\n        \"DynamicSignature\",\n        __base__=Signature,\n        **fields,  # type: ignore\n    )\n\n    if instructions:\n        sig.__doc__ = instructions\n\n    return sig\n</code></pre>"},{"location":"api/streaming/","title":"API Reference: Streaming","text":""},{"location":"api/streaming/#udspy.streaming","title":"<code>udspy.streaming</code>","text":"<p>Streaming support with event queue for incremental LLM outputs and tool updates.</p>"},{"location":"api/streaming/#udspy.streaming-classes","title":"Classes","text":""},{"location":"api/streaming/#udspy.streaming.OutputStreamChunk","title":"<code>OutputStreamChunk</code>","text":"<p>               Bases: <code>StreamChunk</code></p> <p>A chunk of streamed LLM output for a specific field.</p> <p>Attributes:</p> Name Type Description <code>field_name</code> <code>str</code> <p>Name of the output field</p> <code>delta</code> <code>str</code> <p>Incremental content for this field (new text since last chunk)</p> <code>content</code> <code>str</code> <p>Full accumulated content for this field so far</p> <code>is_complete</code> <code>bool</code> <p>Whether this field is finished streaming</p> Source code in <code>src/udspy/streaming.py</code> <pre><code>class OutputStreamChunk(StreamChunk):\n    \"\"\"A chunk of streamed LLM output for a specific field.\n\n    Attributes:\n        field_name: Name of the output field\n        delta: Incremental content for this field (new text since last chunk)\n        content: Full accumulated content for this field so far\n        is_complete: Whether this field is finished streaming\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/streaming/#udspy.streaming.Prediction","title":"<code>Prediction</code>","text":"<p>               Bases: <code>StreamEvent</code>, <code>dict[str, Any]</code></p> <p>Final prediction result with attribute access.</p> <p>This is both a StreamEvent (can be yielded from astream) and a dict (for convenient attribute access to outputs).</p> <p>Attributes:</p> Name Type Description <code>module</code> <p>The module that produced this prediction</p> <code>native_tool_calls</code> <p>Tool calls from native LLM response (if any)</p> Example <pre><code>pred = Prediction(answer=\"Paris\", reasoning=\"France's capital\")\nprint(pred.answer)  # \"Paris\"\nprint(pred[\"answer\"])  # \"Paris\"\nprint(pred.is_final)  # True for top-level result\nprint(pred.module)  # Module instance that produced this\n</code></pre> Source code in <code>src/udspy/streaming.py</code> <pre><code>class Prediction(StreamEvent, dict[str, Any]):\n    \"\"\"Final prediction result with attribute access.\n\n    This is both a StreamEvent (can be yielded from astream) and a dict\n    (for convenient attribute access to outputs).\n\n    Attributes:\n        module: The module that produced this prediction\n        native_tool_calls: Tool calls from native LLM response (if any)\n\n    Example:\n        ```python\n        pred = Prediction(answer=\"Paris\", reasoning=\"France's capital\")\n        print(pred.answer)  # \"Paris\"\n        print(pred[\"answer\"])  # \"Paris\"\n        print(pred.is_final)  # True for top-level result\n        print(pred.module)  # Module instance that produced this\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        /,\n        module: \"Module | None\" = None,\n        native_tool_calls: list[\"ToolCall\"] | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(**kwargs)\n        self.module = module\n        self.native_tool_calls = native_tool_calls\n\n    @property\n    def is_final(self) -&gt; bool:\n        \"\"\"Whether this is the final prediction (no pending tool calls).\"\"\"\n        return bool(len(self.keys()) and not self.native_tool_calls)\n\n    def __getattr__(self, name: str) -&gt; Any:\n        try:\n            return self[name]\n        except KeyError:\n            raise AttributeError(f\"Prediction has no attribute '{name}'\") from None\n\n    def __setattr__(self, name: str, value: Any) -&gt; None:\n        self[name] = value\n</code></pre>"},{"location":"api/streaming/#udspy.streaming.Prediction-attributes","title":"Attributes","text":""},{"location":"api/streaming/#udspy.streaming.Prediction.is_final","title":"<code>is_final</code>  <code>property</code>","text":"<p>Whether this is the final prediction (no pending tool calls).</p>"},{"location":"api/streaming/#udspy.streaming.StreamChunk","title":"<code>StreamChunk</code>","text":"<p>               Bases: <code>StreamEvent</code></p> <p>A chunk of streamed output from a Module.</p> Source code in <code>src/udspy/streaming.py</code> <pre><code>class StreamChunk(StreamEvent):\n    \"\"\"A chunk of streamed output from a Module.\"\"\"\n\n    module: \"Module\"\n    field_name: str\n    delta: str\n    content: str\n    is_complete: bool\n\n    def __init__(\n        self,\n        module: \"Module\",\n        field_name: str,\n        delta: str,\n        content: str,\n        is_complete: bool,\n    ) -&gt; None:\n        self.module = module\n        self.field_name = field_name\n        self.delta = delta\n        self.content = content\n        self.is_complete = is_complete\n\n    def __repr__(self) -&gt; str:\n        status = \"complete\" if self.is_complete else \"streaming\"\n        return (\n            f\"{self.__class__.__name__}(field={self.field_name}, \"\n            f\"status={status}, delta={self.delta!r}, content={self.content!r})\"\n        )\n</code></pre>"},{"location":"api/streaming/#udspy.streaming.StreamEvent","title":"<code>StreamEvent</code>","text":"<p>Base class for all stream events.</p> <p>Users can define custom event types by inheriting from this class. The only built-in events are OutputStreamChunk and Prediction.</p> Example <pre><code>from dataclasses import dataclass\nfrom udspy.streaming import StreamEvent, emit_event\n\n@dataclass\nclass ToolProgress(StreamEvent):\n    tool_name: str\n    message: str\n    progress: float  # 0.0 to 1.0\n\n# In your tool:\nasync def my_tool():\n    emit_event(ToolProgress(\"search\", \"Searching...\", 0.5))\n</code></pre> Source code in <code>src/udspy/streaming.py</code> <pre><code>class StreamEvent:\n    \"\"\"Base class for all stream events.\n\n    Users can define custom event types by inheriting from this class.\n    The only built-in events are OutputStreamChunk and Prediction.\n\n    Example:\n        ```python\n        from dataclasses import dataclass\n        from udspy.streaming import StreamEvent, emit_event\n\n        @dataclass\n        class ToolProgress(StreamEvent):\n            tool_name: str\n            message: str\n            progress: float  # 0.0 to 1.0\n\n        # In your tool:\n        async def my_tool():\n            emit_event(ToolProgress(\"search\", \"Searching...\", 0.5))\n        ```\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/streaming/#udspy.streaming.ThoughtStreamChunk","title":"<code>ThoughtStreamChunk</code>","text":"<p>               Bases: <code>StreamChunk</code></p> <p>A chunk of streamed reasoning output for a specific step.</p> <p>Attributes:</p> Name Type Description <code>module</code> <code>Module</code> <p>The module emitting this chunk</p> <code>delta</code> <code>str</code> <p>Incremental content for this step (new text since last chunk)</p> <code>content</code> <code>str</code> <p>Full accumulated content for this step so far</p> <code>is_complete</code> <code>bool</code> <p>Whether this step is finished streaming</p> Source code in <code>src/udspy/streaming.py</code> <pre><code>class ThoughtStreamChunk(StreamChunk):\n    \"\"\"A chunk of streamed reasoning output for a specific step.\n\n    Attributes:\n        module: The module emitting this chunk\n\n        delta: Incremental content for this step (new text since last chunk)\n        content: Full accumulated content for this step so far\n        is_complete: Whether this step is finished streaming\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/streaming/#udspy.streaming-functions","title":"Functions","text":""},{"location":"api/streaming/#udspy.streaming.emit_event","title":"<code>emit_event(event)</code>","text":"<p>Emit an event to the active stream.</p> <p>This can be called from anywhere (tools, callbacks, etc.) to inject events into the current streaming context. If no stream is active, this is a no-op (silently ignored).</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>StreamEvent</code> <p>The event to emit (any subclass of StreamEvent)</p> required Example <pre><code>from udspy.streaming import emit_event, StreamEvent\nfrom dataclasses import dataclass\n\n@dataclass\nclass ToolStatus(StreamEvent):\n    message: str\n\nasync def my_tool():\n    emit_event(ToolStatus(\"Starting search...\"))\n    result = await do_search()\n    emit_event(ToolStatus(\"Search complete\"))\n    return result\n\n# In the stream consumer:\nasync for event in predictor.astream(question=\"...\"):\n    if isinstance(event, ToolStatus):\n        print(f\"\ud83d\udcca {event.message}\")\n    elif isinstance(event, OutputStreamChunk):\n        print(event.delta, end=\"\", flush=True)\n</code></pre> Source code in <code>src/udspy/streaming.py</code> <pre><code>def emit_event(event: StreamEvent) -&gt; None:\n    \"\"\"Emit an event to the active stream.\n\n    This can be called from anywhere (tools, callbacks, etc.) to inject\n    events into the current streaming context. If no stream is active,\n    this is a no-op (silently ignored).\n\n    Args:\n        event: The event to emit (any subclass of StreamEvent)\n\n    Example:\n        ```python\n        from udspy.streaming import emit_event, StreamEvent\n        from dataclasses import dataclass\n\n        @dataclass\n        class ToolStatus(StreamEvent):\n            message: str\n\n        async def my_tool():\n            emit_event(ToolStatus(\"Starting search...\"))\n            result = await do_search()\n            emit_event(ToolStatus(\"Search complete\"))\n            return result\n\n        # In the stream consumer:\n        async for event in predictor.astream(question=\"...\"):\n            if isinstance(event, ToolStatus):\n                print(f\"\ud83d\udcca {event.message}\")\n            elif isinstance(event, OutputStreamChunk):\n                print(event.delta, end=\"\", flush=True)\n        ```\n    \"\"\"\n    queue = _stream_queue.get()\n    if queue is not None:\n        queue.put_nowait(event)\n</code></pre>"},{"location":"api/tool/","title":"Tool API Reference","text":"<p>API documentation for creating and using tools with native OpenAI function calling.</p>"},{"location":"api/tool/#module-udspytool","title":"Module: <code>udspy.tool</code>","text":""},{"location":"api/tool/#tool-decorator","title":"<code>@tool</code> Decorator","text":"<pre><code>@tool(\n    name: str | None = None,\n    description: str | None = None,\n    *,\n    require_confirmation: bool = False,\n) -&gt; Callable[[Callable], Tool]\n</code></pre> <p>Decorator to mark a function as a tool for use with <code>Predict</code> and <code>ReAct</code> modules.</p> <p>Parameters:</p> <ul> <li><code>name</code> (<code>str | None</code>, default: <code>None</code>): Tool name visible to the LLM</li> <li>If not provided, uses the function name</li> <li><code>description</code> (<code>str | None</code>, default: <code>None</code>): Tool description visible to the LLM</li> <li>If not provided, uses the function's docstring</li> <li>This helps the LLM decide when to use the tool</li> <li><code>require_confirmation</code> (<code>bool</code>, default: <code>False</code>): Whether to require user confirmation before execution</li> <li>If <code>True</code>, wraps the tool with <code>@confirm_first</code> decorator</li> <li>Raises <code>ConfirmationRequired</code> on first call, executes after approval</li> <li>Useful for destructive or sensitive operations</li> </ul> <p>Returns:</p> <ul> <li><code>Tool</code>: A wrapped tool object with metadata</li> </ul> <p>Example:</p> <pre><code>from pydantic import Field\nfrom udspy import tool\n\n@tool(name=\"calculator\", description=\"Perform arithmetic operations\")\ndef calculator(\n    operation: str = Field(description=\"add, subtract, multiply, or divide\"),\n    a: float = Field(description=\"First number\"),\n    b: float = Field(description=\"Second number\"),\n) -&gt; float:\n    \"\"\"Execute arithmetic operation.\"\"\"\n    ops = {\n        \"add\": a + b,\n        \"subtract\": a - b,\n        \"multiply\": a * b,\n        \"divide\": a / b if b != 0 else float(\"inf\"),\n    }\n    return ops[operation]\n</code></pre>"},{"location":"api/tool/#tool-confirmation-example","title":"Tool Confirmation Example","text":"<pre><code>import os\nfrom pydantic import Field\nfrom udspy import tool\n\n@tool(\n    name=\"delete_file\",\n    description=\"Delete a file from the filesystem\",\n    require_confirmation=True  # Requires user confirmation\n)\ndef delete_file(path: str = Field(description=\"File path to delete\")) -&gt; str:\n    \"\"\"Delete a file (requires confirmation).\"\"\"\n    os.remove(path)\n    return f\"Deleted {path}\"\n</code></pre>"},{"location":"api/tool/#tool-class","title":"<code>Tool</code> Class","text":"<pre><code>class Tool:\n    \"\"\"Wrapper for a tool function with metadata.\"\"\"\n</code></pre> <p>The <code>Tool</code> class wraps a function and adds metadata for OpenAI function calling. You typically don't instantiate this directly; use the <code>@tool</code> decorator instead.</p>"},{"location":"api/tool/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    func: Callable,\n    name: str | None = None,\n    description: str | None = None,\n    *,\n    require_confirmation: bool = False,\n    desc: str | None = None,\n    args: dict[str, str] | None = None,\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>func</code> (<code>Callable</code>): The function to wrap</li> <li><code>name</code> (<code>str | None</code>): Tool name (defaults to function name)</li> <li><code>description</code> (<code>str | None</code>): Tool description (defaults to docstring)</li> <li><code>require_confirmation</code> (<code>bool</code>, default: <code>False</code>): Whether to require confirmation before execution</li> <li><code>desc</code> (<code>str | None</code>): Alias for <code>description</code> (DSPy compatibility)</li> <li><code>args</code> (<code>dict[str, str] | None</code>): Manual argument specification (DSPy compatibility)</li> </ul> <p>Example:</p> <pre><code>from udspy import Tool\n\ndef my_function(x: int, y: int) -&gt; int:\n    \"\"\"Add two numbers.\"\"\"\n    return x + y\n\ntool = Tool(my_function, name=\"adder\", description=\"Adds numbers\")\n</code></pre> <p>However, using the <code>@tool</code> decorator is preferred:</p> <pre><code>@tool(name=\"adder\", description=\"Adds numbers\")\ndef my_function(x: int, y: int) -&gt; int:\n    \"\"\"Add two numbers.\"\"\"\n    return x + y\n</code></pre>"},{"location":"api/tool/#attributes","title":"Attributes","text":""},{"location":"api/tool/#func","title":"<code>func</code>","text":"<pre><code>func: Callable\n</code></pre> <p>The underlying function that this tool wraps.</p>"},{"location":"api/tool/#name","title":"<code>name</code>","text":"<pre><code>name: str\n</code></pre> <p>The tool's name as seen by the LLM.</p>"},{"location":"api/tool/#description","title":"<code>description</code>","text":"<pre><code>description: str\n</code></pre> <p>The tool's description as seen by the LLM.</p>"},{"location":"api/tool/#require_confirmation","title":"<code>require_confirmation</code>","text":"<pre><code>require_confirmation: bool\n</code></pre> <p>Whether this tool requires user confirmation before execution.</p>"},{"location":"api/tool/#parameters","title":"<code>parameters</code>","text":"<pre><code>parameters: dict[str, dict[str, Any]]\n</code></pre> <p>Dictionary mapping parameter names to their metadata:</p> <pre><code>{\n    \"param_name\": {\n        \"type\": str,  # Python type\n        \"description\": \"Parameter description\",\n        \"required\": True  # Whether parameter is required\n    },\n    # ...\n}\n</code></pre>"},{"location":"api/tool/#desc-dspy-compatibility","title":"<code>desc</code> (DSPy compatibility)","text":"<pre><code>desc: str\n</code></pre> <p>Alias for <code>description</code>. Provided for DSPy compatibility.</p>"},{"location":"api/tool/#args-dspy-compatibility","title":"<code>args</code> (DSPy compatibility)","text":"<pre><code>args: dict[str, str]\n</code></pre> <p>Dictionary mapping parameter names to type + description strings. Provided for DSPy compatibility.</p> <p>Example:</p> <pre><code>{\n    \"operation\": \"str - add, subtract, multiply, or divide\",\n    \"a\": \"float - First number\",\n    \"b\": \"float - Second number\"\n}\n</code></pre>"},{"location":"api/tool/#methods","title":"Methods","text":""},{"location":"api/tool/#__call__args-kwargs-any","title":"<code>__call__(*args, **kwargs) -&gt; Any</code>","text":"<p>Call the wrapped function synchronously.</p> <p>Parameters:</p> <ul> <li><code>*args</code>: Positional arguments</li> <li><code>**kwargs</code>: Keyword arguments</li> </ul> <p>Returns:</p> <ul> <li>Function result</li> </ul> <p>Example:</p> <pre><code>@tool(name=\"add\", description=\"Add numbers\")\ndef add(a: int, b: int) -&gt; int:\n    return a + b\n\nresult = add(2, 3)  # Returns 5\n</code></pre>"},{"location":"api/tool/#acallkwargs-any","title":"<code>acall(**kwargs) -&gt; Any</code>","text":"<p>Call the wrapped function asynchronously.</p> <ul> <li>If the function is async, awaits it directly</li> <li>If the function is sync, runs it in an executor to avoid blocking (unless <code>require_confirmation=True</code>)</li> <li>If <code>require_confirmation=True</code>, may raise <code>ConfirmationRequired</code> before execution</li> </ul> <p>Parameters:</p> <ul> <li><code>**kwargs</code>: Keyword arguments</li> </ul> <p>Returns:</p> <ul> <li>Awaitable that resolves to the function result</li> </ul> <p>Raises:</p> <ul> <li><code>ConfirmationRequired</code>: If <code>require_confirmation=True</code> and not yet approved</li> </ul> <p>Example:</p> <pre><code>import asyncio\n\n@tool(name=\"fetch\", description=\"Fetch data\")\nasync def fetch_data(url: str) -&gt; str:\n    # Async operation\n    return f\"Data from {url}\"\n\n# Call async\nresult = await fetch_data.acall(url=\"https://example.com\")\n</code></pre> <p>Sync function example:</p> <pre><code>@tool(name=\"compute\", description=\"Compute value\")\ndef compute(x: int) -&gt; int:\n    return x * 2\n\n# Still works with acall - runs in executor\nresult = await compute.acall(x=5)\n</code></pre>"},{"location":"api/tool/#parameters-property","title":"<code>parameters</code> Property","text":"<p>Get the JSON schema for OpenAI function calling.</p> <p>Returns:</p> <ul> <li><code>dict</code>: Complete JSON schema with type, properties, and required fields</li> </ul> <p>Example:</p> <pre><code>@tool(name=\"calculator\", description=\"Do math\")\ndef calculator(\n    operation: str = Field(description=\"Operation type\"),\n    a: float = Field(description=\"First number\"),\n    b: float = Field(description=\"Second number\"),\n) -&gt; float:\n    return eval(f\"{a} {operation} {b}\")\n\n# Get the parameters schema for OpenAI\nparams = calculator.parameters\n# {\n#     \"type\": \"object\",\n#     \"properties\": {\n#         \"operation\": {\n#             \"type\": \"string\",\n#             \"description\": \"Operation type\"\n#         },\n#         \"a\": {\n#             \"type\": \"number\",\n#             \"description\": \"First number\"\n#         },\n#         \"b\": {\n#             \"type\": \"number\",\n#             \"description\": \"Second number\"\n#         }\n#     },\n#     \"required\": [\"operation\", \"a\", \"b\"]\n# }\n\n# The adapter uses this to build OpenAI function schemas:\nfrom udspy.adapter import ChatAdapter\nadapter = ChatAdapter()\nopenai_schema = adapter.format_tool_schema(calculator)\n# {\n#     \"type\": \"function\",\n#     \"function\": {\n#         \"name\": \"calculator\",\n#         \"description\": \"Do math\",\n#         \"parameters\": calculator.parameters  # \u2190 Uses this property\n#     }\n# }\n</code></pre>"},{"location":"api/tool/#format-str","title":"<code>format() -&gt; str</code>","text":"<p>Format the tool as a human-readable string for LLM prompts.</p> <p>Returns:</p> <ul> <li><code>str</code>: Human-readable description with name, description, and parameters</li> </ul> <p>Example:</p> <pre><code>@tool(name=\"calculator\", description=\"Perform arithmetic operations\")\ndef calculator(\n    operation: str = Field(description=\"Operation type\"),\n    a: float = Field(description=\"First number\"),\n) -&gt; float:\n    return a * 2\n\n# Get human-readable format for module prompts\ndescription = calculator.format()\n# \"calculator, whose description is &lt;desc&gt;Perform arithmetic operations&lt;/desc&gt;. It takes arguments {&lt;properties&gt;}.\"\n</code></pre>"},{"location":"api/tool/#parameter-type-annotations","title":"Parameter Type Annotations","text":"<p>Tools use Python type hints to generate OpenAI schemas. Supported types:</p> Python Type JSON Schema Type <code>str</code> <code>\"string\"</code> <code>int</code> <code>\"integer\"</code> <code>float</code> <code>\"number\"</code> <code>bool</code> <code>\"boolean\"</code> <code>list</code> <code>\"array\"</code> <code>dict</code> <code>\"object\"</code> <code>Optional[T]</code> Type of <code>T</code> (nullable) <p>Example:</p> <pre><code>from typing import Optional\nfrom pydantic import Field\nfrom udspy import tool\n\n@tool(name=\"search\", description=\"Search with filters\")\ndef search(\n    query: str = Field(description=\"Search query\"),\n    max_results: int = Field(description=\"Maximum results\", default=10),\n    include_archived: bool = Field(description=\"Include archived\", default=False),\n    tags: Optional[list] = Field(description=\"Filter by tags\", default=None),\n) -&gt; str:\n    return f\"Searching for: {query}\"\n</code></pre>"},{"location":"api/tool/#using-pydantic-fields","title":"Using Pydantic Fields","text":"<p>Use <code>pydantic.Field()</code> to add parameter descriptions and defaults:</p> <pre><code>from pydantic import Field\n\n@tool(name=\"example\", description=\"Example tool\")\ndef example_tool(\n    # Required parameter with description\n    query: str = Field(description=\"The search query\"),\n\n    # Optional parameter with default\n    limit: int = Field(description=\"Result limit\", default=10),\n\n    # Optional parameter that can be None\n    filter: Optional[str] = Field(description=\"Optional filter\", default=None),\n) -&gt; str:\n    return f\"Query: {query}, Limit: {limit}\"\n</code></pre> <p>Important:</p> <ul> <li>Always provide descriptions for parameters</li> <li>Use <code>Field(...)</code> or <code>Field()</code> for required parameters (no default)</li> <li>Use <code>Field(default=value)</code> for optional parameters</li> <li>Descriptions help the LLM understand when and how to use the tool</li> </ul>"},{"location":"api/tool/#tool-confirmation","title":"Tool Confirmation","text":"<p>For destructive or sensitive operations, use <code>require_confirmation=True</code>:</p> <pre><code>import os\nfrom pydantic import Field\nfrom udspy import tool, ConfirmationRequired\n\n@tool(\n    name=\"delete_all_files\",\n    description=\"Delete all files in a directory\",\n    require_confirmation=True  # Requires confirmation\n)\ndef delete_all_files(\n    directory: str = Field(description=\"Directory path\")\n) -&gt; str:\n    \"\"\"Delete all files in directory (requires confirmation).\"\"\"\n    for file in os.listdir(directory):\n        os.remove(os.path.join(directory, file))\n    return f\"Deleted all files in {directory}\"\n\n# When ReAct tries to call this tool, it raises ConfirmationRequired on first call\n# After user approves, the tool executes normally\n</code></pre> <p>How it works:</p> <ol> <li>LLM decides to call the tool</li> <li>Tool function (wrapped with <code>@confirm_first</code>) raises <code>ConfirmationRequired</code> on first call</li> <li>User sees confirmation prompt: <code>\"Confirm execution of delete_all_files with args: {...}? (yes/no)\"</code></li> <li>User responds with <code>\"yes\"</code>, <code>\"no\"</code>, or modified arguments</li> <li>ReAct resumes execution based on user's response</li> <li>If approved, subsequent calls to the same tool with same args execute normally</li> </ol>"},{"location":"api/tool/#usage-with-react","title":"Usage with ReAct","text":"<pre><code>from pydantic import Field\nfrom udspy import InputField, OutputField, ReAct, Signature, tool\n\n# Define tools\n@tool(name=\"search\", description=\"Search for information\")\ndef search(query: str = Field(description=\"Search query\")) -&gt; str:\n    return f\"Results for: {query}\"\n\n@tool(name=\"calculate\", description=\"Perform calculations\")\ndef calculate(expression: str = Field(description=\"Math expression\")) -&gt; str:\n    return str(eval(expression))\n\n# Define signature\nclass QA(Signature):\n    \"\"\"Answer questions using tools.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\n# Create agent with tools\nagent = ReAct(QA, tools=[search, calculate])\n\n# Execute\nresult = agent(question=\"What is the population of Tokyo times 2?\")\n# Agent will:\n# 1. Call search(\"Tokyo population\")\n# 2. Call calculate(\"population * 2\")\n# 3. Synthesize final answer\n</code></pre>"},{"location":"api/tool/#usage-with-predict","title":"Usage with Predict","text":"<pre><code>from udspy import Predict, Signature, InputField, OutputField, tool\nfrom pydantic import Field\n\n@tool(name=\"get_weather\", description=\"Get current weather\")\ndef get_weather(city: str = Field(description=\"City name\")) -&gt; str:\n    return f\"Weather in {city}: Sunny, 72\u00b0F\"\n\nclass WeatherQuery(Signature):\n    \"\"\"Get weather information.\"\"\"\n    city: str = InputField()\n    weather: str = OutputField()\n\npredictor = Predict(WeatherQuery, tools=[get_weather])\nresult = predictor(city=\"San Francisco\")\n</code></pre>"},{"location":"api/tool/#dspy-compatibility","title":"DSPy Compatibility","text":"<p>The <code>Tool</code> class includes DSPy-compatible attributes:</p> <pre><code>@tool(name=\"search\", description=\"Search tool\")\ndef search(query: str = Field(description=\"Search query\")) -&gt; str:\n    return \"results\"\n\n# DSPy-style access\nprint(search.desc)  # Same as search.description: \"Search tool\"\nprint(search.args)  # Properties dict: {\"query\": {\"type\": \"string\", \"description\": \"Search query\", ...}}\n</code></pre>"},{"location":"api/tool/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Clear descriptions: Write clear, concise tool and parameter descriptions    <pre><code>@tool(\n    name=\"search_papers\",\n    description=\"Search academic papers by keyword, author, or topic\"\n)\n</code></pre></p> </li> <li> <p>Use Field() for all parameters: Always use <code>Field()</code> with descriptions    <pre><code>def search(\n    query: str = Field(description=\"Search query\"),\n    year: Optional[int] = Field(description=\"Publication year\", default=None)\n)\n</code></pre></p> </li> <li> <p>Require confirmation for destructive ops: Use <code>require_confirmation=True</code> <pre><code>@tool(name=\"delete\", description=\"Delete data\", require_confirmation=True)\n</code></pre></p> </li> <li> <p>Handle errors gracefully: Return error messages as strings    <pre><code>@tool(name=\"divide\", description=\"Divide numbers\")\ndef divide(a: float = Field(...), b: float = Field(...)) -&gt; str:\n    if b == 0:\n        return \"Error: Cannot divide by zero\"\n    return str(a / b)\n</code></pre></p> </li> <li> <p>Keep tools focused: Each tool should do one thing well    <pre><code># Good: Focused tool\n@tool(name=\"search_users\", description=\"Search for users\")\ndef search_users(query: str = Field(...)) -&gt; str: ...\n\n# Bad: Too many responsibilities\n@tool(name=\"user_management\", description=\"Manage all user operations\")\ndef user_management(action: str, query: str, data: dict) -&gt; str: ...\n</code></pre></p> </li> </ol>"},{"location":"api/tool/#type-annotations","title":"Type Annotations","text":"<pre><code>from typing import Callable, Any\nfrom udspy import Tool\n\n# Decorator signature\ndef tool(\n    name: str | None = None,\n    description: str | None = None,\n    *,\n    require_confirmation: bool = False,\n) -&gt; Callable[[Callable[..., Any]], Tool]: ...\n\n# Tool class\nclass Tool:\n    func: Callable[..., Any]\n    name: str\n    description: str\n    require_confirmation: bool\n    parameters: dict[str, dict[str, Any]]\n    desc: str  # Alias for description\n    args: dict[str, str]  # DSPy compatibility\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any: ...\n    async def acall(self, **kwargs: Any) -&gt; Any: ...\n    def to_openai_schema(self) -&gt; dict[str, Any]: ...\n</code></pre>"},{"location":"api/tool/#dynamic-tool-management","title":"Dynamic Tool Management","text":"<p>Tools can return module callbacks to dynamically add other tools during execution. This is useful for:</p> <ul> <li>Loading specialized tools on demand</li> <li>Adding tools based on user permissions</li> <li>Progressive tool discovery</li> </ul>"},{"location":"api/tool/#basic-example","title":"Basic Example","text":"<pre><code>from udspy import tool, module_callback\nfrom pydantic import Field\n\n# The tool that will be loaded dynamically\n@tool(name=\"calculator\", description=\"Perform calculations\")\ndef calculator(expression: str = Field(...)) -&gt; str:\n    return str(eval(expression, {\"__builtins__\": {}}, {}))\n\n# Meta-tool that loads the calculator\n@tool(name=\"load_calculator\", description=\"Load calculator tool\")\ndef load_calculator() -&gt; callable:\n    \"\"\"Load calculator dynamically.\"\"\"\n\n    @module_callback\n    def add_calculator(context):\n        # Get current tools (excluding built-ins)\n        current = [\n            t for t in context.module.tools.values()\n            if t.name not in (\"finish\", \"user_clarification\")\n        ]\n\n        # Add calculator\n        context.module.init_module(tools=current + [calculator])\n\n        return \"Calculator loaded successfully\"\n\n    return add_calculator\n\n# Use with ReAct\nfrom udspy import ReAct, Signature, InputField, OutputField\n\nclass Question(Signature):\n    \"\"\"Answer questions. Load tools if needed.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\nagent = ReAct(Question, tools=[load_calculator])\nresult = agent(question=\"What is 157 * 834?\")\n# Agent will:\n# 1. Call load_calculator() to get the calculator tool\n# 2. Use calculator(expression=\"157 * 834\")\n# 3. Return the answer\n</code></pre>"},{"location":"api/tool/#how-it-works","title":"How It Works","text":"<ol> <li>Tool returns callable: Instead of a string/value, return a function decorated with <code>@module_callback</code></li> <li>Callback receives context: Context has the module instance and execution state</li> <li>Callback modifies tools: Call <code>context.module.init_module(tools=[...])</code> to add/remove tools</li> <li>Callback returns observation: Return a string that appears in the trajectory</li> <li>Tools persist: Newly added tools remain available for the rest of execution</li> </ol>"},{"location":"api/tool/#category-based-loading","title":"Category-Based Loading","text":"<pre><code>@tool(name=\"load_tools\", description=\"Load tools by category\")\ndef load_tools(category: str = Field(...)) -&gt; callable:\n    @module_callback\n    def add_tools(context):\n        current = list(context.module.tools.values())\n\n        if category == \"math\":\n            new_tools = [calculator, statistics]\n        elif category == \"web\":\n            new_tools = [search, scrape]\n        else:\n            return f\"Unknown category: {category}\"\n\n        context.module.init_module(tools=current + new_tools)\n        return f\"Loaded {len(new_tools)} {category} tools\"\n\n    return add_tools\n</code></pre>"},{"location":"api/tool/#important-notes","title":"Important Notes","text":"<ul> <li>Must return string: Module callbacks must return a string (the observation)</li> <li>Async by default: Callbacks are called during tool execution</li> <li>Built-ins preserved: <code>finish</code> and user clarification are automatically kept</li> <li>Persistence: Tools remain available for the entire execution</li> </ul> <p>For complete documentation and examples, see: - Dynamic Tools Guide - Complete guide with examples</p> <p>Example Code: See <code>examples/dynamic_calculator.py</code> and <code>examples/dynamic_tools.py</code> in the GitHub repository</p>"},{"location":"api/tool/#see-also","title":"See Also","text":"<ul> <li>Dynamic Tools Guide - Dynamic tool management</li> <li>Confirmation API - Confirmation system and <code>@confirm_first</code> decorator</li> <li>ReAct API - Using tools with ReAct agents</li> <li>ReAct Examples - Tool usage examples</li> <li>Module API - Base module documentation</li> </ul>"},{"location":"architecture/adapters/","title":"Adapters","text":"<p>Adapters handle the translation between Signatures and LLM-specific message formats.</p>"},{"location":"architecture/adapters/#overview","title":"Overview","text":"<p>The <code>ChatAdapter</code> is responsible for:</p> <ol> <li>Converting signatures into system prompts</li> <li>Formatting inputs into user messages</li> <li>Parsing LLM completions into structured outputs</li> <li>Converting Pydantic models to tool schemas</li> </ol>"},{"location":"architecture/adapters/#usage","title":"Usage","text":"<pre><code>from udspy import ChatAdapter\n\nadapter = ChatAdapter()\n</code></pre> <p>Adapters are typically used internally by modules, but can be used directly:</p> <pre><code># Format instructions\ninstructions = adapter.format_instructions(signature)\n\n# Format inputs\nformatted = adapter.format_inputs(signature, {\"question\": \"What is AI?\"})\n\n# Parse outputs\noutputs = adapter.parse_outputs(signature, completion_text)\n</code></pre>"},{"location":"architecture/adapters/#custom-adapters","title":"Custom Adapters","text":"<p>You can create custom adapters by subclassing <code>ChatAdapter</code>:</p> <pre><code>class CustomAdapter(ChatAdapter):\n    def format_instructions(self, signature):\n        # Custom instruction formatting\n        return super().format_instructions(signature) + \"\\nBe creative!\"\n</code></pre> <p>See API: Adapters for detailed documentation.</p>"},{"location":"architecture/callbacks/","title":"Callback System","text":"<p>The callback system provides telemetry and monitoring capabilities for tracking LLM calls, module executions, and tool invocations. It's designed to be compatible with DSPy callbacks, enabling integration with observability tools like Opik, MLflow, and other platforms that support DSPy.</p>"},{"location":"architecture/callbacks/#overview","title":"Overview","text":"<p>Callbacks allow you to hook into the execution flow of udspy modules and capture events at key points:</p> <ul> <li>Module execution: When modules (Predict, ChainOfThought, ReAct) start and complete</li> <li>LLM calls: When OpenAI API calls are made and receive responses</li> <li>Tool invocations: When tools are called and return results</li> </ul> <p>This enables use cases like: - Logging and debugging - Performance monitoring - Cost tracking - Experiment tracking (MLflow, W&amp;B) - Observability platforms (Opik, Langfuse)</p>"},{"location":"architecture/callbacks/#core-components","title":"Core Components","text":""},{"location":"architecture/callbacks/#basecallback","title":"BaseCallback","text":"<p>The base class for all callback handlers. Subclass this and implement the handlers you need:</p> <pre><code>from udspy import BaseCallback\n\nclass LoggingCallback(BaseCallback):\n    def on_module_start(self, call_id, instance, inputs):\n        \"\"\"Called when a module's forward() method starts.\"\"\"\n        print(f\"Module {type(instance).__name__} started with inputs: {inputs}\")\n\n    def on_module_end(self, call_id, outputs, exception):\n        \"\"\"Called when a module's forward() method completes.\"\"\"\n        if exception:\n            print(f\"Module failed: {exception}\")\n        else:\n            print(f\"Module completed with outputs: {outputs}\")\n\n    def on_lm_start(self, call_id, instance, inputs):\n        \"\"\"Called when an LLM API call starts.\"\"\"\n        print(f\"LLM call started with model: {inputs.get('model')}\")\n\n    def on_lm_end(self, call_id, outputs, exception):\n        \"\"\"Called when an LLM API call completes.\"\"\"\n        if exception:\n            print(f\"LLM call failed: {exception}\")\n        else:\n            print(f\"LLM call completed\")\n\n    def on_tool_start(self, call_id, instance, inputs):\n        \"\"\"Called when a tool is invoked.\"\"\"\n        print(f\"Tool {instance.name} called with: {inputs}\")\n\n    def on_tool_end(self, call_id, outputs, exception):\n        \"\"\"Called when a tool invocation completes.\"\"\"\n        if exception:\n            print(f\"Tool failed: {exception}\")\n        else:\n            print(f\"Tool returned: {outputs}\")\n</code></pre>"},{"location":"architecture/callbacks/#callback-handler-parameters","title":"Callback Handler Parameters","text":"<p>All callback handlers receive consistent parameters:</p> <ul> <li>call_id (str): Unique identifier for this execution (useful for tracing nested calls)</li> <li>instance (Any): The module or tool instance being executed (only in <code>_start</code> handlers)</li> <li>inputs (dict): Input parameters (only in <code>_start</code> handlers)</li> <li>outputs (Any | None): Execution results (only in <code>_end</code> handlers)</li> <li>exception (Exception | None): Exception if execution failed (only in <code>_end</code> handlers)</li> </ul>"},{"location":"architecture/callbacks/#with_callbacks-decorator","title":"with_callbacks Decorator","text":"<p>The <code>@with_callbacks</code> decorator is applied to module and tool methods to enable callback execution. It:</p> <ol> <li>Retrieves active callbacks (global + instance-level)</li> <li>Generates a unique call_id</li> <li>Calls <code>on_*_start</code> handlers before execution</li> <li>Executes the wrapped method</li> <li>Calls <code>on_*_end</code> handlers after execution (even if exception occurs)</li> </ol> <p>The decorator handles both sync and async methods automatically.</p>"},{"location":"architecture/callbacks/#configuration","title":"Configuration","text":""},{"location":"architecture/callbacks/#global-callbacks","title":"Global Callbacks","text":"<p>Configure callbacks globally via <code>settings.configure()</code>:</p> <pre><code>import udspy\n\ncallback = LoggingCallback()\nudspy.settings.configure(\n    api_key=\"sk-...\",\n    model=\"gpt-4o-mini\",\n    callbacks=[callback]  # Applied to all modules and tools\n)\n</code></pre>"},{"location":"architecture/callbacks/#per-module-callbacks","title":"Per-Module Callbacks","text":"<p>Configure callbacks for specific module instances:</p> <pre><code>from udspy import Predict, Signature, InputField, OutputField\n\nclass QA(Signature):\n    question: str = InputField()\n    answer: str = OutputField()\n\n# This callback only applies to this predictor instance\npredictor = Predict(QA, callbacks=[LoggingCallback()])\n</code></pre>"},{"location":"architecture/callbacks/#context-specific-callbacks","title":"Context-Specific Callbacks","text":"<p>Use temporary callbacks within a context:</p> <pre><code># Temporarily override callbacks for specific operations\nwith udspy.settings.context(callbacks=[DebugCallback()]):\n    result = predictor(question=\"...\")\n</code></pre>"},{"location":"architecture/callbacks/#combining-callbacks","title":"Combining Callbacks","text":"<p>Callbacks are combined from multiple sources:</p> <pre><code># Global callbacks + instance callbacks are all executed\nudspy.settings.configure(callbacks=[GlobalCallback()])\npredictor = Predict(QA, callbacks=[InstanceCallback()])\n\n# Both GlobalCallback and InstanceCallback will be invoked\nresult = predictor(question=\"...\")\n</code></pre>"},{"location":"architecture/callbacks/#callback-execution-flow","title":"Callback Execution Flow","text":""},{"location":"architecture/callbacks/#module-execution","title":"Module Execution","text":"<pre><code>1. User calls predictor(question=\"...\")\n2. @with_callbacks on aexecute() is triggered\n3. on_module_start(call_id, predictor, {\"question\": \"...\", \"stream\": False, ...})\n4. Module executes (may call LLM and tools internally)\n5. on_module_end(call_id, outputs=Prediction(...), exception=None)\n6. Return result to user\n</code></pre>"},{"location":"architecture/callbacks/#llm-calls","title":"LLM Calls","text":"<pre><code>1. Module calls OpenAI API\n2. on_lm_start(call_id, module, {\"messages\": [...], \"model\": \"...\", ...})\n3. API request is made\n4. on_lm_end(call_id, outputs={\"response\": {...}}, exception=None)\n5. Response is processed\n</code></pre>"},{"location":"architecture/callbacks/#tool-invocations","title":"Tool Invocations","text":"<pre><code>1. Module calls tool.acall(...)\n2. @with_callbacks on acall() is triggered\n3. on_tool_start(call_id, tool, {\"query\": \"...\"})\n4. Tool function executes\n5. on_tool_end(call_id, outputs=\"...\", exception=None)\n6. Return result to module\n</code></pre>"},{"location":"architecture/callbacks/#nested-calls","title":"Nested Calls","text":"<p>The callback system tracks nested calls using <code>ACTIVE_CALL_ID</code> ContextVar:</p> <pre><code>Module.aexecute()  -&gt; call_id_1\n\u251c\u2500 on_module_start(call_id_1)\n\u251c\u2500 LLM call        -&gt; call_id_2 (parent: call_id_1)\n\u2502  \u251c\u2500 on_lm_start(call_id_2)\n\u2502  \u2514\u2500 on_lm_end(call_id_2)\n\u251c\u2500 Tool call       -&gt; call_id_3 (parent: call_id_1)\n\u2502  \u251c\u2500 on_tool_start(call_id_3)\n\u2502  \u2514\u2500 on_tool_end(call_id_3)\n\u2514\u2500 on_module_end(call_id_1)\n</code></pre>"},{"location":"architecture/callbacks/#error-handling","title":"Error Handling","text":"<p>Callbacks are designed to be non-invasive:</p> <ul> <li>Exceptions in callbacks are caught and logged - they don't break execution</li> <li>Failed callbacks don't affect module behavior - other callbacks still run</li> <li>Logging warnings are emitted when callbacks fail</li> </ul> <pre><code>class FaultyCallback(BaseCallback):\n    def on_module_start(self, call_id, instance, inputs):\n        raise ValueError(\"Oops!\")  # This won't break the module\n\n# Module still executes normally, warning is logged\npredictor = Predict(QA, callbacks=[FaultyCallback()])\nresult = predictor(question=\"...\")  # Works fine\n</code></pre>"},{"location":"architecture/callbacks/#dspy-compatibility","title":"DSPy Compatibility","text":"<p>The callback interface is designed to be compatible with DSPy's callback system. This means:</p> <ol> <li>Same handler names: <code>on_module_start</code>, <code>on_module_end</code>, <code>on_lm_start</code>, <code>on_lm_end</code>, <code>on_tool_start</code>, <code>on_tool_end</code></li> <li>Same parameter structure: <code>call_id</code>, <code>instance</code>, <code>inputs</code>, <code>outputs</code>, <code>exception</code></li> <li>Same execution model: Callbacks are invoked before/after operations</li> </ol> <p>Tools like Opik and MLflow that provide DSPy callbacks will work with udspy:</p> <pre><code># Example with Opik (hypothetical - check Opik docs for actual API)\nfrom opik import OpikCallback\n\nudspy.settings.configure(\n    api_key=\"sk-...\",\n    callbacks=[OpikCallback(project=\"my-project\")]\n)\n\n# All LLM calls and module executions are now tracked in Opik\n</code></pre>"},{"location":"architecture/callbacks/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Minimal overhead: Callbacks only add overhead if configured</li> <li>No overhead when disabled: If no callbacks are set, decorator short-circuits immediately</li> <li>Async-friendly: Callbacks don't block async execution</li> <li>Thread-safe: Uses ContextVar for proper isolation</li> </ul>"},{"location":"architecture/callbacks/#best-practices","title":"Best Practices","text":""},{"location":"architecture/callbacks/#1-use-global-callbacks-for-cross-cutting-concerns","title":"1. Use Global Callbacks for Cross-Cutting Concerns","text":"<pre><code># Logging, metrics, cost tracking\nudspy.settings.configure(callbacks=[\n    LoggingCallback(),\n    MetricsCallback(),\n    CostTracker()\n])\n</code></pre>"},{"location":"architecture/callbacks/#2-use-instance-callbacks-for-specific-monitoring","title":"2. Use Instance Callbacks for Specific Monitoring","text":"<pre><code># Monitor only critical paths\ncritical_predictor = Predict(ImportantTask, callbacks=[AlertCallback()])\n</code></pre>"},{"location":"architecture/callbacks/#3-use-context-callbacks-for-debugging","title":"3. Use Context Callbacks for Debugging","text":"<pre><code># Enable verbose logging only when debugging\nwith udspy.settings.context(callbacks=[VerboseDebugCallback()]):\n    result = complex_operation()\n</code></pre>"},{"location":"architecture/callbacks/#4-implement-selective-logging","title":"4. Implement Selective Logging","text":"<pre><code>class SelectiveCallback(BaseCallback):\n    def on_lm_start(self, call_id, instance, inputs):\n        # Only log expensive models\n        if inputs.get(\"model\") == \"gpt-4\":\n            logger.info(f\"Expensive model call: {call_id}\")\n</code></pre>"},{"location":"architecture/callbacks/#5-track-costs","title":"5. Track Costs","text":"<pre><code>class CostTracker(BaseCallback):\n    def __init__(self):\n        self.total_cost = 0.0\n\n    def on_lm_end(self, call_id, outputs, exception):\n        if outputs and \"response\" in outputs:\n            # Calculate cost based on tokens\n            # (This is simplified - real implementation would parse response)\n            self.total_cost += 0.0001  # Example cost\n</code></pre>"},{"location":"architecture/callbacks/#common-patterns","title":"Common Patterns","text":""},{"location":"architecture/callbacks/#requestresponse-logging","title":"Request/Response Logging","text":"<pre><code>class RequestResponseLogger(BaseCallback):\n    def on_module_start(self, call_id, instance, inputs):\n        logger.info(f\"[{call_id}] Request: {inputs}\")\n\n    def on_module_end(self, call_id, outputs, exception):\n        logger.info(f\"[{call_id}] Response: {outputs}\")\n</code></pre>"},{"location":"architecture/callbacks/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>import time\n\nclass PerformanceMonitor(BaseCallback):\n    def __init__(self):\n        self.start_times = {}\n\n    def on_module_start(self, call_id, instance, inputs):\n        self.start_times[call_id] = time.time()\n\n    def on_module_end(self, call_id, outputs, exception):\n        duration = time.time() - self.start_times.pop(call_id, time.time())\n        logger.info(f\"Module took {duration:.2f}s\")\n</code></pre>"},{"location":"architecture/callbacks/#error-tracking","title":"Error Tracking","text":"<pre><code>class ErrorTracker(BaseCallback):\n    def __init__(self):\n        self.errors = []\n\n    def on_module_end(self, call_id, outputs, exception):\n        if exception:\n            self.errors.append({\n                \"call_id\": call_id,\n                \"error\": str(exception),\n                \"timestamp\": time.time()\n            })\n</code></pre>"},{"location":"architecture/callbacks/#migration-from-dspy","title":"Migration from DSPy","text":"<p>If you're using DSPy callbacks, migration is straightforward:</p> <pre><code># DSPy code\nimport dspy\ndspy.settings.configure(callbacks=[MyCallback()])\n\n# udspy code - exactly the same!\nimport udspy\nudspy.settings.configure(callbacks=[MyCallback()])\n</code></pre> <p>The callback interface is identical, so existing DSPy callbacks should work without modification.</p>"},{"location":"architecture/confirmation/","title":"Confirmation System Architecture","text":"<p>The confirmation system provides human-in-the-loop (HITL) capabilities for udspy, allowing agents and tools to pause execution and request human approval, clarification, or feedback.</p>"},{"location":"architecture/confirmation/#overview","title":"Overview","text":"<p>The confirmation system is built on three core principles:</p> <ol> <li>Exception-based Flow Control: Uses Python exceptions (<code>ConfirmationRequired</code>, <code>ConfirmationRejected</code>) to pause execution naturally</li> <li>Stateful Resumption: Saves module state in the exception to enable resuming from the exact point where confirmation was requested</li> <li>Thread-safe Context: Uses Python's <code>contextvars</code> for isolated, thread-safe confirmation tracking</li> </ol>"},{"location":"architecture/confirmation/#core-components","title":"Core Components","text":""},{"location":"architecture/confirmation/#1-exceptions","title":"1. Exceptions","text":""},{"location":"architecture/confirmation/#confirmationrequired","title":"<code>ConfirmationRequired</code>","text":"<p>Raised when human input is needed to proceed. Contains:</p> <pre><code>class ConfirmationRequired(Exception):\n    question: str              # Question to ask the user\n    confirmation_id: str       # Unique ID for this confirmation\n    tool_call: ToolCall | None # Optional tool call information\n    context: dict[str, Any]    # Module-specific state for resumption\n</code></pre> <p>Usage scenarios: - Tools decorated with <code>@confirm_first</code> before execution - ReAct agent's user clarification tool for clarification - Custom code needing human interaction</p>"},{"location":"architecture/confirmation/#confirmationrejected","title":"<code>ConfirmationRejected</code>","text":"<p>Raised when user explicitly rejects a confirmation. Distinguishes \"user said no\" from \"pending approval\".</p> <pre><code>class ConfirmationRejected(Exception):\n    message: str\n    confirmation_id: str\n    tool_call: ToolCall | None\n</code></pre>"},{"location":"architecture/confirmation/#2-decorator-confirm_first","title":"2. Decorator: <code>@confirm_first</code>","text":"<p>Makes any function require confirmation before execution:</p> <pre><code>from udspy import confirm_first\n\n@confirm_first\ndef delete_database(name: str) -&gt; str:\n    # Dangerous operation\n    return f\"Deleted {name}\"\n\n# First call raises ConfirmationRequired\ntry:\n    delete_database(\"production\")\nexcept ConfirmationRequired as e:\n    # Get user approval\n    respond_to_confirmation(e.confirmation_id, approved=True)\n\n# Second call executes\nresult = delete_database(\"production\")\n</code></pre> <p>How it works: 1. Generates stable confirmation ID from <code>function_name:hash(args)</code> 2. Checks confirmation context for approval status 3. If not approved: raises <code>ConfirmationRequired</code> 4. If approved: executes function and clears confirmation 5. If rejected: raises <code>ConfirmationRejected</code></p>"},{"location":"architecture/confirmation/#3-confirmation-context","title":"3. Confirmation Context","text":"<p>Thread-safe and async-safe state storage using <code>contextvars</code>:</p> <pre><code>_confirmation_context: ContextVar[dict[str, Any] | None] = ContextVar(\n    \"confirmation_context\", default=None\n)\n</code></pre> <p>Each confirmation is stored as: <pre><code>{\n    \"confirmation_id\": {\n        \"approved\": bool,\n        \"data\": Any,  # Modified arguments if edited\n        \"status\": str  # \"pending\", \"approved\", \"rejected\", \"edited\", \"feedback\"\n    }\n}\n</code></pre></p> <p>Benefits: - Isolated per thread/async task - No global state contamination - Works in concurrent environments</p>"},{"location":"architecture/confirmation/#4-state-management-functions","title":"4. State Management Functions","text":"<pre><code># Set confirmation response\nrespond_to_confirmation(\n    confirmation_id: str,\n    approved: bool = True,\n    data: Any = None,\n    status: str | None = None\n)\n\n# Check confirmation status\nstatus = get_confirmation_status(confirmation_id)  # \"pending\" | \"approved\" | \"rejected\"\n\n# Cleanup\nclear_confirmation(confirmation_id)\nclear_all_confirmations()\n</code></pre>"},{"location":"architecture/confirmation/#integration-with-modules","title":"Integration with Modules","text":""},{"location":"architecture/confirmation/#module-resume-pattern","title":"Module Resume Pattern","text":"<p>Modules support two resumption patterns:</p>"},{"location":"architecture/confirmation/#pattern-1-explicit-aresume-method","title":"Pattern 1: Explicit <code>aresume()</code> Method","text":"<pre><code>try:\n    result = await agent.aforward(question=\"Delete files\")\nexcept ConfirmationRequired as e:\n    result = await agent.aresume(\"yes\", e)\n</code></pre> <p>Used when: - Need to handle confirmation differently than normal flow - Want explicit control over resumption</p>"},{"location":"architecture/confirmation/#pattern-2-loop-based-with-resume_state","title":"Pattern 2: Loop-based with <code>resume_state</code>","text":"<pre><code>from udspy import ResumeState\n\nresume_state = None\n\nwhile True:\n    try:\n        result = await agent.aforward(\n            question=\"Delete files\",\n            resume_state=resume_state\n        )\n        break\n    except ConfirmationRequired as e:\n        user_response = input(f\"{e.question} (yes/no): \")\n        resume_state = ResumeState(e, user_response)\n</code></pre> <p>Used when: - Multiple confirmations may be needed - Want uniform handling loop - Building interactive CLIs or web APIs</p>"},{"location":"architecture/confirmation/#how-resume-works","title":"How Resume Works","text":"<p>When <code>resume_state</code> is provided:</p> <ol> <li><code>aforward()</code> detects <code>resume_state is not None</code></li> <li>Delegates to <code>aresume(user_response, resume_state)</code></li> <li>Module extracts saved state from exception's <code>.context</code></li> <li>Continues execution from saved point</li> </ol> <p>Example state in ReAct: <pre><code>{\n    \"trajectory\": {...},      # Reasoning/action history\n    \"iteration\": 3,          # Which iteration to resume from\n    \"input_args\": {...},     # Original input arguments\n}\n</code></pre></p>"},{"location":"architecture/confirmation/#tool-confirmation-flow","title":"Tool Confirmation Flow","text":""},{"location":"architecture/confirmation/#creating-confirmable-tools","title":"Creating Confirmable Tools","text":"<pre><code>from udspy import tool\n\n@tool(name=\"delete_file\", require_confirmation=True)\ndef delete_file(path: str) -&gt; str:\n    os.remove(path)\n    return f\"Deleted {path}\"\n</code></pre> <p>Internally, this wraps the function with <code>@confirm_first</code>.</p>"},{"location":"architecture/confirmation/#execution-flow","title":"Execution Flow","text":"<pre><code>1. ReAct decides to call delete_file\n   \u2193\n2. Tool.__call__() is invoked\n   \u2193\n3. @confirm_first decorator checks context\n   \u2193\n4. No approval found \u2192 raises ConfirmationRequired\n   \u2193\n5. ReAct catches exception, saves state\n   \u2193\n6. User responds via agent.aresume() or resume_state\n   \u2193\n7. respond_to_confirmation() marks approved\n   \u2193\n8. ReAct resumes with pending_tool_call\n   \u2193\n9. @confirm_first sees approval, executes\n   \u2193\n10. Result added to trajectory\n</code></pre>"},{"location":"architecture/confirmation/#design-rationale","title":"Design Rationale","text":""},{"location":"architecture/confirmation/#why-exceptions","title":"Why Exceptions?","text":"<p>Alternatives considered: - Callbacks: More complex, harder to reason about control flow - Async generators with yield: Breaks module composability - Return sentinel values: Ambiguous, requires checking every return</p> <p>Why exceptions work: - Natural suspension of call stack - Carries state in exception object - Explicit handling with try/except - Composes well with async/await</p>"},{"location":"architecture/confirmation/#why-stable-ids","title":"Why Stable IDs?","text":"<p>Generated from <code>function_name:hash(args)</code> to enable: - Same function call to resume after approval - Idempotent confirmation (multiple attempts use same ID) - Deterministic behavior for testing</p>"},{"location":"architecture/confirmation/#why-two-exception-types","title":"Why Two Exception Types?","text":"<ul> <li><code>ConfirmationRequired</code>: Suspends execution, waiting for response</li> <li><code>ConfirmationRejected</code>: Terminates execution, user said \"no\"</li> </ul> <p>Allows code to distinguish and handle differently: <pre><code>try:\n    result = agent(question=\"Delete production data\")\nexcept ConfirmationRequired:\n    # Still possible to proceed with approval\n    pass\nexcept ConfirmationRejected:\n    # User explicitly denied - different handling\n    log_denial()\n</code></pre></p>"},{"location":"architecture/confirmation/#thread-safety","title":"Thread Safety","text":""},{"location":"architecture/confirmation/#guarantees","title":"Guarantees","text":"<ol> <li>Isolated contexts: Each thread/task has its own confirmation state</li> <li>No race conditions: <code>ContextVar</code> provides isolation</li> <li>Concurrent safe: Multiple agents can run simultaneously</li> </ol>"},{"location":"architecture/confirmation/#example","title":"Example","text":"<pre><code>import asyncio\n\nasync def run_agent(agent_id: int):\n    agent = ReAct(...)\n    try:\n        return await agent.aforward(question=f\"Task {agent_id}\")\n    except ConfirmationRequired as e:\n        # Each agent has its own confirmation context\n        respond_to_confirmation(e.confirmation_id, approved=True)\n        return await agent.aforward(\n            question=f\"Task {agent_id}\",\n            resume_state=e,\n            user_response=\"yes\"\n        )\n\n# Run 10 agents concurrently - each isolated\nresults = await asyncio.gather(*[run_agent(i) for i in range(10)])\n</code></pre>"},{"location":"architecture/confirmation/#best-practices","title":"Best Practices","text":""},{"location":"architecture/confirmation/#1-always-clean-up","title":"1. Always Clean Up","text":"<p>Confirmations are auto-cleared on success, but clean up manually if needed:</p> <pre><code>try:\n    result = agent(question=\"...\", resume_state=state)\nexcept Exception:\n    clear_all_confirmations()  # Reset on error\n    raise\n</code></pre>"},{"location":"architecture/confirmation/#2-limit-confirmation-rounds","title":"2. Limit Confirmation Rounds","text":"<p>Prevent infinite loops:</p> <pre><code>from udspy import ResumeState\n\nMAX_CONFIRMATIONS = 5\nfor attempt in range(MAX_CONFIRMATIONS):\n    try:\n        result = agent(question=\"...\", resume_state=state)\n        break\n    except ConfirmationRequired as e:\n        response = get_user_input(e.question)\n        state = ResumeState(e, response)\nelse:\n    raise RuntimeError(\"Too many confirmation requests\")\n</code></pre>"},{"location":"architecture/confirmation/#3-validate-user-responses","title":"3. Validate User Responses","text":"<pre><code>except ConfirmationRequired as e:\n    response = input(f\"{e.question} (yes/no/edit): \").lower()\n\n    if response not in (\"yes\", \"no\", \"edit\"):\n        print(\"Invalid response. Please enter yes, no, or edit.\")\n        continue\n\n    state = e\n    user_response = response\n</code></pre>"},{"location":"architecture/confirmation/#4-provide-context-to-users","title":"4. Provide Context to Users","text":"<pre><code>except ConfirmationRequired as e:\n    print(f\"\\n\u26a0\ufe0f  Confirmation Required\")\n    print(f\"   Question: {e.question}\")\n\n    if e.tool_call:\n        print(f\"   Tool: {e.tool_call.name}\")\n        print(f\"   Args: {json.dumps(e.tool_call.args, indent=2)}\")\n\n    response = input(\"\\nYour response: \")\n</code></pre>"},{"location":"architecture/confirmation/#common-patterns","title":"Common Patterns","text":""},{"location":"architecture/confirmation/#pattern-web-api-with-session-state","title":"Pattern: Web API with Session State","text":"<pre><code># POST /agent/start\n@app.post(\"/agent/start\")\nasync def start_agent(request: AgentRequest):\n    try:\n        result = await agent.aforward(question=request.question)\n        return {\"status\": \"completed\", \"result\": result.answer}\n    except ConfirmationRequired as e:\n        # Save state in session/DB\n        session_id = save_state(e)\n        return {\n            \"status\": \"confirmation_required\",\n            \"session_id\": session_id,\n            \"question\": e.question,\n            \"tool_call\": e.tool_call\n        }\n\n# POST /agent/resume\n@app.post(\"/agent/resume\")\nasync def resume_agent(request: ResumeRequest):\n    from udspy import ResumeState\n\n    exception = load_state(request.session_id)\n    resume_state = ResumeState(exception, request.user_response)\n    result = await agent.aforward(\n        question=exception.context[\"original_question\"],\n        resume_state=resume_state\n    )\n    return {\"status\": \"completed\", \"result\": result.answer}\n</code></pre>"},{"location":"architecture/confirmation/#pattern-interactive-cli","title":"Pattern: Interactive CLI","text":"<pre><code>from udspy import ResumeState\n\ndef run_interactive(agent, question):\n    resume_state = None\n\n    while True:\n        try:\n            result = agent(\n                question=question,\n                resume_state=resume_state\n            )\n            print(f\"\\n\u2713 {result.answer}\")\n            return result\n\n        except ConfirmationRequired as e:\n            print(f\"\\n\u26a0\ufe0f  {e.question}\")\n\n            if e.tool_call:\n                print(f\"   Tool: {e.tool_call.name}({e.tool_call.args})\")\n\n            response = input(\"\\n[yes/no/edit]: \").strip()\n\n            if response == \"edit\" and e.tool_call:\n                print(\"Enter new args as JSON:\")\n                new_args = input(\"&gt; \")\n                resume_state = ResumeState(e, new_args)\n            else:\n                resume_state = ResumeState(e, response)\n\n        except ConfirmationRejected as e:\n            print(f\"\\n\u2717 Rejected: {e.message}\")\n            return None\n</code></pre>"},{"location":"architecture/confirmation/#pattern-batch-processing-with-selective-confirmation","title":"Pattern: Batch Processing with Selective Confirmation","text":"<pre><code>from udspy import ResumeState\n\nasync def process_batch(items, agent):\n    results = []\n\n    for item in items:\n        resume_state = None\n        attempts = 0\n\n        while attempts &lt; 3:\n            try:\n                result = await agent.aforward(\n                    question=f\"Process {item}\",\n                    resume_state=resume_state\n                )\n                results.append(result)\n                break\n\n            except ConfirmationRequired as e:\n                # Auto-approve low-risk items\n                if is_low_risk(e.tool_call):\n                    response = \"yes\"\n                else:\n                    # Human review for high-risk\n                    response = await get_human_approval(e)\n\n                resume_state = ResumeState(e, response)\n                attempts += 1\n\n    return results\n</code></pre>"},{"location":"architecture/confirmation/#see-also","title":"See Also","text":"<ul> <li>Confirmation API Reference - API documentation</li> <li>Confirmation Examples - Practical examples</li> <li>ReAct Module - ReAct integration with confirmations</li> <li>Tool API - Creating confirmable tools</li> </ul>"},{"location":"architecture/decisions/","title":"Architectural Decision Records (ADR)","text":"<p>This document tracks major architectural decisions made in udspy, presented chronologically with context, rationale, and consequences.</p>"},{"location":"architecture/decisions/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Initial Project Setup (2025-10-24)</li> <li>Context Manager for Settings (2025-10-24)</li> <li>Chain of Thought Module (2025-10-24)</li> <li>Human-in-the-Loop with Confirmation System (2025-10-25)</li> <li>ReAct Agent Module (2025-10-25)</li> <li>Unified Module Execution Pattern (aexecute) (2025-10-25)</li> <li>Automatic Retry on Parse Errors (2025-10-29)</li> <li>Module Callbacks and Dynamic Tool Management (2025-10-31)</li> <li>History Management with System Prompts (2025-10-31)</li> <li>LM Callable Interface with String Prompts (2025-10-31)</li> </ol>"},{"location":"architecture/decisions/#adr-001-initial-project-setup","title":"ADR-001: Initial Project Setup","text":"<p>Date: 2025-10-24</p> <p>Status: Accepted</p>"},{"location":"architecture/decisions/#context","title":"Context","text":"<p>Needed a minimal library for LLM-powered applications in resource-constrained environments, specifically for Baserow's AI assistant where ~200MB dependencies are prohibitive.</p>"},{"location":"architecture/decisions/#decision","title":"Decision","text":"<p>Build a lightweight library with: - Native OpenAI tool calling as the primary approach - Minimal dependencies (~10MB: <code>openai</code> + <code>pydantic</code>) - Streaming support for reasoning and output fields - Async-first architecture - Modern Python tooling (uv, ruff, justfile)</p> <p>Note: Heavily inspired by DSPy's excellent abstractions and API patterns.</p>"},{"location":"architecture/decisions/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"architecture/decisions/#1-native-tool-calling","title":"1. Native Tool Calling","text":"<p>Use OpenAI's native function calling API directly as the primary approach.</p> <p>Rationale: - OpenAI's tool calling is optimized and well-tested - Reduces complexity - no need for multi-provider adapter layer - Forward compatible with future OpenAI improvements - Works with any OpenAI-compatible provider (Together, Ollama, etc.) - Sufficient for Baserow's AI assistant needs</p> <p>Trade-offs: - Couples to OpenAI's API format (acceptable for our use case) - Limited to OpenAI-compatible providers</p>"},{"location":"architecture/decisions/#2-minimal-dependencies","title":"2. Minimal Dependencies","text":"<p>Only <code>openai</code> and <code>pydantic</code> in core dependencies.</p> <p>Rationale: - Keeps the library lightweight (~10MB) - Reduces potential dependency conflicts in Baserow - Faster installation and lower memory usage - Suitable for serverless, edge, and embedded deployments</p> <p>Trade-offs: - Limited to OpenAI-compatible providers - No multi-provider abstraction layer</p>"},{"location":"architecture/decisions/#3-pydantic-v2","title":"3. Pydantic v2","text":"<p>Use Pydantic v2 for all models and validation.</p> <p>Rationale: - Modern, fast, well-maintained - Excellent JSON schema generation for tools - Built-in validation and type coercion - Great developer experience with IDE support</p> <p>Trade-offs: - Requires Python 3.7+ (we target 3.11+)</p>"},{"location":"architecture/decisions/#4-streaming-architecture","title":"4. Streaming Architecture","text":"<p>Async-first design using Python's async/await.</p> <p>Rationale: - Python's async is the standard for I/O-bound operations - Native support from OpenAI SDK - Composable with Baserow's async infrastructure - First-class support for streaming reasoning and outputs</p> <p>Trade-offs: - Requires async runtime (asyncio) - Steeper learning curve for beginners</p>"},{"location":"architecture/decisions/#5-module-abstraction","title":"5. Module Abstraction","text":"<p>Modules compose via Python class inheritance.</p> <p>Rationale: - Familiar Python patterns (no custom DSL) - Good IDE and type checker support - Signatures define I/O contracts using Pydantic models - Predict is the core primitive for LLM calls</p> <p>Trade-offs: - Requires more explicit code vs meta-programming approaches - Less abstraction = more boilerplate for advanced use cases</p>"},{"location":"architecture/decisions/#consequences","title":"Consequences","text":"<p>Benefits: - Small memory footprint (~10MB) - Works in resource-constrained environments (Baserow AI assistant) - Simple, maintainable codebase - Compatible with any OpenAI-compatible provider - Fast installation and startup</p> <p>Trade-offs: - Limited to OpenAI-compatible providers - No built-in optimizers or teleprompters - Fewer abstractions = more manual work for complex scenarios</p>"},{"location":"architecture/decisions/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Use existing frameworks: Larger footprints, more dependencies</li> <li>Build from scratch: Chose this - start minimal, add what's needed</li> </ul>"},{"location":"architecture/decisions/#adr-002-context-manager-for-settings","title":"ADR-002: Context Manager for Settings","text":"<p>Date: 2025-10-24</p> <p>Status: Accepted</p>"},{"location":"architecture/decisions/#context_1","title":"Context","text":"<p>Need to support different API keys and models in different contexts (e.g., multi-tenant apps, different users, testing scenarios, concurrent async operations).</p>"},{"location":"architecture/decisions/#decision_1","title":"Decision","text":"<p>Implement thread-safe context manager using Python's <code>contextvars</code> module:</p> <pre><code>from udspy import LM\n\n# Global settings\nglobal_lm = LM(model=\"gpt-4o-mini\", api_key=\"global-key\")\nudspy.settings.configure(lm=global_lm)\n\n# Temporary override in context\nuser_lm = LM(model=\"gpt-4\", api_key=\"user-key\")\nwith udspy.settings.context(lm=user_lm):\n    result = predictor(question=\"...\")  # Uses user-key and gpt-4\n\n# Back to global settings\nresult = predictor(question=\"...\")  # Uses global-key and gpt-4o-mini\n</code></pre>"},{"location":"architecture/decisions/#implementation-details","title":"Implementation Details","text":"<ul> <li>Added <code>ContextVar</code> fields to <code>Settings</code> class for each configurable attribute</li> <li>Properties now check context first, then fall back to global settings</li> <li>Context manager saves/restores context state using try/finally</li> <li>Proper cleanup ensures no context leakage</li> </ul>"},{"location":"architecture/decisions/#key-features","title":"Key Features","text":"<ol> <li>Thread-Safe: Uses <code>ContextVar</code> for thread-safe context isolation</li> <li>Nestable: Contexts can be nested with proper inheritance</li> <li>Comprehensive: Supports overriding lm, callbacks, and any kwargs</li> <li>Clean API: Simple context manager interface with LM instances</li> <li>Flexible: Use different LM providers per context</li> </ol>"},{"location":"architecture/decisions/#use-cases","title":"Use Cases","text":"<ol> <li> <p>Multi-tenant applications: Different API keys per user    <pre><code>user_lm = LM(model=\"gpt-4o-mini\", api_key=user.api_key)\nwith udspy.settings.context(lm=user_lm):\n    result = predictor(question=user.question)\n</code></pre></p> </li> <li> <p>Model selection per request: Use different models for different tasks    <pre><code>powerful_lm = LM(model=\"gpt-4\", api_key=api_key)\nwith udspy.settings.context(lm=powerful_lm):\n    result = expensive_predictor(question=complex_question)\n</code></pre></p> </li> <li> <p>Testing: Isolate test settings without affecting global state    <pre><code>test_lm = LM(model=\"gpt-4o-mini\", api_key=\"sk-test\")\nwith udspy.settings.context(lm=test_lm, temperature=0.0):\n    assert predictor(question=\"2+2\").answer == \"4\"\n</code></pre></p> </li> <li> <p>Async operations: Safe concurrent operations with different settings    <pre><code>async def handle_user(user):\n    user_lm = LM(model=\"gpt-4o-mini\", api_key=user.api_key)\n    with udspy.settings.context(lm=user_lm):\n        async for chunk in streaming_predictor.stream(...):\n            yield chunk\n</code></pre></p> </li> </ol>"},{"location":"architecture/decisions/#consequences_1","title":"Consequences","text":"<p>Benefits: - Clean separation of concerns (global vs context-specific settings) - No need to pass settings through function parameters - Thread-safe and asyncio task-safe for concurrent operations - Flexible and composable</p> <p>Trade-offs: - Slight complexity increase in Settings class - Context variables have a small performance overhead (negligible) - Must remember to use context manager (but gracefully degrades to global settings)</p>"},{"location":"architecture/decisions/#alternatives-considered_1","title":"Alternatives Considered","text":"<ul> <li>Dependency Injection: More verbose, harder to use</li> <li>Environment Variables: Not dynamic enough for multi-tenant use cases</li> <li>Pass settings everywhere: Too cumbersome</li> </ul>"},{"location":"architecture/decisions/#migration-guide","title":"Migration Guide","text":"<p>No migration needed - feature is additive and backwards compatible.</p>"},{"location":"architecture/decisions/#adr-003-chain-of-thought-module","title":"ADR-003: Chain of Thought Module","text":"<p>Date: 2025-10-24</p> <p>Status: Accepted</p>"},{"location":"architecture/decisions/#context_2","title":"Context","text":"<p>Chain of Thought (CoT) is a proven prompting technique that improves LLM reasoning by explicitly requesting step-by-step thinking. Research shows ~25-30% accuracy improvement on math and reasoning tasks (Wei et al., 2022).</p>"},{"location":"architecture/decisions/#decision_2","title":"Decision","text":"<p>Implement <code>ChainOfThought</code> module that automatically adds a reasoning field to any signature:</p> <pre><code>class QA(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\n# Automatically extends to: question -&gt; reasoning, answer\ncot = ChainOfThought(QA)\nresult = cot(question=\"What is 15 * 23?\")\n\nprint(result.reasoning)  # Shows step-by-step calculation\nprint(result.answer)     # \"345\"\n</code></pre>"},{"location":"architecture/decisions/#implementation-approach","title":"Implementation Approach","text":"<p>Unlike DSPy which uses a <code>signature.prepend()</code> method, udspy takes a simpler approach:</p> <ol> <li>Extract fields from original signature</li> <li>Create extended outputs with reasoning prepended: <code>{\"reasoning\": str, **original_outputs}</code></li> <li>Use make_signature to create new signature dynamically</li> <li>Wrap in Predict with the extended signature</li> </ol> <p>This approach: - Doesn't require adding prepend/insert methods to Signature - Leverages existing <code>make_signature</code> utility - Keeps ChainOfThought as a pure Module wrapper - Only ~45 lines of code</p>"},{"location":"architecture/decisions/#key-features_1","title":"Key Features","text":"<ol> <li>Automatic reasoning field: No manual signature modification needed</li> <li>Customizable description: Override reasoning field description</li> <li>Works with any signature: Single or multiple outputs</li> <li>Transparent: Reasoning is always accessible in results</li> <li>Configurable: All Predict parameters (model, temperature, tools) supported</li> </ol>"},{"location":"architecture/decisions/#research-evidence","title":"Research Evidence","text":"<p>Chain of Thought prompting improves performance on: - Math: ~25-30% accuracy improvement (Wei et al., 2022) - Reasoning: Significant gains on logic puzzles - Multi-step: Better at complex multi-hop reasoning - Transparency: Shows reasoning for verification</p>"},{"location":"architecture/decisions/#use-cases_1","title":"Use Cases","text":"<ol> <li> <p>Math and calculation <pre><code>cot = ChainOfThought(QA, temperature=0.0)\nresult = cot(question=\"What is 157 * 234?\")\n</code></pre></p> </li> <li> <p>Analysis and decision-making <pre><code>class Decision(Signature):\n    scenario: str = InputField()\n    decision: str = OutputField()\n    justification: str = OutputField()\n\ndecider = ChainOfThought(Decision)\n</code></pre></p> </li> <li> <p>Educational applications: Show work/reasoning</p> </li> <li>High-stakes decisions: Require explicit justification</li> <li>Debugging: Understand why LLM made specific choices</li> </ol>"},{"location":"architecture/decisions/#consequences_2","title":"Consequences","text":"<p>Benefits: - Improved accuracy on reasoning tasks - Transparent reasoning process - Easy to verify correctness - Simple API (just wrap any signature) - Minimal code overhead</p> <p>Trade-offs: - Increased token usage (~2-3x for simple tasks) - Slightly higher latency - Not always needed for simple factual queries - Reasoning quality depends on model capability</p>"},{"location":"architecture/decisions/#alternatives-considered_2","title":"Alternatives Considered","text":"<ul> <li>Prompt Engineering: Less reliable than structured reasoning field</li> <li>Tool-based Reasoning: Too heavyweight for simple reasoning</li> <li>Custom Signature per Use: Too much boilerplate</li> </ul>"},{"location":"architecture/decisions/#future-considerations","title":"Future Considerations","text":"<ol> <li>Streaming support: StreamingChainOfThought for incremental reasoning</li> <li>Few-shot examples: Add example reasoning patterns to improve quality</li> <li>Verification: Automatic reasoning quality checks</li> <li>Caching: Built-in caching for repeated queries</li> </ol>"},{"location":"architecture/decisions/#migration-guide_1","title":"Migration Guide","text":"<p>Feature is additive - no migration needed.</p>"},{"location":"architecture/decisions/#adr-004-human-in-the-loop-with-confirmation-system","title":"ADR-004: Human-in-the-Loop with Confirmation System","text":"<p>Date: 2025-10-25 (Updated: 2025-10-31)</p> <p>Status: Accepted</p>"},{"location":"architecture/decisions/#context_3","title":"Context","text":"<p>Many agent applications require human approval for certain actions (e.g., deleting files, sending emails, making purchases). We needed a clean way to suspend execution, ask for user input, and resume where we left off. The system must support: - Multiple confirmation rounds (clarifications, edits, iterations) - State preservation for resumption - Thread-safe concurrent operations - Integration with ReAct agent trajectories</p>"},{"location":"architecture/decisions/#decision_3","title":"Decision","text":"<p>Implement exception-based confirmation system with: - Exceptions for control flow: <code>ConfirmationRequired</code>, <code>ConfirmationRejected</code> - @confirm_first decorator: Wraps functions to require confirmation - ResumeState: Container for resuming execution after confirmation - Type-safe status tracking: Literal types for compile-time validation - Thread-safe context: Uses <code>contextvars</code> for isolated state</p> <pre><code>from udspy import (\n    confirm_first,\n    ConfirmationRequired,\n    ConfirmationRejected,\n    ResumeState,\n    respond_to_confirmation\n)\n\n@confirm_first\ndef delete_file(path: str) -&gt; str:\n    os.remove(path)\n    return f\"Deleted {path}\"\n\n# Interactive loop pattern\nresume_state = None\nwhile True:\n    try:\n        result = delete_file(\"/important.txt\", resume_state=resume_state)\n        break\n    except ConfirmationRequired as e:\n        response = input(f\"{e.question} (yes/no): \")\n        resume_state = ResumeState(e, response)\n    except ConfirmationRejected as e:\n        print(f\"Rejected: {e.message}\")\n        break\n</code></pre>"},{"location":"architecture/decisions/#implementation-details_1","title":"Implementation Details","text":"<ol> <li>Stable Confirmation IDs: Generated from <code>function_name:hash(args)</code> for idempotent resumption</li> <li>Type-safe Status: <code>ConfirmationStatus = Literal[\"pending\", \"approved\", \"rejected\", \"edited\", \"feedback\"]</code></li> <li>ApprovalData TypedDict: Structured approval data with type safety</li> <li>ResumeState Container: Combines exception + user response for clean resumption API</li> <li>Context Storage: Thread-safe <code>ContextVar[dict[str, ApprovalData]]</code></li> <li>Tool Integration: <code>check_tool_confirmation()</code> for tool-level confirmations</li> <li>Automatic Cleanup: Confirmations cleared after successful execution</li> </ol>"},{"location":"architecture/decisions/#key-types","title":"Key Types","text":"<pre><code># Type-safe status\nConfirmationStatus = Literal[\"pending\", \"approved\", \"rejected\", \"edited\", \"feedback\"]\n\n# Typed approval data\nclass ApprovalData(TypedDict, total=False):\n    approved: bool\n    data: dict[str, Any] | None\n    status: ConfirmationStatus\n\n# Exception classes\nclass ConfirmationRequired(Exception):\n    question: str\n    confirmation_id: str\n    tool_call: ToolCall | None\n    context: dict[str, Any]  # Module state for resumption\n\nclass ConfirmationRejected(Exception):\n    message: str\n    confirmation_id: str\n    tool_call: ToolCall | None\n\n# Resume state container\nclass ResumeState:\n    exception: ConfirmationRequired\n    user_response: str\n    confirmation_id: str  # Property\n    question: str  # Property\n    tool_call: ToolCall | None  # Property\n    context: dict[str, Any]  # Property\n</code></pre>"},{"location":"architecture/decisions/#resumption-patterns","title":"Resumption Patterns","text":"<p>Pattern 1: Explicit respond_to_confirmation() <pre><code>try:\n    delete_file(\"/data\")\nexcept ConfirmationRequired as e:\n    respond_to_confirmation(e.confirmation_id, approved=True)\n    delete_file(\"/data\")  # Resumes\n</code></pre></p> <p>Pattern 2: ResumeState loop (recommended) <pre><code>resume_state = None\nwhile True:\n    try:\n        result = agent(question=\"Task\", resume_state=resume_state)\n        break\n    except ConfirmationRequired as e:\n        response = get_user_input(e.question)\n        resume_state = ResumeState(e, response)\n</code></pre></p>"},{"location":"architecture/decisions/#react-integration","title":"ReAct Integration","text":"<p>ReAct automatically catches <code>ConfirmationRequired</code> and adds execution state:</p> <pre><code>try:\n    result = await tool.acall(**tool_args)\nexcept ConfirmationRequired as e:\n    # ReAct enriches context with trajectory state\n    e.context = {\n        \"trajectory\": trajectory.copy(),\n        \"iteration\": idx,\n        \"input_args\": input_args.copy(),\n    }\n    if e.tool_call and tool_call_id:\n        e.tool_call.call_id = tool_call_id\n    raise  # Re-raise for caller\n</code></pre> <p>This enables resuming from exact point in trajectory.</p>"},{"location":"architecture/decisions/#key-features_2","title":"Key Features","text":"<ol> <li>Exception-based control: Natural suspension of call stack</li> <li>ResumeState container: Clean API for resumption with user response</li> <li>Type-safe: Literal types and TypedDict for status tracking</li> <li>Thread-safe: <code>ContextVar</code> isolation per thread/task</li> <li>Async-safe: Works with asyncio concurrent operations</li> <li>Module integration: Modules can save/restore state in exception context</li> <li>Tool confirmations: <code>check_tool_confirmation()</code> for tool-level checks</li> <li>Argument editing: Users can modify arguments before approval</li> </ol>"},{"location":"architecture/decisions/#use-cases_2","title":"Use Cases","text":"<ol> <li>Dangerous operations: File deletion, system commands, database changes</li> <li>User confirmation: Sending emails, making purchases, API calls</li> <li>Clarification loops: Ask user for additional information</li> <li>Argument editing: Let user modify parameters before execution</li> <li>Multi-step workflows: Multiple confirmation rounds in agent execution</li> <li>Web APIs: Save state in session, resume later</li> <li>Batch processing: Auto-approve low-risk, human review high-risk</li> </ol>"},{"location":"architecture/decisions/#consequences_3","title":"Consequences","text":"<p>Benefits: - Clean separation of business logic from approval logic - Works naturally with ReAct agent trajectories - Thread-safe and async-safe out of the box - Easy to test (deterministic based on confirmation state) - Type-safe with Literal types and TypedDict - ResumeState provides clean resumption API - Supports multiple confirmation rounds - State preservation enables complex workflows</p> <p>Trade-offs: - Requires exception handling (explicit and clear) - Confirmation state is per-process (doesn't persist across restarts) - Hash-based IDs could collide (extremely rare) - Learning curve for exception-based control flow - Must manage confirmation rounds to prevent infinite loops</p>"},{"location":"architecture/decisions/#alternatives-considered_3","title":"Alternatives Considered","text":"<ul> <li>Callback-based: More complex, harder to reason about flow</li> <li>Async/await pattern: Breaks with mixed sync/async code</li> <li>Return sentinel values: Ambiguous, requires checking every return</li> <li>Async generators with yield: Breaks module composability</li> <li>Middleware pattern: Too heavyweight for this use case</li> <li>Global registry: Testing difficulties, not thread-safe</li> <li>Manual state management: Error-prone, inconsistent</li> </ul>"},{"location":"architecture/decisions/#migration-guide_2","title":"Migration Guide","text":"<p>Feature is additive - no migration needed.</p> <p>Basic usage: <pre><code>@confirm_first\ndef dangerous_op():\n    ...\n</code></pre></p> <p>Recommended pattern: <pre><code>from udspy import ResumeState\n\nresume_state = None\nwhile True:\n    try:\n        result = agent(question=\"...\", resume_state=resume_state)\n        break\n    except ConfirmationRequired as e:\n        response = input(f\"{e.question}: \")\n        resume_state = ResumeState(e, response)\n</code></pre></p>"},{"location":"architecture/decisions/#see-also","title":"See Also","text":"<ul> <li>Confirmation Architecture - Detailed architecture and patterns</li> <li>Confirmation API - API documentation</li> <li>ReAct Module - Integration with agents</li> </ul>"},{"location":"architecture/decisions/#adr-005-react-agent-module","title":"ADR-005: ReAct Agent Module","text":"<p>Date: 2025-10-25</p> <p>Status: Accepted</p>"},{"location":"architecture/decisions/#context_4","title":"Context","text":"<p>The ReAct (Reasoning + Acting) pattern combines chain-of-thought reasoning with tool usage in an iterative loop. This is essential for building agents that can solve complex tasks by breaking them down and using tools.</p>"},{"location":"architecture/decisions/#decision_4","title":"Decision","text":"<p>Implement a <code>ReAct</code> module that: - Alternates between reasoning and tool execution - Supports human-in-the-loop for clarifications and confirmations - Tracks full trajectory of reasoning and actions - Handles errors gracefully with retries - Works with both streaming and non-streaming modes</p> <pre><code>from udspy import ReAct, InputField, OutputField, Signature, tool\n\n@tool(name=\"search\")\ndef search(query: str) -&gt; str:\n    return search_api(query)\n\nclass ResearchTask(Signature):\n    \"\"\"Research and answer questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\nagent = ReAct(ResearchTask, tools=[search], max_iters=5)\nresult = agent(question=\"What is the population of Tokyo?\")\n</code></pre>"},{"location":"architecture/decisions/#implementation-approach_1","title":"Implementation Approach","text":"<ol> <li>Iterative Loop: Continues until final answer or max iterations</li> <li>Dynamic Signature: Extends signature with reasoning_N, tool_name_N, tool_args_N fields</li> <li>Tool Execution: Automatically executes tools and adds results to context</li> <li>Error Handling: Retries with error feedback if tool execution fails</li> <li>Human Confirmations: Integrates with <code>@confirm_first</code> for user input</li> </ol>"},{"location":"architecture/decisions/#key-features_3","title":"Key Features","text":"<ol> <li>Flexible Tool Usage: Agent decides when and which tools to use</li> <li>Self-Correction: Can retry if tool execution fails</li> <li>Trajectory Tracking: Full history of reasoning and actions</li> <li>Streaming Support: Can stream reasoning in real-time</li> <li>Human-in-the-Loop: Built-in support for asking users</li> </ol>"},{"location":"architecture/decisions/#research-evidence_1","title":"Research Evidence","text":"<p>ReAct improves performance on: - Complex Tasks: 15-30% improvement on multi-step reasoning (Yao et al., 2023) - Tool Usage: More accurate tool selection vs. pure CoT - Error Recovery: Better handling of failed tool calls</p>"},{"location":"architecture/decisions/#use-cases_3","title":"Use Cases","text":"<ol> <li>Research Agents: Answer questions using search and APIs</li> <li>Task Automation: Multi-step workflows with tool usage</li> <li>Data Analysis: Fetch data, analyze, and summarize</li> <li>Interactive Assistants: Ask users for clarification when needed</li> </ol>"},{"location":"architecture/decisions/#consequences_4","title":"Consequences","text":"<p>Benefits: - Powerful agent capabilities with minimal code - Transparent reasoning process - Handles complex multi-step tasks - Built-in error handling and retries</p> <p>Trade-offs: - Higher token usage due to multiple iterations - Slower than single-shot predictions - Quality depends on LLM's reasoning ability - Can get stuck in loops if not properly configured</p>"},{"location":"architecture/decisions/#alternatives-considered_4","title":"Alternatives Considered","text":"<ul> <li>Chain-based approach: Too rigid, hard to add dynamic behavior</li> <li>State machine: Overly complex for the use case</li> <li>Pure prompting: Less reliable than structured approach</li> </ul>"},{"location":"architecture/decisions/#future-considerations_1","title":"Future Considerations","text":"<ol> <li>Memory/History: Long-term memory across sessions</li> <li>Tool Chaining: Automatic sequencing of tool calls</li> <li>Parallel Tool Execution: Execute independent tools concurrently</li> <li>Learning: Optimize tool selection based on feedback</li> </ol>"},{"location":"architecture/decisions/#migration-guide_3","title":"Migration Guide","text":"<p>Feature is additive - no migration needed.</p>"},{"location":"architecture/decisions/#adr-006-unified-module-execution-pattern-aexecute","title":"ADR-006: Unified Module Execution Pattern (aexecute)","text":"<p>Date: 2025-10-25</p> <p>Status: Accepted</p>"},{"location":"architecture/decisions/#context_5","title":"Context","text":"<p>Initially, <code>astream()</code> and <code>aforward()</code> had duplicated logic for executing modules. This made maintenance difficult and increased the chance of bugs when updating behavior.</p>"},{"location":"architecture/decisions/#decision_5","title":"Decision","text":"<p>Introduce a single <code>aexecute()</code> method that handles both streaming and non-streaming execution:</p> <pre><code>class Module:\n    async def aexecute(self, *, stream: bool = False, **inputs):\n        \"\"\"Core execution logic - handles both streaming and non-streaming.\"\"\"\n        # Implementation here\n\n    async def astream(self, **inputs):\n        \"\"\"Public streaming API.\"\"\"\n        async for event in self.aexecute(stream=True, **inputs):\n            yield event\n\n    async def aforward(self, **inputs):\n        \"\"\"Public non-streaming API.\"\"\"\n        async for event in self.aexecute(stream=False, **inputs):\n            if isinstance(event, Prediction):\n                return event\n</code></pre>"},{"location":"architecture/decisions/#implementation-details_2","title":"Implementation Details","text":"<ol> <li>Single Source of Truth: All execution logic in <code>aexecute()</code></li> <li>Stream Parameter: Boolean flag controls behavior</li> <li>Generator Pattern: Always yields events, even in non-streaming mode</li> <li>Clean Separation: Public methods are thin wrappers</li> </ol>"},{"location":"architecture/decisions/#key-benefits","title":"Key Benefits","text":"<ol> <li>No Duplication: Write logic once, use in both modes</li> <li>Easier Testing: Test one method instead of two</li> <li>Consistent Behavior: Streaming and non-streaming guaranteed to behave identically</li> <li>Maintainable: Changes only need to be made in one place</li> <li>Extensible: Easy to add new execution modes</li> </ol>"},{"location":"architecture/decisions/#consequences_5","title":"Consequences","text":"<p>Benefits: - Reduced code duplication (~40% less code in modules) - Easier to maintain and debug - Consistent behavior across modes - Simpler to understand (one execution path)</p> <p>Trade-offs: - Slightly more complex to implement initially - Need to handle both streaming and non-streaming cases in same method - Generator pattern requires understanding of async generators</p>"},{"location":"architecture/decisions/#before-and-after","title":"Before and After","text":"<p>Before: <pre><code>async def astream(self, **inputs):\n    # 100 lines of logic\n    ...\n\nasync def aforward(self, **inputs):\n    # 100 lines of DUPLICATED logic with minor differences\n    ...\n</code></pre></p> <p>After: <pre><code>async def aexecute(self, *, stream: bool, **inputs):\n    # 100 lines of logic (used by both)\n    ...\n\nasync def astream(self, **inputs):\n    async for event in self.aexecute(stream=True, **inputs):\n        yield event\n\nasync def aforward(self, **inputs):\n    async for event in self.aexecute(stream=False, **inputs):\n        if isinstance(event, Prediction):\n            return event\n</code></pre></p>"},{"location":"architecture/decisions/#naming-rationale","title":"Naming Rationale","text":"<p>We chose <code>aexecute()</code> (without underscore prefix) because: - Public Method: This is the main extension point for subclasses - Clear Intent: \"Execute\" is explicit about what it does - Python Conventions: No underscore = public API, expected to be overridden - Not Abbreviated: Full word avoids ambiguity (vs <code>aexec</code> or <code>acall</code>)</p>"},{"location":"architecture/decisions/#migration-guide_4","title":"Migration Guide","text":"<p>For Users: No changes needed - public API remains the same</p> <p>For Module Authors: When creating custom modules, implement <code>aexecute()</code> instead of both <code>astream()</code> and <code>aforward()</code>.</p>"},{"location":"architecture/decisions/#additional-design-decisions","title":"Additional Design Decisions","text":""},{"location":"architecture/decisions/#field-markers-for-parsing","title":"Field Markers for Parsing","text":"<p>Decision: Use <code>[[ ## field_name ## ]]</code> markers to delineate fields in completions.</p> <p>Rationale: - Simple, regex-parseable format - Clear visual separation - Consistent with DSPy's approach (proven) - Fallback when native tools aren't available</p> <p>Trade-offs: - Requires careful prompt engineering - LLM might not always respect markers - Uses extra tokens</p>"},{"location":"architecture/decisions/#see-also_1","title":"See Also","text":"<ul> <li>CLAUDE.md - Chronological architectural changes (development log)</li> <li>Architecture Overview - Component relationships</li> <li>Contributing Guide - How to propose new decisions</li> </ul>"},{"location":"architecture/decisions/#adr-007-automatic-retry-on-parse-errors","title":"ADR-007: Automatic Retry on Parse Errors","text":"<p>Date: 2025-10-29</p> <p>Status: Accepted</p>"},{"location":"architecture/decisions/#context_6","title":"Context","text":"<p>LLMs occasionally generate responses that don't match the expected output format, causing <code>AdapterParseError</code> to be raised. This is especially common with: - Field markers being omitted or malformed - JSON parsing errors in structured outputs - Missing required output fields - Format inconsistencies</p> <p>These errors are usually transient - the LLM can often generate a valid response on retry. Without automatic retry, users had to implement retry logic themselves, leading to boilerplate code and inconsistent error handling.</p>"},{"location":"architecture/decisions/#decision_6","title":"Decision","text":"<p>Implement automatic retry logic using the <code>tenacity</code> library on both <code>Predict._aforward()</code> and <code>Predict._astream()</code> methods:</p> <pre><code>from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_exponential\n\n@retry(\n    retry=retry_if_exception_type(AdapterParseError),\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=0.1, max=3),\n)\nasync def _aforward(self, completion_kwargs: dict[str, Any], should_emit: bool) -&gt; Prediction:\n    \"\"\"Process non-streaming LLM call with automatic retry on parse errors.\n\n    Retries up to 2 times (3 total attempts) with exponential backoff (0.1-3s)\n    when AdapterParseError occurs, giving the LLM multiple chances to format\n    the response correctly.\n    \"\"\"\n</code></pre> <p>Key parameters: - Max attempts: 3 (1 initial + 2 retries) - Retry condition: Only retry on <code>AdapterParseError</code> (not other exceptions) - Wait strategy: Exponential backoff starting at 0.1s, max 3s - Applies to: Both streaming (<code>_astream</code>) and non-streaming (<code>_aforward</code>) execution</p>"},{"location":"architecture/decisions/#implementation-details_3","title":"Implementation Details","text":"<ol> <li>Decorator location: Applied to internal <code>_aforward</code> and <code>_astream</code> methods (not public API methods)</li> <li>Tenacity library: Minimal dependency (~50KB) with excellent async support</li> <li>Error propagation: After 3 failed attempts, raises <code>tenacity.RetryError</code> wrapping the original <code>AdapterParseError</code></li> <li>Test isolation: Tests use a <code>fast_retry</code> fixture in <code>conftest.py</code> that patches retry decorators to use <code>wait_none()</code> for instant retries</li> </ol>"},{"location":"architecture/decisions/#consequences_6","title":"Consequences","text":"<p>Benefits: - Improved reliability: Transient parse errors are automatically recovered - Better user experience: Users don't see spurious errors from LLM format issues - Reduced boilerplate: No need for users to implement retry logic - Consistent behavior: All modules get retry logic automatically - Configurable backoff: Exponential backoff prevents API hammering</p> <p>Trade-offs: - Increased latency on errors: Failed attempts add 0.1-3s delay per retry (max ~6s for 3 attempts) - Hidden failures: First 2 parse errors are not visible to users (but logged internally) - Token usage: Failed attempts consume tokens without producing results - Test complexity: Tests need to mock/patch retry behavior to avoid slow tests</p>"},{"location":"architecture/decisions/#alternatives-considered_5","title":"Alternatives Considered","text":"<p>1. No automatic retry (status quo before this ADR) - Pros: Simpler, explicit, no hidden behavior - Cons: Every user has to implement retry logic themselves - Rejected: Too much boilerplate, inconsistent handling</p> <p>2. Configurable retry parameters (e.g., <code>max_retries</code>, <code>backoff_multiplier</code>) - Pros: More flexible, users can tune for their needs - Cons: More complexity, more surface area for bugs - Rejected: Current defaults work well for 95% of cases, can be added later if needed</p> <p>3. Retry at higher level (e.g., in <code>aexecute</code> instead of <code>_aforward</code>/<code>_astream</code>) - Pros: Simpler implementation, single retry point - Cons: Would retry tool calls and other non-LLM logic unnecessarily - Rejected: Parse errors only occur in LLM response parsing, not tool execution</p> <p>4. Use different retry library (e.g., <code>backoff</code>, manual implementation) - Pros: Potentially smaller dependency - Cons: Tenacity is well-maintained, widely used, excellent async support - Rejected: Tenacity is the industry standard for Python retry logic</p>"},{"location":"architecture/decisions/#testing-strategy","title":"Testing Strategy","text":"<p>To keep tests fast, a global <code>fast_retry</code> fixture is used in <code>tests/conftest.py</code>:</p> <pre><code>@pytest.fixture(autouse=True)\ndef fast_retry():\n    \"\"\"Patch retry decorators to use no wait time for fast tests.\"\"\"\n    fast_retry_decorator = retry(\n        retry=retry_if_exception_type(AdapterParseError),\n        stop=stop_after_attempt(3),\n        wait=wait_none(),  # No wait between retries\n    )\n\n    with patch(\"udspy.module.predict.Predict._aforward\",\n               new=fast_retry_decorator(Predict._aforward.__wrapped__)):\n        with patch(\"udspy.module.predict.Predict._astream\",\n                   new=fast_retry_decorator(Predict._astream.__wrapped__)):\n            yield\n</code></pre> <p>This ensures: - Tests run instantly (no exponential backoff wait times) - Retry logic is still exercised in tests - Production code uses proper backoff timings</p>"},{"location":"architecture/decisions/#migration-guide_5","title":"Migration Guide","text":"<p>This is a non-breaking change - no user code needs to be updated.</p> <p>Users who previously implemented their own retry logic can remove it:</p> <pre><code># Before (manual retry)\nfor attempt in range(3):\n    try:\n        result = predictor(question=\"...\")\n        break\n    except AdapterParseError:\n        if attempt == 2:\n            raise\n        time.sleep(0.1 * (2 ** attempt))\n\n# After (automatic retry)\nresult = predictor(question=\"...\")  # Retry is automatic\n</code></pre>"},{"location":"architecture/decisions/#future-considerations_2","title":"Future Considerations","text":"<ol> <li>Make retry configurable: Add <code>max_retries</code> parameter to <code>Predict.__init__()</code> if users need to tune it</li> <li>Add retry callback: Allow users to hook into retry events for logging/metrics</li> <li>Smarter retry: Analyze parse error type and adjust retry strategy (e.g., don't retry on schema validation errors that won't be fixed by retry)</li> <li>Retry budget: Add global retry limit to prevent excessive token usage from many retries</li> </ol>"},{"location":"architecture/decisions/#adr-008-module-callbacks-and-dynamic-tool-management","title":"ADR-008: Module Callbacks and Dynamic Tool Management","text":"<p>Date: 2025-10-31</p> <p>Status: Accepted</p>"},{"location":"architecture/decisions/#context_7","title":"Context","text":"<p>Agents often need specialized tools that should only be loaded on demand rather than being available from the start. Use cases include: - Loading expensive or resource-intensive tools only when needed - Progressive tool discovery (agent figures out what tools it needs as it works) - Category-based tool loading (math tools, web tools, data tools) - Multi-tenant applications with user-specific tool permissions - Reducing initial token usage and context size</p>"},{"location":"architecture/decisions/#decision_7","title":"Decision","text":"<p>Implement a module callback system where tools can return special callables decorated with <code>@module_callback</code> that modify the module's available tools during execution:</p> <pre><code>from udspy import ReAct, tool, module_callback\n\n@tool(name=\"calculator\", description=\"Perform calculations\")\ndef calculator(expression: str) -&gt; str:\n    return str(eval(expression, {\"__builtins__\": {}}, {}))\n\n@tool(name=\"load_calculator\", description=\"Load calculator tool\")\ndef load_calculator() -&gt; callable:\n    \"\"\"Load calculator tool dynamically.\"\"\"\n\n    @module_callback\n    def add_calculator(context):\n        # Get current tools (excluding built-ins)\n        current_tools = [\n            t for t in context.module.tools.values()\n            if t.name not in (\"finish\", \"user_clarification\")\n        ]\n\n        # Add calculator to available tools\n        context.module.init_module(tools=current_tools + [calculator])\n\n        return \"Calculator loaded successfully\"\n\n    return add_calculator\n\n# Agent starts with only the loader\nagent = ReAct(Question, tools=[load_calculator])\n\n# Agent loads calculator when needed, then uses it\nresult = agent(question=\"What is 157 * 834?\")\n</code></pre>"},{"location":"architecture/decisions/#implementation-details_4","title":"Implementation Details","text":"<ol> <li>@module_callback Decorator: Simple marker decorator that adds <code>__udspy_module_callback__</code> attribute</li> <li>Return Value Detection: After tool execution, check <code>is_module_callback(result)</code></li> <li>Context Objects: Pass execution context to callbacks:</li> <li><code>ReactContext</code>: Includes trajectory history</li> <li><code>PredictContext</code>: Includes conversation history</li> <li><code>ModuleContext</code>: Base with module reference</li> <li>init_module() Pattern: Unified method to reinitialize tools and regenerate signatures</li> <li>Tool Persistence: Dynamically loaded tools remain available until module execution completes</li> </ol>"},{"location":"architecture/decisions/#key-features_4","title":"Key Features","text":"<ol> <li>Decorator-based API: Clean, explicit marking of module callbacks</li> <li>Full module access: Callbacks can inspect and modify module state</li> <li>Works with all modules: Predict, ChainOfThought, ReAct</li> <li>Observation return: Callbacks return strings that appear in trajectory</li> <li>Type-safe: Context objects provide proper type hints</li> </ol>"},{"location":"architecture/decisions/#use-cases_4","title":"Use Cases","text":"<ol> <li> <p>On-demand capabilities: Load expensive tools only when needed    <pre><code>agent = ReAct(Task, tools=[load_nlp_tools, load_vision_tools])\n</code></pre></p> </li> <li> <p>Progressive discovery: Agent discovers needed tools as it works    <pre><code>agent = ReAct(Task, tools=[load_tools])  # Figures out what's needed\n</code></pre></p> </li> <li> <p>Multi-tenant: Load user-specific tools based on permissions    <pre><code>@tool(name=\"load_user_tools\")\ndef load_user_tools(user_id: str) -&gt; callable:\n    @module_callback\n    def add_tools(context):\n        tools = get_tools_for_user(user_id)\n        context.module.init_module(tools=tools)\n        return f\"Loaded tools for user {user_id}\"\n    return add_tools\n</code></pre></p> </li> <li> <p>Category loading: Load tool groups on demand    <pre><code>@tool(name=\"load_tools\")\ndef load_tools(category: str) -&gt; callable:  # \"math\", \"web\", \"data\"\n    @module_callback\n    def add_category_tools(context):\n        tools = get_tools_by_category(category)\n        context.module.init_module(tools=current + tools)\n        return f\"Loaded {len(tools)} {category} tools\"\n    return add_category_tools\n</code></pre></p> </li> </ol>"},{"location":"architecture/decisions/#consequences_7","title":"Consequences","text":"<p>Benefits: - Reduced token usage and context size (only load tools when needed) - Adaptive agent behavior (discovers capabilities progressively) - Clean API with decorator pattern - Full module state access through context - Works seamlessly with existing tool system - Enables multi-tenant tool isolation</p> <p>Trade-offs: - Additional complexity in tool execution logic - Must remember to return string from callbacks (for trajectory) - Tool persistence requires new instance for fresh state - Context objects add small memory overhead - Learning curve for callback pattern</p>"},{"location":"architecture/decisions/#alternatives-considered_6","title":"Alternatives Considered","text":"<ul> <li>Direct module mutation: Rejected due to lack of encapsulation and thread safety concerns</li> <li>Event system: Rejected as too complex and heavyweight for this use case</li> <li>Plugin architecture: Rejected as overkill for simple tool management</li> <li>Configuration-based loading: Rejected as less flexible than programmatic control</li> </ul>"},{"location":"architecture/decisions/#migration-guide_6","title":"Migration Guide","text":"<p>Feature is additive - existing code continues to work unchanged.</p> <p>To use dynamic tools:</p> <ol> <li>Define tools that return <code>@module_callback</code> decorated functions</li> <li>Callbacks receive context and call <code>context.module.init_module(tools=[...])</code></li> <li>Return string observation from callback</li> <li>Tool persists for remainder of module execution</li> </ol> <p>Example: <pre><code># Before: All tools loaded upfront\nagent = ReAct(Task, tools=[calculator, search, weather, ...])\n\n# After: Load tools on demand\nagent = ReAct(Task, tools=[load_calculator, load_search, load_weather])\n</code></pre></p>"},{"location":"architecture/decisions/#see-also_2","title":"See Also","text":"<ul> <li>Dynamic Tools Guide</li> <li>Module Callbacks API</li> <li>Tool Calling Guide</li> </ul>"},{"location":"architecture/decisions/#adr-009-history-management-with-system-prompts","title":"ADR-009: History Management with System Prompts","text":"<p>Date: 2025-10-31</p> <p>Status: Accepted</p>"},{"location":"architecture/decisions/#context_8","title":"Context","text":"<p>Chat histories need special handling for system prompts to ensure they're always positioned first in the message list. Module behavior depends on having system instructions properly placed, and tools may manipulate histories during execution. Without dedicated management, it's easy to accidentally insert system prompts mid-conversation or lose them during history manipulation.</p>"},{"location":"architecture/decisions/#decision_8","title":"Decision","text":"<p>Implement <code>History</code> class with dedicated <code>system_prompt</code> property that ensures system messages always appear first:</p> <pre><code>from udspy import History\n\nhistory = History()\n\n# Add conversation messages\nhistory.add_message(role=\"user\", content=\"Hello\")\nhistory.add_message(role=\"assistant\", content=\"Hi there!\")\n\n# System prompt always goes first, even if set later\nhistory.system_prompt = \"You are a helpful assistant\"\n\nmessages = history.messages\n# [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n#  {\"role\": \"user\", \"content\": \"Hello\"},\n#  {\"role\": \"assistant\", \"content\": \"Hi there!\"}]\n</code></pre>"},{"location":"architecture/decisions/#implementation-details_5","title":"Implementation Details","text":"<pre><code>class History:\n    def __init__(self, system_prompt: str | None = None):\n        self._messages: list[dict[str, Any]] = []\n        self._system_prompt: str | None = system_prompt\n\n    @property\n    def system_prompt(self) -&gt; str | None:\n        return self._system_prompt\n\n    @system_prompt.setter\n    def system_prompt(self, value: str | None) -&gt; None:\n        self._system_prompt = value\n\n    @property\n    def messages(self) -&gt; list[dict[str, Any]]:\n        \"\"\"Get all messages with system prompt first (if set).\"\"\"\n        if self._system_prompt:\n            return [\n                {\"role\": \"system\", \"content\": self._system_prompt},\n                *self._messages\n            ]\n        return self._messages.copy()\n</code></pre> <p>Key aspects: - System prompt stored separately from regular messages - <code>messages</code> property dynamically constructs full list - No risk of system prompt appearing mid-conversation - Simple to update system prompt without rebuilding list - Clear ownership (History manages system message)</p>"},{"location":"architecture/decisions/#key-features_5","title":"Key Features","text":"<ol> <li>Dedicated system_prompt property: Special handling for system messages</li> <li>Automatic positioning: System prompt always first in messages list</li> <li>Mutable: Can update system prompt at any time, position maintained</li> <li>Copy support: <code>history.copy()</code> includes system prompt</li> <li>Clear separation: Regular messages in <code>_messages</code>, system prompt separate</li> </ol>"},{"location":"architecture/decisions/#use-cases_5","title":"Use Cases","text":"<ol> <li> <p>Module initialization: Set system prompt per module type    <pre><code>history = History(system_prompt=\"You are a ReAct agent. Use tools to solve tasks.\")\n</code></pre></p> </li> <li> <p>Dynamic prompts: Update based on context or user    <pre><code>history.system_prompt = f\"You are assisting {user.name}. Use their preferences: {prefs}\"\n</code></pre></p> </li> <li> <p>Tool manipulation: Tools can safely update system prompt    <pre><code>@tool(name=\"change_persona\")\ndef change_persona(persona: str) -&gt; str:\n    # Tool can access and modify history.system_prompt\n    return f\"Changed to {persona} persona\"\n</code></pre></p> </li> <li> <p>History replay: Maintain system prompt across sessions    <pre><code>saved_history = history.to_dict()  # Save including system prompt\nloaded_history = History.from_dict(saved_history)  # Restore\n</code></pre></p> </li> <li> <p>Multi-turn conversations: System prompt persists correctly    <pre><code># System prompt set once, remains first through all turns\nfor user_msg in conversation:\n    history.add_message(role=\"user\", content=user_msg)\n    # System prompt still first\n</code></pre></p> </li> </ol>"},{"location":"architecture/decisions/#consequences_8","title":"Consequences","text":"<p>Benefits: - System prompt guaranteed to be first (LLM APIs require this) - Can update system prompt at any time safely - Clean property-based API - Prevents common mistakes (system prompt mid-conversation) - Supports all history manipulation patterns - No manual list management required</p> <p>Trade-offs: - Small overhead constructing messages list on each access (negligible) - System message can't be treated like regular message (by design) - Slight complexity in History implementation vs. simple list - Property access pattern may surprise developers expecting plain list</p>"},{"location":"architecture/decisions/#alternatives-considered_7","title":"Alternatives Considered","text":"<ul> <li>Insert at index 0: Rejected as error-prone with mutations, easy to forget</li> <li>Validation on add: Rejected as too restrictive, doesn't prevent mid-conversation insertion</li> <li>Separate system field in messages: Rejected as doesn't integrate with standard message format</li> <li>Manual management: Status quo before this ADR, too error-prone</li> </ul>"},{"location":"architecture/decisions/#migration-guide_7","title":"Migration Guide","text":"<p>Existing code using <code>History.add_message()</code> continues to work unchanged.</p> <p>To use system prompts:</p> <p>Create with system prompt: <pre><code>history = History(system_prompt=\"You are a helpful assistant\")\n</code></pre></p> <p>Set later: <pre><code>history = History()\n# ... add messages ...\nhistory.system_prompt = \"You are a math tutor\"\n</code></pre></p> <p>Update dynamically: <pre><code>history.system_prompt = f\"You are assisting {user.name}\"\n</code></pre></p> <p>Always correctly positioned: <pre><code>messages = history.messages  # System prompt is always first\n</code></pre></p>"},{"location":"architecture/decisions/#see-also_3","title":"See Also","text":"<ul> <li>History API Reference</li> <li>Module Architecture</li> </ul>"},{"location":"architecture/decisions/#adr-010-lm-callable-interface-with-string-prompts","title":"ADR-010: LM Callable Interface with String Prompts","text":"<p>Date: 2025-10-31</p> <p>Status: Accepted</p>"},{"location":"architecture/decisions/#context_9","title":"Context","text":"<p>Users want the simplest possible interface for quick LLM queries without needing to construct message dictionaries. Common use cases include: - Prototyping and experimentation - Simple scripts and utilities - Interactive sessions (REPL) - Learning and onboarding new users - Quick one-off queries</p> <p>The existing API required constructing message lists even for simple prompts: <pre><code>response = lm.complete([{\"role\": \"user\", \"content\": \"Hello\"}], model=\"gpt-4o\")\ntext = response.choices[0].message.content\n</code></pre></p>"},{"location":"architecture/decisions/#decision_9","title":"Decision","text":"<p>Enhanced LM base class to accept simple string prompts via <code>__call__()</code> and return just the text content:</p> <pre><code>from udspy import OpenAILM\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(api_key=\"sk-...\")\nlm = OpenAILM(client=client, default_model=\"gpt-4o-mini\")\n\n# Simple string prompt - returns just text\nanswer = lm(\"What is the capital of France?\")\nprint(answer)  # \"Paris\"\n\n# Override model\nanswer = lm(\"Explain quantum physics\", model=\"gpt-4\")\n\n# With parameters\nanswer = lm(\"Write a haiku\", temperature=0.9, max_tokens=100)\n</code></pre>"},{"location":"architecture/decisions/#implementation-details_6","title":"Implementation Details","text":"<pre><code>from typing import overload\n\nclass LM(ABC):\n    @property\n    def model(self) -&gt; str | None:\n        \"\"\"Get default model for this LM instance.\"\"\"\n        return None\n\n    @overload\n    def __call__(self, prompt: str, *, model: str | None = None, **kwargs: Any) -&gt; str: ...\n\n    @overload\n    def __call__(\n        self,\n        messages: list[dict[str, Any]],\n        *,\n        model: str | None = None,\n        tools: list[dict[str, Any]] | None = None,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; Any: ...\n\n    def __call__(\n        self,\n        prompt_or_messages: str | list[dict[str, Any]],\n        *,\n        model: str | None = None,\n        **kwargs: Any,\n    ) -&gt; str | Any:\n        if isinstance(prompt_or_messages, str):\n            messages = [{\"role\": \"user\", \"content\": prompt_or_messages}]\n            response = self.complete(messages, model=model, **kwargs)\n            # Extract just the text content\n            if hasattr(response, \"choices\") and len(response.choices) &gt; 0:\n                message = response.choices[0].message\n                if hasattr(message, \"content\") and message.content:\n                    return message.content\n            return str(response)\n        else:\n            return self.complete(prompt_or_messages, model=model, **kwargs)\n</code></pre> <p>Key aspects: 1. Overloaded signatures: <code>@overload</code> provides proper type hints for both modes 2. Type-based dispatch: <code>isinstance(prompt_or_messages, str)</code> determines behavior 3. Message wrapping: String prompts wrapped as <code>[{\"role\": \"user\", \"content\": prompt}]</code> 4. Text extraction: For strings, extract <code>response.choices[0].message.content</code> 5. Fallback: If extraction fails, fall back to <code>str(response)</code> 6. Optional model: Made <code>model</code> parameter optional everywhere, uses <code>self.model</code> as default</p>"},{"location":"architecture/decisions/#key-features_6","title":"Key Features","text":"<ol> <li>Two modes:</li> <li>String input \u2192 returns text only (str)</li> <li>Messages list \u2192 returns full response object (Any)</li> <li>Type-safe: Proper overloads for IDE autocomplete</li> <li>Backward compatible: Existing message-list usage unchanged</li> <li>Optional model: Falls back to instance's default model</li> <li>Passes kwargs: Temperature, max_tokens, etc. work in both modes</li> </ol>"},{"location":"architecture/decisions/#use-cases_6","title":"Use Cases","text":"<ol> <li> <p>Prototyping: Quick tests without boilerplate    <pre><code>answer = lm(\"Is Python interpreted or compiled?\")\n</code></pre></p> </li> <li> <p>Simple scripts: One-line LLM queries    <pre><code>summary = lm(f\"Summarize this in one sentence: {long_text}\")\n</code></pre></p> </li> <li> <p>Interactive sessions: REPL-friendly API    <pre><code>&gt;&gt;&gt; lm(\"What's 2+2?\")\n'4'\n</code></pre></p> </li> <li> <p>Learning: Easiest API for newcomers    <pre><code># First udspy program\nlm = OpenAILM(client, \"gpt-4o-mini\")\nprint(lm(\"Hello!\"))\n</code></pre></p> </li> <li> <p>Utilities: Helper functions    <pre><code>def translate(text: str, target_lang: str) -&gt; str:\n    return lm(f\"Translate to {target_lang}: {text}\")\n</code></pre></p> </li> </ol>"},{"location":"architecture/decisions/#consequences_9","title":"Consequences","text":"<p>Benefits: - Simplest possible API for common case (string prompt) - No need to construct message dictionaries - Backward compatible with existing code - Proper type hints for IDE support (overloads) - Falls back gracefully if text extraction fails - Model parameter now optional everywhere</p> <p>Trade-offs: - Slight complexity in <code>__call__</code> implementation (type dispatch) - String/list dispatch adds minor overhead (negligible) - Text extraction logic specific to OpenAI response format - Two different return types require overloads for type safety - Can't use tools or streaming with string prompt mode</p>"},{"location":"architecture/decisions/#alternatives-considered_8","title":"Alternatives Considered","text":"<ul> <li>Separate method (<code>lm.ask(\"prompt\")</code>): Rejected as less convenient, extra method to learn</li> <li>Always return text: Rejected as losing access to full response metadata</li> <li>Factory function: Rejected as less object-oriented, doesn't fit with LM abstraction</li> <li>Auto-detect return type: Rejected as confusing, breaks type safety</li> </ul>"},{"location":"architecture/decisions/#migration-guide_8","title":"Migration Guide","text":"<p>No migration needed - feature is additive and backward compatible.</p> <p>Before (verbose): <pre><code>response = lm.complete([{\"role\": \"user\", \"content\": \"Hello\"}], model=\"gpt-4o\")\ntext = response.choices[0].message.content\n</code></pre></p> <p>After (concise): <pre><code>text = lm(\"Hello\", model=\"gpt-4o\")\n</code></pre></p> <p>Still supported (full control): <pre><code>response = lm(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"gpt-4o\",\n    tools=[...],\n    stream=True\n)\n</code></pre></p>"},{"location":"architecture/decisions/#see-also_4","title":"See Also","text":"<ul> <li>Basic Usage Guide</li> <li>LM Architecture</li> </ul>"},{"location":"architecture/decisions/#template-for-future-adrs","title":"Template for Future ADRs","text":"<p>When adding new architectural decisions, use this template:</p>"},{"location":"architecture/decisions/#adr-xxx-decision-title","title":"ADR-XXX: Decision Title","text":"<p>Date: YYYY-MM-DD</p> <p>Status: Proposed | Accepted | Deprecated | Superseded</p>"},{"location":"architecture/decisions/#context_10","title":"Context","text":"<p>Why was this change needed? What problem does it solve?</p>"},{"location":"architecture/decisions/#decision_10","title":"Decision","text":"<p>What was decided and implemented? Include code examples if relevant.</p>"},{"location":"architecture/decisions/#implementation-details_7","title":"Implementation Details","text":"<p>How is this implemented? Key technical details.</p>"},{"location":"architecture/decisions/#consequences_10","title":"Consequences","text":"<p>Benefits: - What are the advantages?</p> <p>Trade-offs: - What are the disadvantages or limitations?</p>"},{"location":"architecture/decisions/#alternatives-considered_9","title":"Alternatives Considered","text":"<ul> <li>What other approaches were considered?</li> <li>Why were they rejected?</li> </ul>"},{"location":"architecture/decisions/#migration-guide-if-applicable","title":"Migration Guide (if applicable)","text":"<p>How should users update their code?</p>"},{"location":"architecture/lm/","title":"Language Model (LM) Abstraction","text":"<p>The LM abstraction layer provides a unified interface for interacting with different language model providers in udspy. Through a single factory function and registry-based provider detection, you can seamlessly work with OpenAI, Groq, AWS Bedrock, Ollama, and custom providers.</p>"},{"location":"architecture/lm/#overview","title":"Overview","text":"<p>The LM abstraction consists of:</p> <ol> <li><code>LM()</code> factory function - Creates provider-specific LM instances with auto-detection</li> <li>Provider registry - Maps provider names to configuration (base URLs, etc.)</li> <li><code>BaseLM</code> abstract class - Interface all providers must implement</li> <li><code>OpenAILM</code> implementation - Native OpenAI support and OpenAI-compatible providers</li> <li>Settings integration - Seamless configuration and context management</li> </ol>"},{"location":"architecture/lm/#quick-start","title":"Quick Start","text":""},{"location":"architecture/lm/#basic-usage","title":"Basic Usage","text":"<pre><code>import udspy\n\n# Configure from environment variables\n# Set: UDSPY_LM_MODEL=\"gpt-4o-mini\" and OPENAI_API_KEY=\"sk-...\"\nudspy.settings.configure()\n\n# Or use explicit LM instance\nfrom udspy import LM\nlm = LM(model=\"gpt-4o-mini\", api_key=\"sk-...\")\nudspy.settings.configure(lm=lm)\n</code></pre>"},{"location":"architecture/lm/#multiple-providers","title":"Multiple Providers","text":"<p>The recommended way to work with multiple providers is to set provider-specific API keys and switch via <code>UDSPY_LM_MODEL</code>:</p> <pre><code># .env file\nOPENAI_API_KEY=\"sk-...\"\nGROQ_API_KEY=\"gsk-...\"\nUDSPY_LM_MODEL=\"gpt-4o-mini\"  # Change this to switch providers\n</code></pre> <pre><code>import udspy\n\n# Auto-configures based on UDSPY_LM_MODEL prefix\nudspy.settings.configure()\n\n# Or create LM instances directly:\nfrom udspy import LM\n\n# OpenAI (uses OPENAI_API_KEY from environment)\nlm = LM(model=\"gpt-4o-mini\")\n\n# Groq (uses GROQ_API_KEY from environment)\nlm = LM(model=\"groq/llama-3-70b\")\n\n# Ollama (local, minimal config needed)\nlm = LM(model=\"ollama/llama2\")\n\n# Custom endpoint (only when needed)\nlm = LM(\n    model=\"my-model\",\n    api_key=\"...\",\n    base_url=\"https://my-endpoint.com/v1\"\n)\n</code></pre>"},{"location":"architecture/lm/#lm-factory-function","title":"LM Factory Function","text":"<p>The <code>LM()</code> factory function provides a litellm-style interface for creating language model instances:</p> <pre><code>from udspy import LM\n\nlm = LM(\n    model: str,                    # Required: model identifier\n    api_key: str | None = None,    # Optional: API key (not needed for Ollama)\n    base_url: str | None = None,   # Optional: custom endpoint\n    **kwargs                       # Optional: client configuration\n) -&gt; BaseLM\n</code></pre>"},{"location":"architecture/lm/#provider-detection","title":"Provider Detection","text":"<p>The factory auto-detects the provider from:</p> <ol> <li>Model prefix: <code>\"groq/llama-3-70b\"</code> \u2192 Groq provider</li> <li>Base URL keywords: <code>\"https://api.groq.com\"</code> \u2192 Groq provider</li> <li>Fallback: OpenAI provider</li> </ol>"},{"location":"architecture/lm/#supported-providers","title":"Supported Providers","text":"Provider Prefix Implementation API Key Required OpenAI None (default) Native via <code>openai</code> library Yes Groq <code>groq/</code> OpenAI-compatible endpoint Yes AWS Bedrock <code>bedrock/</code> OpenAI-compatible endpoint Yes Ollama <code>ollama/</code> OpenAI-compatible endpoint No"},{"location":"architecture/lm/#provider-configuration","title":"Provider Configuration","text":"<p>udspy supports multiple LLM providers through a unified interface. All providers use OpenAI-compatible APIs, making it easy to switch between them.</p>"},{"location":"architecture/lm/#environment-variable-precedence","title":"Environment Variable Precedence","text":"<p>When configuring providers, udspy follows this precedence order for API keys and base URLs:</p> <p>API Key Precedence (highest to lowest): 1. Explicitly passed <code>api_key</code> parameter to <code>LM()</code> 2. <code>UDSPY_LM_API_KEY</code> environment variable (general override) 3. Provider-specific environment variable (e.g., <code>OPENAI_API_KEY</code>, <code>GROQ_API_KEY</code>)</p> <p>Base URL Precedence (highest to lowest): 1. Explicitly passed <code>base_url</code> parameter to <code>LM()</code> 2. <code>UDSPY_LM_OPENAI_COMPATIBLE_BASE_URL</code> environment variable (general override) 3. Provider's default base URL from registry</p> <p>This precedence system allows you to: - Override all providers with <code>UDSPY_LM_API_KEY</code> when you want to use the same key everywhere - Fall back to provider-specific keys when <code>UDSPY_LM_API_KEY</code> is not set - Set custom base URLs for self-hosted or custom endpoints</p> <p>Important: <code>UDSPY_LM_API_KEY</code> takes precedence over provider-specific keys. If you want to use different keys for different providers, don't set <code>UDSPY_LM_API_KEY</code>.</p>"},{"location":"architecture/lm/#best-practice-switching-between-providers","title":"Best Practice: Switching Between Providers","text":"<p>There are two recommended approaches depending on your use case:</p>"},{"location":"architecture/lm/#approach-1-provider-specific-keys-recommended-for-multi-provider","title":"Approach 1: Provider-Specific Keys (Recommended for Multi-Provider)","text":"<p>Best when you need different API keys for different providers:</p> <ol> <li> <p>Set provider-specific API keys (don't set <code>UDSPY_LM_API_KEY</code>):    <pre><code>export OPENAI_API_KEY=\"sk-...\"\nexport GROQ_API_KEY=\"gsk-...\"\nexport AWS_BEARER_TOKEN_BEDROCK=\"...\"\n# Don't set UDSPY_LM_API_KEY - it would override all provider-specific keys!\n</code></pre></p> </li> <li> <p>Switch providers by changing only the model:    <pre><code># Switch to OpenAI (uses OPENAI_API_KEY)\nexport UDSPY_LM_MODEL=\"gpt-4o-mini\"\n\n# Switch to Groq (uses GROQ_API_KEY)\nexport UDSPY_LM_MODEL=\"groq/llama-3.1-70b-versatile\"\n\n# Switch to Ollama (local)\nexport UDSPY_LM_MODEL=\"ollama/llama2\"\n</code></pre></p> </li> </ol> <p>Example: <pre><code># .env file\nOPENAI_API_KEY=\"sk-proj-...\"\nGROQ_API_KEY=\"gsk_...\"\nAWS_BEARER_TOKEN_BEDROCK=\"eyJ...\"\nAWS_REGION_NAME=\"us-east-1\"\n\n# Switch providers by changing this single variable:\nUDSPY_LM_MODEL=\"gpt-4o-mini\"          # Uses OPENAI_API_KEY\n# UDSPY_LM_MODEL=\"groq/llama-3-70b\"   # Uses GROQ_API_KEY\n# UDSPY_LM_MODEL=\"bedrock/claude-3\"   # Uses AWS_BEARER_TOKEN_BEDROCK\n</code></pre></p> <pre><code>import udspy\nudspy.settings.configure()  # Auto-configures based on UDSPY_LM_MODEL\n</code></pre>"},{"location":"architecture/lm/#approach-2-single-key-override-developmenttesting","title":"Approach 2: Single Key Override (Development/Testing)","text":"<p>Best when using the same API key across all providers (e.g., testing with one account):</p> <pre><code># .env file\nUDSPY_LM_API_KEY=\"sk-...\"  # This overrides ALL provider-specific keys\nUDSPY_LM_MODEL=\"gpt-4o-mini\"\n</code></pre> <pre><code>import udspy\nudspy.settings.configure()\n</code></pre> <p>Note: Because <code>UDSPY_LM_API_KEY</code> has higher precedence than provider-specific keys, it will be used for all providers regardless of whether <code>OPENAI_API_KEY</code>, <code>GROQ_API_KEY</code>, etc. are set.</p>"},{"location":"architecture/lm/#provider-examples","title":"Provider Examples","text":"<pre><code>from udspy import LM\n\n# OpenAI (uses OPENAI_API_KEY from environment)\nlm = LM(model=\"gpt-4o-mini\")\n\n# OpenAI with explicit key\nlm = LM(model=\"gpt-4o-mini\", api_key=\"sk-...\")\n\n# Groq with prefix (uses GROQ_API_KEY from environment)\nlm = LM(model=\"groq/llama-3-70b\")\n\n# Groq with explicit key\nlm = LM(model=\"groq/llama-3-70b\", api_key=\"gsk-...\")\n\n# Groq without prefix (explicit base_url)\nlm = LM(\n    model=\"llama-3.1-70b-versatile\",\n    api_key=\"gsk-...\",\n    base_url=\"https://api.groq.com/openai/v1\"\n)\n\n# Ollama (local, uses UDSPY_LM_API_KEY or empty string)\nlm = LM(model=\"ollama/llama2\")\n\n# Ollama with explicit base_url\nlm = LM(model=\"llama2\", base_url=\"http://localhost:11434/v1\")\n\n# AWS Bedrock (uses AWS_BEARER_TOKEN_BEDROCK and AWS_REGION_NAME)\nlm = LM(model=\"bedrock/anthropic.claude-3\")\n\n# AWS Bedrock with explicit configuration\nlm = LM(\n    model=\"bedrock/anthropic.claude-3\",\n    api_key=\"eyJ...\",\n    base_url=\"https://bedrock-runtime.us-east-1.amazonaws.com/openai/v1\"\n)\n</code></pre>"},{"location":"architecture/lm/#when-to-use-custom-base-urls","title":"When to Use Custom Base URLs","text":"<p>Only set <code>UDSPY_LM_OPENAI_COMPATIBLE_BASE_URL</code> or pass <code>base_url</code> when:</p> <ol> <li> <p>Self-hosted models: Running your own OpenAI-compatible server    <pre><code>export UDSPY_LM_OPENAI_COMPATIBLE_BASE_URL=\"http://localhost:8000/v1\"\nexport UDSPY_LM_MODEL=\"my-custom-model\"\n</code></pre></p> </li> <li> <p>Proxy/Gateway: Using a proxy that forwards to multiple providers    <pre><code>export UDSPY_LM_OPENAI_COMPATIBLE_BASE_URL=\"https://my-proxy.com/v1\"\n</code></pre></p> </li> <li> <p>Custom Ollama port: Running Ollama on a non-standard port    <pre><code>export UDSPY_LM_OPENAI_COMPATIBLE_BASE_URL=\"http://localhost:8080/v1\"\nexport UDSPY_LM_MODEL=\"ollama/llama2\"\n</code></pre></p> </li> </ol> <p>Don't use custom base URLs when switching between standard providers - the registry already knows the correct endpoints!</p>"},{"location":"architecture/lm/#provider-registry","title":"Provider Registry","text":"<p>The provider registry maps provider names to default configuration and implementation classes:</p> <pre><code>PROVIDER_REGISTRY: dict[str, ProviderConfig] = {\n    \"openai\": {\n        \"default_base_url\": None,  # Uses OpenAI's default\n        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n    },\n    \"groq\": {\n        \"default_base_url\": \"https://api.groq.com/openai/v1\",\n        \"api_key\": os.getenv(\"GROQ_API_KEY\"),\n    },\n    \"bedrock\": {\n        \"default_base_url\": f\"https://bedrock-runtime.{os.getenv('AWS_REGION_NAME', 'us-east-1')}.amazonaws.com/openai/v1\",\n        \"api_key\": os.getenv(\"AWS_BEARER_TOKEN_BEDROCK\"),\n    },\n    \"ollama\": {\n        \"default_base_url\": \"http://localhost:11434/v1\",\n        \"api_key\": os.getenv(\"OLLAMA_API_KEY\", \"\"),\n    },\n}\n# Note: All providers use OpenAILM implementation (OpenAI-compatible APIs)\n</code></pre>"},{"location":"architecture/lm/#adding-custom-providers","title":"Adding Custom Providers","text":"<p>To add a new provider to the registry:</p> <pre><code>from udspy.lm.factory import PROVIDER_REGISTRY\n\n# Add your custom provider\nPROVIDER_REGISTRY[\"myapi\"] = {\n    \"default_base_url\": \"https://api.myservice.com/v1\",\n}\n\n# Now you can use it with model prefix\nfrom udspy import LM\nlm = LM(model=\"myapi/my-model\", api_key=\"...\")\n</code></pre>"},{"location":"architecture/lm/#baselm-abstract-class","title":"BaseLM Abstract Class","text":"<p>All LM implementations must implement the <code>BaseLM</code> interface:</p> <pre><code>from abc import ABC, abstractmethod\nfrom typing import Any\n\nclass BaseLM(ABC):\n    @abstractmethod\n    async def acomplete(\n        self,\n        messages: list[dict[str, Any]],\n        *,\n        model: str | None = None,\n        tools: list[dict[str, Any]] | None = None,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Generate a completion from the language model.\"\"\"\n        pass\n</code></pre> <p>Parameters: - <code>messages</code>: List of messages in OpenAI format - <code>model</code>: Optional model override (uses default if not provided) - <code>tools</code>: Optional tool schemas in OpenAI format - <code>stream</code>: If True, return streaming response - <code>**kwargs</code>: Provider-specific parameters (temperature, etc.)</p>"},{"location":"architecture/lm/#openailm-implementation","title":"OpenAILM Implementation","text":"<p><code>OpenAILM</code> provides the native OpenAI implementation:</p> <pre><code>from udspy.lm import OpenAILM\n\n# Create directly\nlm = OpenAILM(api_key=\"sk-...\", default_model=\"gpt-4o\")\n\n# Access the model\nprint(lm.model)  # \"gpt-4o\"\n\n# Use directly\nresponse = await lm.acomplete(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    temperature=0.7\n)\n</code></pre> <p>Key features: - Uses the official <code>openai</code> library - Supports default model (optional override per call) - Passes through all OpenAI parameters - Handles both streaming and non-streaming - Used for all OpenAI-compatible providers (Groq, Bedrock, Ollama, etc.)</p>"},{"location":"architecture/lm/#settings-integration","title":"Settings Integration","text":"<p>The LM abstraction is deeply integrated with udspy's settings system.</p>"},{"location":"architecture/lm/#configuration-methods","title":"Configuration Methods","text":"<pre><code>import udspy\nfrom udspy import LM\n\n# Method 1: Auto-create from environment variables\n# Set: UDSPY_LM_MODEL=gpt-4o, UDSPY_LM_API_KEY=sk-...\nudspy.settings.configure()\n\n# Method 2: Provide LM instance\nlm = LM(model=\"gpt-4o\", api_key=\"sk-...\")\nudspy.settings.configure(lm=lm)\n\n# Method 3: With Groq\nlm = LM(model=\"groq/llama-3-70b\", api_key=\"gsk-...\")\nudspy.settings.configure(lm=lm)\n\n# Method 4: With callbacks and kwargs\nlm = LM(model=\"gpt-4o\", api_key=\"sk-...\")\nudspy.settings.configure(lm=lm, callbacks=[MyCallback()], temperature=0.7)\n</code></pre>"},{"location":"architecture/lm/#accessing-the-lm","title":"Accessing the LM","text":"<pre><code># Get the configured LM\nlm = udspy.settings.lm\n\n# Access the underlying client\nclient = udspy.settings.lm.client\n\n# Get the model\nmodel = udspy.settings.lm.model\n\n# Use directly\nresponse = await lm.acomplete(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n</code></pre>"},{"location":"architecture/lm/#context-manager-support","title":"Context Manager Support","text":"<p>Use context managers for per-request LM overrides:</p> <pre><code>import udspy\nfrom udspy import LM\n\n# Global settings\nglobal_lm = LM(model=\"gpt-4o-mini\", api_key=\"global-key\")\nudspy.settings.configure(lm=global_lm)\n\n# Temporary override with different LM\ncontext_lm = LM(model=\"gpt-4\", api_key=\"tenant-key\")\nwith udspy.settings.context(lm=context_lm):\n    result = predictor(question=\"...\")  # Uses gpt-4 with tenant-key\n\n# Temporary override with Groq\ngroq_lm = LM(model=\"groq/llama-3-70b\", api_key=\"gsk-...\")\nwith udspy.settings.context(lm=groq_lm):\n    result = predictor(question=\"...\")  # Uses Groq\n\n# Back to global settings\nresult = predictor(question=\"...\")  # Uses gpt-4o-mini with global-key\n</code></pre>"},{"location":"architecture/lm/#multi-tenant-applications","title":"Multi-Tenant Applications","text":"<p>Perfect for serving different users with different API keys:</p> <pre><code>async def handle_user_request(user):\n    # Each user can have their own LM configuration\n    user_lm = LM(model=user.preferred_model, api_key=user.api_key)\n\n    with udspy.settings.context(lm=user_lm):\n        result = predictor(question=user.question)\n        return result\n</code></pre>"},{"location":"architecture/lm/#implementing-custom-providers","title":"Implementing Custom Providers","text":""},{"location":"architecture/lm/#option-1-use-existing-registry","title":"Option 1: Use Existing Registry","text":"<p>If your provider has an OpenAI-compatible API:</p> <pre><code>from udspy import LM\n\n# Just provide the base_url\nlm = LM(\n    model=\"my-model\",\n    api_key=\"...\",\n    base_url=\"https://api.myprovider.com/v1\"\n)\n</code></pre>"},{"location":"architecture/lm/#option-2-extend-baselm","title":"Option 2: Extend BaseLM","text":"<p>For providers that need format conversion:</p> <pre><code>from typing import Any\nfrom udspy.lm import BaseLM\n\nclass AnthropicLM(BaseLM):\n    \"\"\"Anthropic Claude implementation.\"\"\"\n\n    def __init__(self, api_key: str, default_model: str | None = None):\n        from anthropic import AsyncAnthropic\n        self.client = AsyncAnthropic(api_key=api_key)\n        self._default_model = default_model\n\n    @property\n    def model(self) -&gt; str | None:\n        return self._default_model\n\n    async def acomplete(\n        self,\n        messages: list[dict[str, Any]],\n        *,\n        model: str | None = None,\n        tools: list[dict[str, Any]] | None = None,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Generate completion using Anthropic API.\"\"\"\n        actual_model = model or self._default_model\n        if not actual_model:\n            raise ValueError(\"No model specified\")\n\n        # Convert OpenAI format to Anthropic format\n        anthropic_messages = self._convert_messages(messages)\n        anthropic_tools = self._convert_tools(tools) if tools else None\n\n        # Call Anthropic API\n        response = await self.client.messages.create(\n            model=actual_model,\n            messages=anthropic_messages,\n            tools=anthropic_tools,\n            stream=stream,\n            **kwargs\n        )\n\n        return response\n\n    def _convert_messages(self, messages):\n        \"\"\"Convert OpenAI format to Anthropic format.\"\"\"\n        # Implementation...\n        pass\n\n    def _convert_tools(self, tools):\n        \"\"\"Convert OpenAI tools to Anthropic tools.\"\"\"\n        # Implementation...\n        pass\n</code></pre>"},{"location":"architecture/lm/#use-custom-provider","title":"Use Custom Provider","text":"<pre><code>import udspy\nfrom my_providers import AnthropicLM\n\n# Configure with custom provider\nlm = AnthropicLM(api_key=\"sk-ant-...\", default_model=\"claude-3-5-sonnet-20241022\")\nudspy.settings.configure(lm=lm)\n\n# Use normally - all udspy features work!\nfrom udspy import Predict, Signature, InputField, OutputField\n\nclass QA(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\nresult = predictor(question=\"What is the capital of France?\")\nprint(result.answer)  # Uses Anthropic Claude\n</code></pre>"},{"location":"architecture/lm/#message-format-standard","title":"Message Format Standard","text":"<p>The LM abstraction uses OpenAI's message format as the standard:</p> <pre><code>[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"},\n    {\"role\": \"assistant\", \"content\": \"Hi! How can I help?\"},\n    {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n]\n</code></pre> <p>Why OpenAI format? - Industry standard - widely adopted - Simple and flexible - Easy to convert to other formats - Well-documented</p> <p>Custom providers should convert to/from OpenAI format internally.</p>"},{"location":"architecture/lm/#best-practices","title":"Best Practices","text":""},{"location":"architecture/lm/#for-users","title":"For Users","text":"<ol> <li>Use provider-specific API keys for multi-provider setups - See Best Practice: Switching Between Providers</li> <li>Switch providers via <code>UDSPY_LM_MODEL</code> only - avoid changing <code>base_url</code> unless needed</li> <li>Use model prefixes for clarity: <code>\"groq/llama-3-70b\"</code> instead of manual base_url</li> <li>Store API keys in environment variables - never hardcode</li> <li>Use context managers for multi-tenant scenarios</li> <li>Always specify a model to avoid runtime errors</li> <li>Prefer <code>settings.lm.client</code> over deprecated <code>settings.aclient</code></li> </ol>"},{"location":"architecture/lm/#for-provider-implementers","title":"For Provider Implementers","text":"<ol> <li>Convert to/from OpenAI format in your implementation</li> <li>Handle streaming properly - return appropriate type when <code>stream=True</code></li> <li>Validate required parameters - raise clear errors for missing config</li> <li>Document provider-specific kwargs - help users understand options</li> <li>Test thoroughly - ensure compatibility with udspy modules</li> <li>Implement <code>model</code> property - return the default model</li> </ol>"},{"location":"architecture/lm/#environment-variables","title":"Environment Variables","text":"<p>udspy recognizes these environment variables. See Environment Variable Precedence for how these variables are resolved.</p>"},{"location":"architecture/lm/#general-variables","title":"General Variables","text":"Variable Description Example Precedence <code>UDSPY_LM_MODEL</code> Default model identifier <code>gpt-4o-mini</code>, <code>groq/llama-3-70b</code> Required for auto-config <code>UDSPY_LM_API_KEY</code> General API key override <code>sk-...</code> 2nd (overrides provider-specific keys) <code>UDSPY_LM_OPENAI_COMPATIBLE_BASE_URL</code> Custom base URL override <code>https://my-proxy.com/v1</code> 2nd (overrides provider defaults)"},{"location":"architecture/lm/#provider-specific-variables","title":"Provider-Specific Variables","text":"Variable Provider Description Precedence <code>OPENAI_API_KEY</code> OpenAI OpenAI API key 3rd (fallback if no UDSPY_LM_API_KEY) <code>GROQ_API_KEY</code> Groq Groq API key 3rd (fallback if no UDSPY_LM_API_KEY) <code>AWS_BEARER_TOKEN_BEDROCK</code> AWS Bedrock AWS Bedrock bearer token 3rd (fallback if no UDSPY_LM_API_KEY) <code>AWS_REGION_NAME</code> AWS Bedrock AWS region for Bedrock endpoint Used for default base URL <code>OLLAMA_API_KEY</code> Ollama Ollama API key (rarely needed) 3rd (fallback if no UDSPY_LM_API_KEY)"},{"location":"architecture/lm/#variable-resolution-order","title":"Variable Resolution Order","text":"<p>For API Keys: 1. Explicit <code>api_key</code> parameter to <code>LM()</code> 2. <code>UDSPY_LM_API_KEY</code> (general override - takes precedence over provider-specific keys) 3. Provider-specific key (e.g., <code>OPENAI_API_KEY</code>, <code>GROQ_API_KEY</code>)</p> <p>For Base URLs: 1. Explicit <code>base_url</code> parameter to <code>LM()</code> 2. <code>UDSPY_LM_OPENAI_COMPATIBLE_BASE_URL</code> (general override - takes precedence over provider defaults) 3. Provider's default from registry</p> <p>Important: To use different API keys for different providers, do not set <code>UDSPY_LM_API_KEY</code>. Only set provider-specific keys (<code>OPENAI_API_KEY</code>, <code>GROQ_API_KEY</code>, etc.).</p>"},{"location":"architecture/lm/#examples","title":"Examples","text":"<p>Example 1: Single API Key for All Providers: <pre><code># UDSPY_LM_API_KEY overrides provider-specific keys\nexport UDSPY_LM_MODEL=\"gpt-4o-mini\"\nexport UDSPY_LM_API_KEY=\"sk-...\"  # Used for ALL providers\n</code></pre></p> <pre><code>import udspy\nudspy.settings.configure()  # Uses UDSPY_LM_API_KEY for all models\n</code></pre> <p>Example 2: Multi-Provider Setup (Recommended): <pre><code># Set provider-specific keys, DON'T set UDSPY_LM_API_KEY\nexport OPENAI_API_KEY=\"sk-...\"\nexport GROQ_API_KEY=\"gsk-...\"\n# No UDSPY_LM_API_KEY - this allows provider-specific keys to work\n\n# Switch providers by changing model only\nexport UDSPY_LM_MODEL=\"gpt-4o-mini\"          # Uses OPENAI_API_KEY\n# export UDSPY_LM_MODEL=\"groq/llama-3-70b\"   # Uses GROQ_API_KEY\n</code></pre></p> <pre><code>import udspy\nudspy.settings.configure()  # Auto-selects key based on provider\n</code></pre> <p>Example 3: Custom Endpoint: <pre><code># Override base URL for all providers\nexport UDSPY_LM_MODEL=\"my-model\"\nexport UDSPY_LM_API_KEY=\"custom-key\"\nexport UDSPY_LM_OPENAI_COMPATIBLE_BASE_URL=\"http://localhost:8000/v1\"\n</code></pre></p> <pre><code>import udspy\nudspy.settings.configure()  # Uses custom endpoint\n</code></pre>"},{"location":"architecture/lm/#comparison-with-dspy","title":"Comparison with DSPy","text":"Aspect udspy DSPy Factory <code>LM()</code> with auto-detection Manual provider selection Interface <code>BaseLM.acomplete()</code> <code>LM.__call__()</code> Async Async-first Sync-first with async support Message format OpenAI standard LM-specific adapters Settings Integrated Separate configuration Context support Built-in <code>settings.context()</code> Manual per-call Streaming Single method, <code>stream</code> param Separate methods Providers Registry-based Class per provider"},{"location":"architecture/lm/#related-documentation","title":"Related Documentation","text":"<ul> <li>Settings and Configuration</li> <li>Modules Architecture</li> <li>Predict Module</li> <li>Other Providers Example</li> </ul>"},{"location":"architecture/modules/","title":"Modules","text":"<p>Modules are composable units that encapsulate LLM calls. They provide a standard interface for building complex LLM-powered applications.</p>"},{"location":"architecture/modules/#overview","title":"Overview","text":"<p>All modules inherit from the base <code>Module</code> class and implement a unified execution pattern. This allows modules to be composed, nested, and combined to create sophisticated behaviors.</p>"},{"location":"architecture/modules/#core-concepts","title":"Core Concepts","text":""},{"location":"architecture/modules/#unified-interface","title":"Unified Interface","text":"<p>Every module implements: - <code>aexecute(*, stream: bool, **inputs)</code> - Core async execution - <code>aforward(**inputs)</code> - Async convenience (no streaming) - <code>__call__(**inputs)</code> - Synchronous wrapper</p>"},{"location":"architecture/modules/#composition","title":"Composition","text":"<p>Modules can contain other modules:</p> <pre><code>from udspy import Module, Predict, ChainOfThought, Prediction\n\nclass Pipeline(Module):\n    def __init__(self):\n        self.analyze = Predict(\"text -&gt; analysis\")\n        self.summarize = ChainOfThought(\"text, analysis -&gt; summary\")\n\n    async def aexecute(self, *, stream: bool = False, **inputs):\n        # First module: get analysis (stream=False since we need full result)\n        analysis = None\n        async for event in self.analyze.aexecute(stream=False, text=inputs[\"text\"]):\n            if isinstance(event, Prediction):\n                analysis = event\n\n        if not analysis:\n            raise ValueError(\"First module did not produce a result\")\n\n        # Second module: pass down stream parameter\n        async for event in self.summarize.aexecute(\n            stream=stream,\n            text=inputs[\"text\"],\n            analysis=analysis.analysis\n        ):\n            yield event\n</code></pre>"},{"location":"architecture/modules/#streaming-support","title":"Streaming Support","text":"<p>All modules support streaming for real-time output:</p> <pre><code>async for event in module.aexecute(stream=True, **inputs):\n    if isinstance(event, OutputStreamChunk):\n        print(event.delta, end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        print(f\"\\nFinal: {event}\")\n</code></pre>"},{"location":"architecture/modules/#dynamic-tool-management","title":"Dynamic Tool Management","text":"<p>Modules support runtime modification of their toolset via the <code>init_module()</code> method. This enables: - Loading tools on demand (reduce initial context size) - Progressive tool discovery (agent figures out what it needs) - Adaptive behavior (add tools based on task complexity)</p> <p>Key method: <code>init_module(tools=None)</code></p> <p>This method is essential for dynamic tools because when the toolset changes, the module needs to fully reconfigure itself: - Regenerate tool schemas (so the LLM knows how to call new tools) - Rebuild signatures (so tool descriptions appear in the prompt) - Update the tool registry (so the module can execute the tools)</p> <p>It's typically called from within a module callback:</p> <pre><code>from udspy import tool, module_callback, ReAct\n\n@tool(name=\"load_calculator\")\ndef load_calculator() -&gt; callable:\n    \"\"\"Load calculator tool when needed.\"\"\"\n\n    @module_callback\n    def add_calculator(context):\n        # Get current tools\n        current = list(context.module.tools.values())\n\n        # Add new tool\n        context.module.init_module(tools=current + [calculator])\n\n        return \"Calculator loaded\"\n\n    return add_calculator\n\n# Agent starts with only the loader\nagent = ReAct(signature, tools=[load_calculator])\n\n# Agent loads calculator when needed\nresult = agent(question=\"What is 157 * 834?\")\n</code></pre> <p>See: Dynamic Tool Management Guide for detailed examples and patterns.</p>"},{"location":"architecture/modules/#built-in-modules","title":"Built-in Modules","text":"<p>udspy provides three core modules:</p>"},{"location":"architecture/modules/#base-module","title":"Base Module","text":"<p>The foundation for all modules. Provides: - Unified execution interface - Streaming infrastructure - Async-first design - Composition support</p> <p>When to use: When creating custom modules</p>"},{"location":"architecture/modules/#predict","title":"Predict","text":"<p>The core module for LLM predictions. Features: - Maps signature inputs to outputs - Native tool calling support - Conversation history management - Streaming and async execution</p> <p>When to use: For basic LLM calls, tool usage, and as a building block for other modules</p> <pre><code>predictor = Predict(\"question -&gt; answer\")\nresult = predictor(question=\"What is AI?\")\n</code></pre>"},{"location":"architecture/modules/#chainofthought","title":"ChainOfThought","text":"<p>Adds step-by-step reasoning before outputs. Features: - Automatic reasoning field injection - Improves answer quality on complex tasks - Transparent reasoning process - Works with any signature</p> <p>When to use: For tasks requiring reasoning (math, analysis, decision-making)</p> <pre><code>cot = ChainOfThought(\"question -&gt; answer\")\nresult = cot(question=\"What is 157 * 234?\")\nprint(result.reasoning)  # Shows step-by-step work\nprint(result.answer)     # \"36738\"\n</code></pre>"},{"location":"architecture/modules/#react","title":"ReAct","text":"<p>Agent that reasons and acts with tools. Features: - Iterative reasoning and tool usage - Human-in-the-loop support - Built-in user_clarification and finish tools - Full trajectory tracking</p> <p>When to use: For tasks requiring multiple steps, tool usage, or agent-like behavior</p> <pre><code>@tool(name=\"search\")\ndef search(query: str = Field(...)) -&gt; str:\n    return search_web(query)\n\nagent = ReAct(\"question -&gt; answer\", tools=[search])\nresult = agent(question=\"What's the weather in Tokyo?\")\n</code></pre>"},{"location":"architecture/modules/#module-comparison","title":"Module Comparison","text":"Feature Predict ChainOfThought ReAct Basic LLM calls \u2705 \u2705 \u2705 Step-by-step reasoning \u274c \u2705 \u2705 Tool usage \u2705 \u2705 \u2705 Multi-step iteration \u274c \u274c \u2705 Human-in-the-loop \u274c \u274c \u2705 Trajectory tracking \u274c \u274c \u2705 Complexity Low Low Medium Token usage Low Medium High Latency Low Medium High"},{"location":"architecture/modules/#creating-custom-modules","title":"Creating Custom Modules","text":"<p>To create a custom module:</p> <ol> <li>Subclass <code>Module</code></li> <li>Implement <code>aexecute()</code> method</li> <li>Yield <code>StreamEvent</code> objects during execution</li> <li>Yield final <code>Prediction</code> at the end</li> </ol> <pre><code>from udspy import Module, Prediction, OutputStreamChunk\n\nclass CustomModule(Module):\n    def __init__(self, signature):\n        self.predictor = Predict(signature)\n\n    async def aexecute(self, *, stream: bool = False, **inputs):\n        # Custom logic before prediction\n        processed_inputs = preprocess(inputs)\n\n        # Call nested module's aexecute, passing down stream parameter\n        async for event in self.predictor.aexecute(stream=stream, **processed_inputs):\n            if isinstance(event, Prediction):\n                # Custom logic after prediction\n                final_result = postprocess(event)\n                # Yield final prediction\n                yield Prediction(**final_result)\n            else:\n                # Pass through other events (OutputStreamChunks, etc.)\n                yield event\n</code></pre> <p>See Base Module for detailed guidance.</p>"},{"location":"architecture/modules/#best-practices","title":"Best Practices","text":""},{"location":"architecture/modules/#choose-the-right-module","title":"Choose the Right Module","text":"<ul> <li>Simple tasks: Use <code>Predict</code></li> <li>Need reasoning: Use <code>ChainOfThought</code></li> <li>Need tools/agents: Use <code>ReAct</code></li> <li>Custom logic: Create custom module</li> </ul>"},{"location":"architecture/modules/#composition-over-inheritance","title":"Composition over Inheritance","text":"<p>Build complex behaviors by composing modules rather than deep inheritance:</p> <pre><code># Good: Composition\nclass Pipeline(Module):\n    def __init__(self):\n        self.step1 = Predict(sig1)\n        self.step2 = ChainOfThought(sig2)\n\n# Avoid: Deep inheritance\nclass MyComplexModule(ChainOfThought):\n    # Complex overrides\n</code></pre>"},{"location":"architecture/modules/#async-best-practices","title":"Async Best Practices","text":"<ul> <li>Use <code>aforward()</code> when you don't need streaming</li> <li>Use <code>aexecute(stream=True)</code> for real-time output</li> <li>Use <code>__call__()</code> only in sync contexts</li> <li>Always <code>await</code> async operations</li> </ul>"},{"location":"architecture/modules/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    result = await module.aforward(**inputs)\nexcept ConfirmationRequired as e:\n    # Handle confirmations\n    result = await module.aresume(user_input, e)\nexcept Exception as e:\n    # Handle other errors\n    logger.error(f\"Module failed: {e}\")\n</code></pre>"},{"location":"architecture/modules/#see-also","title":"See Also","text":"<ul> <li>Base Module - Module foundation</li> <li>Predict Module - Core prediction</li> <li>ChainOfThought Module - Step-by-step reasoning</li> <li>ReAct Module - Agent with tools</li> <li>Signatures - Define inputs/outputs</li> <li>Streaming - Real-time output</li> <li>API: Modules - Full API reference</li> </ul>"},{"location":"architecture/overview/","title":"udspy Architecture Overview","text":"<p>This document defines the architecture, design principles, and responsibility boundaries for udspy. Use this as the authoritative reference when deciding where to place new code or how to extend the library.</p>"},{"location":"architecture/overview/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Core Philosophy</li> <li>Layered Architecture</li> <li>Core Abstractions</li> <li>Module System</li> <li>LM Abstraction (Language Model Layer)</li> <li>Signatures</li> <li>Tools</li> <li>History</li> <li>Streaming</li> <li>Confirmation &amp; Suspend/Resume</li> <li>Callbacks</li> <li>How to Extend</li> <li>Design Patterns</li> <li>Decision Tree</li> </ol>"},{"location":"architecture/overview/#core-philosophy","title":"Core Philosophy","text":"<p>udspy is a minimal, async-first framework for building LLM applications with clear abstractions and separation of concerns.</p>"},{"location":"architecture/overview/#key-principles","title":"Key Principles","text":"<ol> <li>Simplicity Over Completeness</li> <li>Provide core primitives, not every possible feature</li> <li> <p>Make common cases easy, complex cases possible</p> </li> <li> <p>Async-First</p> </li> <li>All core operations are async</li> <li>Sync wrappers (<code>forward()</code>, <code>__call__()</code>) use <code>asyncio.run()</code> internally</li> <li> <p>Natural support for streaming and concurrent operations</p> </li> <li> <p>Clear Responsibility Boundaries</p> </li> <li>Each layer has ONE well-defined purpose</li> <li>Minimal coupling between layers</li> <li> <p>Easy to test and modify independently</p> </li> <li> <p>Type Safety</p> </li> <li>Pydantic models for runtime validation</li> <li>Type hints throughout</li> <li> <p>Fail fast with clear errors</p> </li> <li> <p>Native Tool Calling</p> </li> <li>Use OpenAI's native function calling API</li> <li>No custom prompt hacking for structured outputs</li> <li>Leverages provider optimizations</li> </ol>"},{"location":"architecture/overview/#layered-architecture","title":"Layered Architecture","text":"<p>udspy is organized into clear layers with well-defined responsibilities:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    User Application                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Module Layer                           \u2502\n\u2502  (Predict, ChainOfThought, ReAct, Custom Modules)       \u2502\n\u2502  - Business logic and orchestration                      \u2502\n\u2502  - Compose other modules                                 \u2502\n\u2502  - Handle tool execution loops                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  LM Layer (Provider Abstraction)         \u2502\n\u2502  - Abstract interface to LLM providers                   \u2502\n\u2502  - Currently: OpenAI via settings.lm                     \u2502\n\u2502  - Extensible: Anthropic, local models, etc.             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Adapter Layer                          \u2502\n\u2502  - Format signatures \u2192 messages                          \u2502\n\u2502  - Parse LLM outputs \u2192 structured data                   \u2502\n\u2502  - Convert tools \u2192 provider schemas                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Supporting Infrastructure                   \u2502\n\u2502  - History: Conversation state                           \u2502\n\u2502  - Tools: Function calling                               \u2502\n\u2502  - Streaming: Event queue and chunks                     \u2502\n\u2502  - Confirmation: Human-in-the-loop                       \u2502\n\u2502  - Callbacks: Telemetry and monitoring                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Settings                              \u2502\n\u2502  - Global and context-specific configuration             \u2502\n\u2502  - Thread-safe via ContextVar                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#layer-responsibilities","title":"Layer Responsibilities","text":""},{"location":"architecture/overview/#1-module-layer-srcudspymodule","title":"1. Module Layer (<code>src/udspy/module/</code>)","text":"<p>What it does: Orchestrates LLM calls and business logic</p> <p>Responsibilities: - Implements business logic (Predict, ChainOfThought, ReAct) - Composes other modules - Manages execution flow (tool loops, retry logic) - Emits streaming events - Returns final Prediction results</p> <p>Key Files: - <code>base.py</code> - Module base class with aexecute/aforward/astream - <code>predict.py</code> - Core LLM prediction with tool calling - <code>chain_of_thought.py</code> - Reasoning wrapper - <code>react.py</code> - Agent with tool iteration</p> <p>DO: Business logic, orchestration, composition DON'T: Direct LLM API calls (use LM layer), message formatting (use Adapter)</p>"},{"location":"architecture/overview/#2-lm-layer-language-model-abstraction","title":"2. LM Layer (Language Model Abstraction)","text":"<p>What it does: Provides abstract interface to LLM providers</p> <p>Current State: - Direct usage of <code>settings.aclient</code> (AsyncOpenAI) - No abstraction yet - coupled to OpenAI</p> <p>Future Design: <pre><code>class LM(ABC):\n    \"\"\"Abstract language model interface.\"\"\"\n\n    @abstractmethod\n    async def acomplete(\n        self,\n        messages: list[dict],\n        tools: list[dict] | None = None,\n        stream: bool = False,\n        **kwargs\n    ) -&gt; AsyncGenerator[ChatCompletion, None] | ChatCompletion:\n        \"\"\"Complete a prompt with optional tools.\"\"\"\n        pass\n\nclass OpenAILM(LM):\n    \"\"\"OpenAI implementation.\"\"\"\n    def __init__(self, client: AsyncOpenAI, model: str):\n        self.client = client\n        self.model = model\n\n    async def acomplete(self, messages, tools=None, stream=False, **kwargs):\n        return await self.client.chat.completions.create(\n            model=self.model,\n            messages=messages,\n            tools=tools,\n            stream=stream,\n            **kwargs\n        )\n</code></pre></p> <p>Responsibilities: - Normalize provider-specific APIs - Handle retries and rate limiting - Abstract away provider differences - Provide unified interface for modules</p> <p>Key Files (when implemented): - <code>lm.py</code> - Base LM class and interface - <code>lm/openai.py</code> - OpenAI implementation - <code>lm/anthropic.py</code> - Anthropic implementation (future)</p> <p>DO: Provider API calls, retry logic, rate limiting DON'T: Message formatting (use Adapter), business logic (use Module)</p>"},{"location":"architecture/overview/#3-adapter-layer-srcudspyadapterpy","title":"3. Adapter Layer (<code>src/udspy/adapter.py</code>)","text":"<p>What it does: Translates between udspy concepts and provider formats</p> <p>Responsibilities: - Format Signature \u2192 system prompts - Format inputs \u2192 user messages - Parse LLM outputs \u2192 structured Prediction - Convert Tool \u2192 OpenAI tool schema - Type coercion and validation</p> <p>Key Methods: - <code>format_instructions(signature)</code> - Signature \u2192 system message - <code>format_inputs(signature, inputs)</code> - Inputs \u2192 user message - <code>parse_outputs(signature, completion)</code> - Completion \u2192 structured dict - <code>tools_to_openai_format(tools)</code> - Tools \u2192 OpenAI schemas</p> <p>DO: Format translation, schema conversion, parsing DON'T: LLM API calls (use LM), orchestration (use Module)</p>"},{"location":"architecture/overview/#4-supporting-infrastructure","title":"4. Supporting Infrastructure","text":"<p>History (<code>src/udspy/history.py</code>) - Stores conversation messages - Simple list wrapper with convenience methods - No LLM coupling - pure data structure</p> <p>Tools (<code>src/udspy/tool.py</code>) - Wraps functions as tools - Extracts schemas from type hints - Handles async/sync execution - Integrates with confirmation system</p> <p>Streaming (<code>src/udspy/streaming.py</code>) - Event queue via ContextVar - StreamEvent base class - emit_event() for custom events - Prediction as final event</p> <p>Confirmation (<code>src/udspy/confirmation.py</code>) - ConfirmationRequired exception - ResumeState for continuation - Context-based approval tracking - @confirm_first decorator</p> <p>Callbacks (<code>src/udspy/callback.py</code>) - BaseCallback interface - @with_callbacks decorator - Telemetry and monitoring hooks - Compatible with Opik, MLflow, etc.</p>"},{"location":"architecture/overview/#core-abstractions","title":"Core Abstractions","text":""},{"location":"architecture/overview/#1-module","title":"1. Module","text":"<p>What: A composable unit that encapsulates LLM operations</p> <p>Interface: <pre><code>class Module:\n    async def aexecute(self, *, stream: bool = False, **inputs) -&gt; Prediction:\n        \"\"\"Core execution - implements business logic.\"\"\"\n\n    async def aforward(self, **inputs) -&gt; Prediction:\n        \"\"\"Non-streaming execution.\"\"\"\n        return await self.aexecute(stream=False, **inputs)\n\n    async def astream(self, **inputs) -&gt; AsyncGenerator[StreamEvent]:\n        \"\"\"Streaming execution - sets up queue and yields events.\"\"\"\n\n    def forward(self, **inputs) -&gt; Prediction:\n        \"\"\"Sync wrapper.\"\"\"\n        return asyncio.run(self.aforward(**inputs))\n\n    def __call__(self, **inputs) -&gt; Prediction:\n        \"\"\"Sync convenience.\"\"\"\n        return self.forward(**inputs)\n</code></pre></p> <p>Key Insight: <code>aexecute()</code> is the single source of truth. Both <code>aforward()</code> and <code>astream()</code> call it with different parameters.</p>"},{"location":"architecture/overview/#2-signature","title":"2. Signature","text":"<p>What: Defines input/output contract for an LLM task using Pydantic</p> <p>Purpose: - Specify expected inputs and outputs - Provide descriptions for prompt construction - Enable runtime validation - Generate tool schemas</p> <p>Example: <pre><code>class QA(Signature):\n    \"\"\"Answer questions concisely.\"\"\"\n    question: str = InputField(description=\"User's question\")\n    answer: str = OutputField(description=\"Concise answer\")\n</code></pre></p> <p>Where Used: - Modules use signatures to define their I/O contract - Adapter formats signatures into prompts - Validation ensures type safety</p>"},{"location":"architecture/overview/#3-prediction","title":"3. Prediction","text":"<p>What: Result of a module execution (dict-like with attribute access)</p> <pre><code>pred = Prediction(answer=\"Paris\", reasoning=\"France's capital\")\nprint(pred.answer)  # \"Paris\"\nprint(pred[\"answer\"])  # \"Paris\"\nprint(pred.is_final)  # True if no pending tool calls\n</code></pre> <p>Key Properties: - <code>native_tool_calls</code> - Pending tool calls (if any) - <code>is_final</code> - True if execution is complete - Inherits from <code>dict</code> and <code>StreamEvent</code></p>"},{"location":"architecture/overview/#4-tool","title":"4. Tool","text":"<p>What: Wrapper for a callable function that can be invoked by LLM</p> <p>Creation: <pre><code>@tool(name=\"search\", description=\"Search the web\")\ndef search(query: str = Field(description=\"Search query\")) -&gt; str:\n    return search_web(query)\n\n# Or manually\nsearch_tool = Tool(\n    func=search_fn,\n    name=\"search\",\n    description=\"Search the web\",\n    require_confirmation=True\n)\n</code></pre></p> <p>Schema Generation: - Extracts type hints from function signature - Uses Pydantic Field for parameter descriptions - Converts to OpenAI function calling format</p>"},{"location":"architecture/overview/#5-history","title":"5. History","text":"<p>What: Conversation message storage</p> <p>Usage: <pre><code>history = History()\n\n# Automatically managed by Predict\nresult = predictor(question=\"What is Python?\", history=history)\n# History now contains: [system, user, assistant]\n\nresult = predictor(question=\"What are its features?\", history=history)\n# LLM has context from previous turn\n</code></pre></p> <p>Storage Format: OpenAI message format (<code>{\"role\": \"...\", \"content\": \"...\"}</code>)</p>"},{"location":"architecture/overview/#module-system","title":"Module System","text":""},{"location":"architecture/overview/#the-aexecute-pattern","title":"The <code>aexecute</code> Pattern","text":"<p>Core Concept: Every module has ONE implementation in <code>aexecute()</code>, which powers BOTH streaming and non-streaming interfaces.</p> <pre><code>class MyModule(Module):\n    async def aexecute(self, *, stream: bool = False, **inputs) -&gt; Prediction:\n        \"\"\"Single source of truth for execution logic.\"\"\"\n\n        # 1. Do the work (call LLM, process data, etc.)\n        result = await self.do_work(inputs, stream=stream)\n\n        # 2. Optionally emit streaming events\n        if self.should_emit_events():\n            emit_event(OutputStreamChunk(...))\n\n        # 3. Always return final Prediction\n        return Prediction(answer=result)\n</code></pre>"},{"location":"architecture/overview/#how-streaming-works","title":"How Streaming Works","text":"<p>Event Queue: - <code>astream()</code> sets up an <code>asyncio.Queue</code> via ContextVar - Modules emit events using <code>emit_event(event)</code> - Queue is automatically available to nested modules</p> <p>Flow: <pre><code>User calls module.astream()\n    \u2193\nastream() creates queue, sets in ContextVar\n    \u2193\nastream() calls aexecute(stream=True)\n    \u2193\naexecute() does work, emits events via emit_event()\n    \u2193\nastream() yields events from queue\n    \u2193\nFinal Prediction is yielded\n</code></pre></p> <p>Example: <pre><code>async for event in predictor.astream(question=\"What is AI?\"):\n    if isinstance(event, OutputStreamChunk):\n        print(event.delta, end=\"\", flush=True)  # Real-time output\n    elif isinstance(event, Prediction):\n        result = event  # Final result\n</code></pre></p>"},{"location":"architecture/overview/#how-non-streaming-works","title":"How Non-Streaming Works","text":"<p>Simple: - <code>aforward()</code> calls <code>aexecute(stream=False)</code> - No queue is set up - Events are not emitted (or silently ignored) - Only final Prediction is returned</p> <pre><code>result = await predictor.aforward(question=\"What is AI?\")\nprint(result.answer)  # Just the final answer\n</code></pre>"},{"location":"architecture/overview/#composing-modules","title":"Composing Modules","text":"<p>Modules can contain other modules:</p> <pre><code>class Pipeline(Module):\n    def __init__(self):\n        self.step1 = Predict(Signature1)\n        self.step2 = ChainOfThought(Signature2)\n\n    async def aexecute(self, *, stream: bool = False, **inputs):\n        # Get result from first module (don't stream intermediate steps)\n        result1 = await self.step1.aforward(**inputs)\n\n        # Stream final module if requested\n        result2 = await self.step2.aforward(\n            input=result1.output,\n            stream=stream  # Pass down stream parameter\n        )\n\n        return Prediction(final=result2.answer)\n</code></pre> <p>Key Pattern: Nested modules automatically emit to the active queue if one exists.</p>"},{"location":"architecture/overview/#lm-abstraction-language-model-layer","title":"LM Abstraction (Language Model Layer)","text":""},{"location":"architecture/overview/#overview","title":"Overview","text":"<p>The LM (Language Model) abstraction provides a provider-agnostic interface for interacting with LLMs. This allows udspy to work with different providers (OpenAI, Anthropic, local models, etc.) through a common interface.</p> <p>Location: <code>src/udspy/lm/</code></p> <p>Key Files: - <code>lm/base.py</code> - Abstract LM interface - <code>lm/openai.py</code> - OpenAI implementation - <code>lm/__init__.py</code> - Public API exports</p>"},{"location":"architecture/overview/#interface","title":"Interface","text":"<pre><code>from abc import ABC, abstractmethod\n\nclass LM(ABC):\n    \"\"\"Abstract language model interface.\"\"\"\n\n    @abstractmethod\n    async def acomplete(\n        self,\n        messages: list[dict[str, Any]],\n        *,\n        model: str,\n        tools: list[dict[str, Any]] | None = None,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; Any | AsyncGenerator[Any, None]:\n        \"\"\"Generate completion from the language model.\n\n        Args:\n            messages: List of messages in OpenAI format\n            model: Model identifier (e.g., \"gpt-4o\")\n            tools: Optional list of tool schemas in OpenAI format\n            stream: If True, return an async generator of chunks\n            **kwargs: Provider-specific parameters\n\n        Returns:\n            If stream=False: Completion response object\n            If stream=True: AsyncGenerator yielding chunks\n        \"\"\"\n        pass\n</code></pre> <p>Design Decisions: - Single method interface - simple and focused - OpenAI message format as standard (widely adopted) - Generic return types to support any provider - Provider implementations handle format conversion internally</p>"},{"location":"architecture/overview/#openai-implementation","title":"OpenAI Implementation","text":"<pre><code>from openai import AsyncOpenAI\nfrom udspy.lm import OpenAILM\n\n# Create instance\nclient = AsyncOpenAI(api_key=\"sk-...\")\nlm = OpenAILM(client, default_model=\"gpt-4o\")\n\n# Use directly\nresponse = await lm.acomplete(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    temperature=0.7\n)\n</code></pre> <p>Features: - Wraps AsyncOpenAI client - Supports default model (optional, can override per call) - Passes through all OpenAI parameters - Handles both streaming and non-streaming</p>"},{"location":"architecture/overview/#settings-integration","title":"Settings Integration","text":"<p>The LM abstraction integrates with udspy's settings system:</p> <pre><code>import udspy\nfrom udspy import LM\n\n# Configure from environment variables (creates OpenAILM automatically)\n# Set: UDSPY_LM_MODEL=gpt-4o, UDSPY_LM_API_KEY=sk-...\nudspy.settings.configure()\n\n# Or provide explicit LM instance\nlm = LM(model=\"gpt-4o\", api_key=\"sk-...\")\nudspy.settings.configure(lm=lm)\n\n# Access the configured LM\nlm = udspy.settings.lm  # Returns OpenAILM instance\n</code></pre> <p>Backward Compatibility: <code>settings.aclient</code> still works but is deprecated. Use <code>settings.lm</code> for new code.</p>"},{"location":"architecture/overview/#context-manager-support","title":"Context Manager Support","text":"<p>LM instances can be overridden per-context:</p> <pre><code>from udspy import LM\n\n# Global settings\nglobal_lm = LM(model=\"gpt-4o-mini\", api_key=\"global-key\")\nudspy.settings.configure(lm=global_lm)\n\n# Temporary override\ncustom_lm = LM(model=\"gpt-4\", api_key=\"custom-key\")\nwith udspy.settings.context(lm=custom_lm):\n    result = predictor(question=\"...\")  # Uses custom_lm\n\n# Back to global LM\nresult = predictor(question=\"...\")  # Uses global LM\n</code></pre> <p>Priority: 1. Explicit <code>lm</code> parameter (highest) 2. <code>aclient</code> parameter (creates OpenAILM wrapper) 3. <code>api_key</code> parameter (creates new client + LM) 4. Global settings (fallback)</p>"},{"location":"architecture/overview/#usage-in-predict-module","title":"Usage in Predict Module","text":"<p>The Predict module accesses LMs via <code>settings.lm</code>:</p> <pre><code># Non-streaming\nresponse = await settings.lm.acomplete(\n    messages=messages,\n    model=model or settings.default_model,\n    tools=tool_schemas,\n    stream=False,\n    **kwargs\n)\n\n# Streaming\nstream = await settings.lm.acomplete(\n    messages=messages,\n    model=model or settings.default_model,\n    tools=tool_schemas,\n    stream=True,\n    **kwargs\n)\n</code></pre> <p>This centralizes all LLM calls and makes provider swapping trivial.</p>"},{"location":"architecture/overview/#implementing-custom-providers","title":"Implementing Custom Providers","text":"<p>To add a new provider, implement the <code>LM</code> interface:</p> <pre><code>from udspy.lm import LM\n\nclass AnthropicLM(LM):\n    \"\"\"Anthropic Claude implementation.\"\"\"\n\n    def __init__(self, api_key: str, default_model: str | None = None):\n        from anthropic import AsyncAnthropic\n        self.client = AsyncAnthropic(api_key=api_key)\n        self.default_model = default_model\n\n    async def acomplete(\n        self,\n        messages: list[dict[str, Any]],\n        *,\n        model: str | None = None,\n        tools: list[dict[str, Any]] | None = None,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; Any | AsyncGenerator[Any, None]:\n        actual_model = model or self.default_model\n        if not actual_model:\n            raise ValueError(\"No model specified\")\n\n        # Convert OpenAI format to Anthropic format\n        anthropic_messages = self._convert_messages(messages)\n        anthropic_tools = self._convert_tools(tools) if tools else None\n\n        # Call Anthropic API\n        return await self.client.messages.create(\n            model=actual_model,\n            messages=anthropic_messages,\n            tools=anthropic_tools,\n            stream=stream,\n            **kwargs\n        )\n\n    def _convert_messages(self, messages):\n        \"\"\"Convert OpenAI \u2192 Anthropic format.\"\"\"\n        # Implementation details...\n</code></pre> <p>Usage: <pre><code>from my_providers import AnthropicLM\n\nlm = AnthropicLM(api_key=\"sk-ant-...\", default_model=\"claude-3-5-sonnet\")\nudspy.settings.configure(lm=lm)\n\n# All udspy features work with your custom provider!\n</code></pre></p>"},{"location":"architecture/overview/#message-format-standard","title":"Message Format Standard","text":"<p>LM implementations should accept/return OpenAI message format:</p> <pre><code>[\n    {\"role\": \"system\", \"content\": \"You are helpful.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"},\n    {\"role\": \"assistant\", \"content\": \"Hi!\"},\n]\n</code></pre> <p>Why OpenAI format? - Industry standard - Simple and flexible - Easy to convert to other formats - Well-documented</p> <p>Custom providers convert internally.</p>"},{"location":"architecture/overview/#responsibility-boundary","title":"Responsibility Boundary","text":"<p>LM Layer Owns: - Making API calls to providers - Handling streaming vs non-streaming responses - Provider-specific parameter passing - Format conversion (provider \u2194 OpenAI format)</p> <p>LM Layer Does NOT Own: - Prompt formatting (Adapter Layer) - Output parsing (Adapter Layer) - Tool execution (Module Layer) - Retry/error handling (Module Layer) - Orchestration logic (Module Layer)</p>"},{"location":"architecture/overview/#related-documentation","title":"Related Documentation","text":"<p>See LM Abstraction for comprehensive documentation including: - Detailed API reference - Custom provider implementation guide - Context manager examples - Type handling - Best practices</p>"},{"location":"architecture/overview/#signatures","title":"Signatures","text":""},{"location":"architecture/overview/#purpose","title":"Purpose","text":"<p>Signatures define the contract between user inputs and LLM outputs.</p>"},{"location":"architecture/overview/#anatomy","title":"Anatomy","text":"<pre><code>class QA(Signature):\n    \"\"\"Answer questions concisely.\"\"\"  # \u2190 Instructions (docstring)\n\n    question: str = InputField(description=\"User's question\")  # \u2190 Input\n    answer: str = OutputField(description=\"Concise answer\")    # \u2190 Output\n</code></pre>"},{"location":"architecture/overview/#how-they-work","title":"How They Work","text":"<ol> <li>Definition: User defines Signature with InputField/OutputField</li> <li>Validation: SignatureMeta validates all fields are marked</li> <li>Formatting: Adapter converts to system prompt:    <pre><code>Answer questions concisely.\n\nInputs:\n- question (str): User's question\n\nOutputs:\n- answer (str): Concise answer\n</code></pre></li> <li>Parsing: Adapter parses LLM output into structured dict</li> <li>Return: Module returns Prediction with typed attributes</li> </ol>"},{"location":"architecture/overview/#field-extraction","title":"Field Extraction","text":"<pre><code># Get inputs\nQA.get_input_fields()  # {\"question\": FieldInfo(...)}\n\n# Get outputs\nQA.get_output_fields()  # {\"answer\": FieldInfo(...)}\n\n# Get instructions\nQA.get_instructions()  # \"Answer questions concisely.\"\n</code></pre>"},{"location":"architecture/overview/#dynamic-creation","title":"Dynamic Creation","text":"<pre><code># String format (all fields are str)\nQA = Signature.from_string(\"question -&gt; answer\", \"Answer questions\")\n\n# Programmatic (custom types)\nQA = make_signature(\n    input_fields={\"question\": str},\n    output_fields={\"answer\": str},\n    instructions=\"Answer questions\"\n)\n</code></pre>"},{"location":"architecture/overview/#where-signatures-live","title":"Where Signatures Live","text":"<p>Module Layer: Modules accept signatures to define I/O Adapter Layer: Formats signatures into prompts NOT in LM Layer: LM layer only sees formatted messages</p>"},{"location":"architecture/overview/#tools","title":"Tools","text":""},{"location":"architecture/overview/#lifecycle","title":"Lifecycle","text":"<pre><code>1. Definition (by user)\n   \u2193\n2. Schema Extraction (Tool.__init__)\n   \u2193\n3. Schema Conversion (Adapter.tools_to_openai_format)\n   \u2193\n4. LLM Call (Module \u2192 LM)\n   \u2193\n5. LLM Returns Tool Calls\n   \u2193\n6. Tool Execution (Module calls Tool.acall)\n   \u2193\n7. Result Formatting (back to messages)\n   \u2193\n8. Loop until final answer\n</code></pre>"},{"location":"architecture/overview/#definition","title":"Definition","text":"<pre><code>from pydantic import Field\nfrom udspy import tool\n\n@tool(name=\"calculator\", description=\"Perform arithmetic\")\ndef calculator(\n    operation: str = Field(description=\"add, subtract, multiply, divide\"),\n    a: float = Field(description=\"First number\"),\n    b: float = Field(description=\"Second number\"),\n) -&gt; float:\n    \"\"\"Perform arithmetic operations.\"\"\"\n    ops = {\"add\": a + b, \"subtract\": a - b, ...}\n    return ops[operation]\n</code></pre>"},{"location":"architecture/overview/#schema-extraction","title":"Schema Extraction","text":"<p>Tool class extracts schema from function signature:</p> <pre><code># Automatic schema generation\ntool.parameters \u2192\n{\n    \"type\": \"object\",\n    \"properties\": {\n        \"operation\": {\"type\": \"string\", \"description\": \"add, subtract, ...\"},\n        \"a\": {\"type\": \"number\", \"description\": \"First number\"},\n        \"b\": {\"type\": \"number\", \"description\": \"Second number\"}\n    },\n    \"required\": [\"operation\", \"a\", \"b\"]\n}\n</code></pre>"},{"location":"architecture/overview/#conversion-to-openai-format","title":"Conversion to OpenAI Format","text":"<p>Adapter converts Tool \u2192 OpenAI schema:</p> <pre><code># Tool provides the parameters schema\nfrom udspy.adapter import ChatAdapter\n\nadapter = ChatAdapter()\nadapter.format_tool_schema(tool) \u2192\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"calculator\",\n        \"description\": \"Perform arithmetic\",\n        \"parameters\": tool.parameters  # Tool provides this\n    }\n}\n</code></pre>"},{"location":"architecture/overview/#execution","title":"Execution","text":"<pre><code># In Predict module:\nasync def _execute_tool_calls(self, tool_calls, tools):\n    results = []\n    for tc in tool_calls:\n        tool = self._find_tool(tc.function.name, tools)\n        result = await tool.acall(**tc.arguments)\n        results.append({\"role\": \"tool\", \"tool_call_id\": tc.id, \"content\": result})\n    return results\n</code></pre>"},{"location":"architecture/overview/#with-confirmation","title":"With Confirmation","text":"<pre><code>@tool(require_confirmation=True)\ndef delete_file(path: str) -&gt; str:\n    os.remove(path)\n    return f\"Deleted {path}\"\n\n# Raises ConfirmationRequired when called\n# Module catches, saves state, waits for user approval\n# Resumes execution after approval\n</code></pre>"},{"location":"architecture/overview/#history","title":"History","text":""},{"location":"architecture/overview/#purpose_1","title":"Purpose","text":"<p>Store conversation context for multi-turn interactions.</p>"},{"location":"architecture/overview/#structure","title":"Structure","text":"<p>Simple wrapper around <code>list[dict[str, Any]]</code> with convenience methods:</p> <pre><code>class History:\n    messages: list[dict[str, Any]]\n\n    def add_user_message(self, content: str)\n    def add_assistant_message(self, content: str, tool_calls: list | None)\n    def add_tool_result(self, tool_call_id: str, content: str)\n    def add_system_message(self, content: str)  # Appends to end\n    def set_system_message(self, content: str)  # Always at position 0 (recommended)\n</code></pre> <p>Note: Use <code>set_system_message()</code> instead of <code>add_system_message()</code> to ensure the system prompt is always at position 0. When using Predict, the system prompt is automatically managed.</p>"},{"location":"architecture/overview/#usage","title":"Usage","text":"<pre><code>history = History()\n\n# First turn\nresult = predictor(question=\"What is Python?\", history=history)\n# history.messages = [\n#     {\"role\": \"system\", \"content\": \"...instructions...\"},\n#     {\"role\": \"user\", \"content\": \"question: What is Python?\"},\n#     {\"role\": \"assistant\", \"content\": \"answer: A programming language\"}\n# ]\n\n# Second turn (context preserved)\nresult = predictor(question=\"What are its features?\", history=history)\n# LLM sees full conversation history\n</code></pre>"},{"location":"architecture/overview/#how-predict-uses-history","title":"How Predict Uses History","text":"<p>Predict automatically manages the system prompt in history:</p> <pre><code>def _build_initial_messages(self, signature, inputs, history):\n    # Always set system message at position 0 (replaces if exists)\n    history.set_system_message(\n        self.adapter.format_instructions(signature)\n    )\n\n    # Add current user input\n    history.add_user_message(\n        self.adapter.format_inputs(signature, inputs)\n    )\n</code></pre> <p>Key behaviors: 1. System prompt is always at position 0 - Managed automatically from signature 2. User message added at the end - Current input appended to history 3. After generation - Assistant response added to history 4. Tool calls recorded - Tool interactions preserved in history</p> <p>This means you can pre-populate history with only user/assistant messages, and the system prompt will be automatically managed.</p>"},{"location":"architecture/overview/#when-to-use","title":"When to Use","text":"<ul> <li>Multi-turn conversations: Chatbots, assistants</li> <li>Context-dependent tasks: \"It\" and \"that\" references</li> <li>Iterative refinement: Follow-up questions</li> </ul>"},{"location":"architecture/overview/#when-not-to-use","title":"When NOT to Use","text":"<ul> <li>Stateless tasks: One-off questions</li> <li>Independent requests: No cross-request context needed</li> </ul>"},{"location":"architecture/overview/#streaming","title":"Streaming","text":""},{"location":"architecture/overview/#architecture","title":"Architecture","text":"<p>Event Queue: ContextVar-based queue for thread-safe event emission</p> <pre><code>_stream_queue: ContextVar[asyncio.Queue | None] = ContextVar(\"_stream_queue\", default=None)\n\nasync def emit_event(event: StreamEvent):\n    \"\"\"Emit event to active stream (if any).\"\"\"\n    queue = _stream_queue.get()\n    if queue is not None:\n        await queue.put(event)\n</code></pre>"},{"location":"architecture/overview/#event-types","title":"Event Types","text":"<p>Base Class: <pre><code>class StreamEvent:\n    \"\"\"Base for all events.\"\"\"\n    pass\n</code></pre></p> <p>Built-in Events: - <code>OutputStreamChunk</code> - LLM output for a field - <code>ThoughtStreamChunk</code> - Reasoning/thought output - <code>Prediction</code> - Final result</p> <p>Custom Events: <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass ToolProgress(StreamEvent):\n    tool_name: str\n    progress: float\n    message: str\n\n# Emit from anywhere\nemit_event(ToolProgress(\"search\", 0.5, \"Searching...\"))\n</code></pre></p>"},{"location":"architecture/overview/#flow","title":"Flow","text":"<pre><code>1. User calls module.astream()\n   \u2193\n2. astream() creates Queue, sets in _stream_queue ContextVar\n   \u2193\n3. astream() spawns task: aexecute(stream=True)\n   \u2193\n4. aexecute() does work, calls emit_event() for chunks\n   \u2193\n5. emit_event() puts events in queue\n   \u2193\n6. astream() yields events from queue\n   \u2193\n7. Final Prediction is yielded\n</code></pre>"},{"location":"architecture/overview/#chunk-structure","title":"Chunk Structure","text":"<pre><code>@dataclass\nclass OutputStreamChunk(StreamEvent):\n    module: Module          # Which module emitted this\n    field_name: str         # Which output field\n    delta: str              # New content since last chunk\n    content: str            # Full accumulated content so far\n    is_complete: bool       # Is this field done?\n</code></pre>"},{"location":"architecture/overview/#usage-pattern","title":"Usage Pattern","text":"<pre><code>async for event in predictor.astream(question=\"Explain AI\"):\n    if isinstance(event, OutputStreamChunk):\n        if event.field_name == \"answer\":\n            print(event.delta, end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        result = event\n        print(f\"\\n\\nFinal: {result.answer}\")\n</code></pre>"},{"location":"architecture/overview/#nested-modules","title":"Nested Modules","text":"<p>Events from nested modules bubble up automatically:</p> <pre><code>class Pipeline(Module):\n    async def aexecute(self, *, stream: bool = False, **inputs):\n        # Nested module emits to same queue\n        async for event in self.predictor.aexecute(stream=stream, **inputs):\n            # Events automatically go to active queue\n            if isinstance(event, Prediction):\n                return event\n</code></pre>"},{"location":"architecture/overview/#confirmation-suspendresume","title":"Confirmation &amp; Suspend/Resume","text":""},{"location":"architecture/overview/#purpose_2","title":"Purpose","text":"<p>Enable human-in-the-loop patterns where execution pauses for user input.</p>"},{"location":"architecture/overview/#flow_1","title":"Flow","text":"<pre><code>1. Tool/Module raises ConfirmationRequired\n   \u2193\n2. Exception propagates to user code\n   \u2193\n3. User sees question, responds\n   \u2193\n4. User creates ResumeState(exception, response)\n   \u2193\n5. User calls module.aforward(resume_state=resume_state)\n   \u2193\n6. Module resumes execution with user response\n   \u2193\n7. Execution completes\n</code></pre>"},{"location":"architecture/overview/#exceptions","title":"Exceptions","text":"<pre><code>class ConfirmationRequired(Exception):\n    question: str                    # What to ask user\n    confirmation_id: str             # Unique ID\n    tool_call: ToolCall | None       # If raised by tool\n    context: dict[str, Any]          # Module-specific state\n\nclass ConfirmationRejected(Exception):\n    message: str                     # Why rejected\n    confirmation_id: str             # Which confirmation\n    tool_call: ToolCall | None       # If raised by tool\n</code></pre>"},{"location":"architecture/overview/#resume-state","title":"Resume State","text":"<pre><code>class ResumeState:\n    exception: ConfirmationRequired  # Original exception\n    user_response: str               # User's answer (\"yes\", \"no\", JSON, etc.)\n</code></pre>"},{"location":"architecture/overview/#usage-loop-pattern","title":"Usage: Loop Pattern","text":"<pre><code>from udspy import ResumeState\n\nresume_state = None\n\nwhile True:\n    try:\n        result = agent(\n            question=\"Delete all files\",\n            resume_state=resume_state\n        )\n        break  # Success\n    except ConfirmationRequired as e:\n        print(f\"\\n{e.question}\")\n        user_input = input(\"Approve? (yes/no): \")\n        resume_state = ResumeState(e, user_input)\n</code></pre>"},{"location":"architecture/overview/#usage-streaming-pattern","title":"Usage: Streaming Pattern","text":"<pre><code>async for event in agent.astream(question=\"Delete files\"):\n    if isinstance(event, Prediction):\n        if not event.is_final:\n            # Has pending tool calls requiring confirmation\n            for tc in event.native_tool_calls:\n                # Show confirmation UI\n                approved = await ask_user(tc)\n\n                # Resume with response\n                resume_state = ResumeState(exception, \"yes\" if approved else \"no\")\n\n                # Continue streaming\n                async for event2 in agent.astream(resume_state=resume_state):\n                    yield event2\n</code></pre>"},{"location":"architecture/overview/#module-implementation","title":"Module Implementation","text":"<p>Modules that support suspend/resume must implement:</p> <pre><code>class MyModule(Module):\n    async def aexecute(self, *, stream: bool = False, resume_state=None, **inputs):\n        # Check for resume\n        if resume_state:\n            return await self.aresume(\n                user_response=resume_state.user_response,\n                saved_state=resume_state.exception.context\n            )\n\n        # Normal execution\n        try:\n            result = await self.do_work()\n            return Prediction(**result)\n        except ConfirmationRequired as e:\n            # Save state in exception context\n            e.context[\"saved_data\"] = self.state\n            raise  # Let user handle\n\n    async def aresume(self, user_response: str, saved_state: dict):\n        # Restore state\n        self.state = saved_state[\"saved_data\"]\n\n        # Process user response\n        if user_response == \"yes\":\n            # Continue\n            return await self.do_work()\n        else:\n            # Abort\n            raise ConfirmationRejected(\"User rejected\")\n</code></pre>"},{"location":"architecture/overview/#decorator-for-tools","title":"Decorator for Tools","text":"<pre><code>from udspy import tool, confirm_first\n\n@tool(require_confirmation=True)\ndef delete_file(path: str) -&gt; str:\n    os.remove(path)\n    return f\"Deleted {path}\"\n\n# Or manually\n@confirm_first\ndef delete_file(path: str) -&gt; str:\n    os.remove(path)\n    return f\"Deleted {path}\"\n</code></pre>"},{"location":"architecture/overview/#callbacks","title":"Callbacks","text":""},{"location":"architecture/overview/#purpose_3","title":"Purpose","text":"<p>Provide hooks for telemetry, monitoring, and observability.</p>"},{"location":"architecture/overview/#interface_1","title":"Interface","text":"<pre><code>class BaseCallback:\n    def on_module_start(self, call_id: str, instance: Module, inputs: dict):\n        \"\"\"Called when module execution starts.\"\"\"\n        pass\n\n    def on_module_end(self, call_id: str, outputs: Any, exception: Exception | None):\n        \"\"\"Called when module execution ends.\"\"\"\n        pass\n\n    def on_lm_start(self, call_id: str, instance: Any, inputs: dict):\n        \"\"\"Called when LLM call starts.\"\"\"\n        pass\n\n    def on_lm_end(self, call_id: str, outputs: Any, exception: Exception | None):\n        \"\"\"Called when LLM call ends.\"\"\"\n        pass\n\n    def on_tool_start(self, call_id: str, instance: Tool, inputs: dict):\n        \"\"\"Called when tool execution starts.\"\"\"\n        pass\n\n    def on_tool_end(self, call_id: str, outputs: Any, exception: Exception | None):\n        \"\"\"Called when tool execution ends.\"\"\"\n        pass\n</code></pre>"},{"location":"architecture/overview/#usage_1","title":"Usage","text":"<pre><code>class LoggingCallback(BaseCallback):\n    def on_lm_start(self, call_id, instance, inputs):\n        print(f\"[{call_id}] LLM called\")\n        print(f\"  Model: {inputs.get('model')}\")\n        print(f\"  Messages: {len(inputs.get('messages', []))}\")\n\n    def on_lm_end(self, call_id, outputs, exception):\n        if exception:\n            print(f\"[{call_id}] LLM failed: {exception}\")\n        else:\n            print(f\"[{call_id}] LLM completed\")\n\n# Configure globally\nudspy.settings.configure(callbacks=[LoggingCallback()])\n\n# Or per-context\nwith udspy.settings.context(callbacks=[LoggingCallback()]):\n    result = predictor(question=\"...\")\n</code></pre>"},{"location":"architecture/overview/#integration","title":"Integration","text":"<p>Callbacks are triggered by <code>@with_callbacks</code> decorator:</p> <pre><code>@with_callbacks\nasync def aexecute(self, *, stream: bool = False, **inputs):\n    # Callbacks automatically called before/after\n    pass\n</code></pre>"},{"location":"architecture/overview/#compatibility","title":"Compatibility","text":"<p>Compatible with: - Opik: MLOps platform for LLM applications - MLflow: ML experiment tracking - Custom: Any monitoring system</p>"},{"location":"architecture/overview/#how-to-extend","title":"How to Extend","text":""},{"location":"architecture/overview/#adding-a-new-module","title":"Adding a New Module","text":"<ol> <li>Subclass Module</li> <li>Implement aexecute()</li> <li>Emit events (if streaming)</li> <li>Return Prediction</li> </ol> <pre><code>from udspy import Module, Prediction, OutputStreamChunk, emit_event\n\nclass MyModule(Module):\n    def __init__(self, signature):\n        self.signature = signature\n        self.predictor = Predict(signature)\n\n    async def aexecute(self, *, stream: bool = False, **inputs):\n        # Custom pre-processing\n        processed = self.preprocess(inputs)\n\n        # Call nested module\n        result = await self.predictor.aforward(**processed)\n\n        # Custom post-processing\n        final = self.postprocess(result)\n\n        # Anything listening to stream gets this chunk\n        emit_event(OutputStreamChunk(\n            module=self,\n            field_name=\"answer\",\n            delta=final[\"answer\"],\n            content=final[\"answer\"],\n            is_complete=True\n        ))\n\n        # Return final prediction\n        return Prediction(**final)\n</code></pre>"},{"location":"architecture/overview/#adding-a-new-lm-provider","title":"Adding a New LM Provider","text":"<ol> <li>Create src/udspy/lm/ package</li> <li>Define LM base class</li> <li>Implement provider-specific class</li> <li>Update settings to use LM</li> </ol> <pre><code># src/udspy/lm/base.py\nfrom abc import ABC, abstractmethod\n\nclass LM(ABC):\n    @abstractmethod\n    async def acomplete(self, messages, *, tools=None, stream=False, **kwargs):\n        pass\n\n# src/udspy/lm/anthropic.py\nclass AnthropicLM(LM):\n    def __init__(self, client, model):\n        self.client = client\n        self.model = model\n\n    async def acomplete(self, messages, *, tools=None, stream=False, **kwargs):\n        # Convert formats\n        # Call Anthropic API\n        # Convert response\n        pass\n</code></pre>"},{"location":"architecture/overview/#adding-custom-stream-events","title":"Adding Custom Stream Events","text":"<pre><code>from dataclasses import dataclass\nfrom udspy.streaming import StreamEvent, emit_event\n\n@dataclass\nclass CustomEvent(StreamEvent):\n    message: str\n    progress: float\n\n# Emit from anywhere\nasync def my_function():\n    emit_event(CustomEvent(\"Processing...\", 0.5))\n</code></pre>"},{"location":"architecture/overview/#adding-custom-tool-logic","title":"Adding Custom Tool Logic","text":"<pre><code>from udspy import Tool\nfrom pydantic import Field\n\n@tool(name=\"custom_tool\")\nasync def custom_tool(param: str = Field(...)) -&gt; str:\n    \"\"\"Custom tool with async logic.\"\"\"\n    result = await async_operation(param)\n    return result\n</code></pre>"},{"location":"architecture/overview/#design-patterns","title":"Design Patterns","text":""},{"location":"architecture/overview/#1-async-first-sync-wrappers","title":"1. Async-First, Sync Wrappers","text":"<p>Pattern: All core logic is async, sync is a wrapper</p> <pre><code>async def aforward(self, **inputs) -&gt; Prediction:\n    \"\"\"Async implementation.\"\"\"\n    return await self.do_async_work(inputs)\n\ndef forward(self, **inputs) -&gt; Prediction:\n    \"\"\"Sync wrapper.\"\"\"\n    ensure_sync_context(\"forward\")\n    return asyncio.run(self.aforward(**inputs))\n</code></pre> <p>Why: Async is more flexible, can't go async\u2192sync\u2192async</p>"},{"location":"architecture/overview/#2-single-execution-path","title":"2. Single Execution Path","text":"<p>Pattern: One <code>aexecute()</code> implementation for both streaming and non-streaming</p> <pre><code>async def aexecute(self, *, stream: bool = False, **inputs):\n    # Check if should stream\n    emit_event(chunk)\n\n    # Always return final result\n    return Prediction(...)\n</code></pre> <p>Why: DRY, easier to maintain, composable</p>"},{"location":"architecture/overview/#3-event-queue-via-contextvar","title":"3. Event Queue via ContextVar","text":"<p>Pattern: Thread-safe, async-safe event queue</p> <pre><code>_stream_queue: ContextVar[Queue | None] = ContextVar(\"_stream_queue\", default=None)\n\nasync def astream(self, **inputs):\n    queue = asyncio.Queue()\n    token = _stream_queue.set(queue)\n    try:\n        # Execute and yield from queue\n        async for event in self._yield_from_queue(queue):\n            yield event\n    finally:\n        _stream_queue.reset(token)\n</code></pre> <p>Why: Works across async tasks, no global state</p>"},{"location":"architecture/overview/#4-context-based-configuration","title":"4. Context-Based Configuration","text":"<p>Pattern: Thread-safe configuration overrides</p> <pre><code>from udspy import LM\n\n# Global\nglobal_lm = LM(model=\"gpt-4o-mini\", api_key=\"sk-...\")\nudspy.settings.configure(lm=global_lm)\n\n# Context-specific\ngpt4_lm = LM(model=\"gpt-4\", api_key=\"sk-...\")\nwith udspy.settings.context(lm=gpt4_lm):\n    result = predictor(...)  # Uses gpt-4\n\n# Back to global\nresult = predictor(...)  # Uses gpt-4o-mini\n</code></pre> <p>Why: Multi-tenant safe, no parameter drilling</p>"},{"location":"architecture/overview/#5-exception-based-flow-control","title":"5. Exception-Based Flow Control","text":"<p>Pattern: Use exceptions for suspend/resume</p> <pre><code>try:\n    result = agent(question=\"...\")\nexcept ConfirmationRequired as e:\n    response = input(e.question)\n    result = agent(resume_state=ResumeState(e, response))\n</code></pre> <p>Why: Clean interrupt, preserves state, composable</p>"},{"location":"architecture/overview/#decision-tree","title":"Decision Tree","text":""},{"location":"architecture/overview/#where-should-this-code-go","title":"Where should this code go?","text":"<pre><code>Is it about LLM provider API calls?\n\u251c\u2500 YES \u2192 LM Layer (future: src/udspy/lm/)\n\u2514\u2500 NO\n   \u251c\u2500 Is it about message formatting or parsing?\n   \u2502  \u2514\u2500 YES \u2192 Adapter Layer (src/udspy/adapter.py)\n   \u2514\u2500 NO\n      \u251c\u2500 Is it about business logic or orchestration?\n      \u2502  \u2514\u2500 YES \u2192 Module Layer (src/udspy/module/)\n      \u2514\u2500 NO\n         \u251c\u2500 Is it about conversation storage?\n         \u2502  \u2514\u2500 YES \u2192 History (src/udspy/history.py)\n         \u251c\u2500 Is it about tool definition or execution?\n         \u2502  \u2514\u2500 YES \u2192 Tool (src/udspy/tool.py)\n         \u251c\u2500 Is it about streaming events?\n         \u2502  \u2514\u2500 YES \u2192 Streaming (src/udspy/streaming.py)\n         \u251c\u2500 Is it about human-in-the-loop?\n         \u2502  \u2514\u2500 YES \u2192 Confirmation (src/udspy/confirmation.py)\n         \u251c\u2500 Is it about telemetry?\n         \u2502  \u2514\u2500 YES \u2192 Callbacks (src/udspy/callback.py)\n         \u2514\u2500 Is it a utility used everywhere?\n            \u2514\u2500 YES \u2192 Utils (src/udspy/utils.py)\n</code></pre>"},{"location":"architecture/overview/#should-this-be-a-new-module","title":"Should this be a new module?","text":"<pre><code>Does it encapsulate LLM call logic?\n\u251c\u2500 NO \u2192 Not a module (maybe a helper/utility)\n\u2514\u2500 YES\n   \u251c\u2500 Is it a variant of Predict?\n   \u2502  \u251c\u2500 YES \u2192 Probably wrapper (like ChainOfThought)\n   \u2502  \u2514\u2500 NO \u2192 Custom module\n   \u2514\u2500 Does it need custom orchestration?\n      \u251c\u2500 YES \u2192 Create new module (like ReAct)\n      \u2514\u2500 NO \u2192 Compose existing modules\n</code></pre>"},{"location":"architecture/overview/#should-this-be-in-the-lm-layer","title":"Should this be in the LM layer?","text":"<pre><code>Does it talk to an LLM provider API?\n\u251c\u2500 NO \u2192 Not LM layer\n\u2514\u2500 YES\n   \u251c\u2500 Is it provider-specific (OpenAI, Anthropic)?\n   \u2502  \u2514\u2500 YES \u2192 LM implementation (src/udspy/lm/openai.py)\n   \u2514\u2500 Is it provider-agnostic (retry, rate limiting)?\n      \u2514\u2500 YES \u2192 LM base (src/udspy/lm/base.py)\n</code></pre>"},{"location":"architecture/overview/#summary","title":"Summary","text":""},{"location":"architecture/overview/#responsibilities-at-a-glance","title":"Responsibilities at a Glance","text":"Layer Responsibilities Key Files Module Business logic, orchestration, composition <code>module/predict.py</code>, <code>module/react.py</code> LM Provider API calls, retries, format conversion (Future) <code>lm/openai.py</code>, <code>lm/anthropic.py</code> Adapter Message formatting, output parsing, schema conversion <code>adapter.py</code> Signature I/O contracts, validation <code>signature.py</code> Tool Function wrapping, schema extraction, execution <code>tool.py</code> History Conversation storage <code>history.py</code> Streaming Event queue, chunks, emission <code>streaming.py</code> Confirmation Suspend/resume, human-in-the-loop <code>confirmation.py</code> Callbacks Telemetry, monitoring hooks <code>callback.py</code> Settings Configuration, client management <code>settings.py</code>"},{"location":"architecture/overview/#core-patterns","title":"Core Patterns","text":"<ol> <li>Async-first - All core operations are async</li> <li>Single execution path - <code>aexecute()</code> powers everything</li> <li>Event queue - ContextVar for streaming</li> <li>Exception-based flow - ConfirmationRequired for suspend/resume</li> <li>Context-based config - Thread-safe overrides</li> </ol>"},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<p>For implementation details, see: - Modules Deep Dive - Signatures Deep Dive - Streaming Deep Dive - Confirmation Deep Dive - Callbacks Deep Dive - Architectural Decisions</p>"},{"location":"architecture/signatures/","title":"Signatures","text":"<p>Signatures define the input/output contract for LLM tasks using Pydantic models.</p>"},{"location":"architecture/signatures/#creating-signatures","title":"Creating Signatures","text":"<p>There are three ways to create signatures in udspy:</p>"},{"location":"architecture/signatures/#1-string-signatures-quick-simple","title":"1. String Signatures (Quick &amp; Simple)","text":"<p>For rapid prototyping, use the DSPy-style string format:</p> <pre><code>from udspy import Signature\n\n# Simple signature\nQA = Signature.from_string(\"question -&gt; answer\")\n\n# Multiple inputs and outputs\nAnalyze = Signature.from_string(\n    \"context, question -&gt; summary, answer\",\n    \"Analyze text and answer questions\"\n)\n</code></pre> <p>Format: <code>\"input1, input2 -&gt; output1, output2\"</code></p> <ul> <li>All fields default to <code>str</code> type</li> <li>Optional second argument for instructions</li> <li>Great for quick prototyping</li> </ul>"},{"location":"architecture/signatures/#2-class-based-signatures-full-control","title":"2. Class-based Signatures (Full Control)","text":"<p>For production code, use class-based signatures:</p> <pre><code>from udspy import Signature, InputField, OutputField\n\nclass QA(Signature):\n    \"\"\"Answer questions concisely and accurately.\"\"\"\n    question: str = InputField(description=\"Question to answer\")\n    answer: str = OutputField(description=\"Concise answer\")\n</code></pre> <p>Benefits: - Custom field types - Field descriptions for better prompts - IDE autocomplete and type checking - Better for complex signatures</p>"},{"location":"architecture/signatures/#3-dynamic-signatures-programmatic","title":"3. Dynamic Signatures (Programmatic)","text":"<p>For runtime signature creation:</p> <pre><code>from udspy import make_signature\n\nQA = make_signature(\n    input_fields={\"question\": str},\n    output_fields={\"answer\": str},\n    instructions=\"Answer questions concisely\",\n)\n</code></pre>"},{"location":"architecture/signatures/#components","title":"Components","text":""},{"location":"architecture/signatures/#docstring","title":"Docstring","text":"<p>The class docstring becomes the task instruction in the system prompt:</p> <pre><code>class Summarize(Signature):\n    \"\"\"Summarize the given text in 2-3 sentences.\"\"\"\n    text: str = InputField()\n    summary: str = OutputField()\n</code></pre>"},{"location":"architecture/signatures/#inputfield","title":"InputField","text":"<p>Marks a field as an input:</p> <pre><code>question: str = InputField(\n    description=\"Question to answer\",  # Used in prompt\n    default=\"\",  # Optional default value\n)\n</code></pre>"},{"location":"architecture/signatures/#outputfield","title":"OutputField","text":"<p>Marks a field as an output:</p> <pre><code>answer: str = OutputField(\n    description=\"Concise answer\",\n)\n</code></pre>"},{"location":"architecture/signatures/#field-types","title":"Field Types","text":"<p>Signatures support various field types:</p>"},{"location":"architecture/signatures/#primitives","title":"Primitives","text":"<pre><code>class Example(Signature):\n    \"\"\"Example signature.\"\"\"\n    text: str = InputField()\n    count: int = InputField()\n    score: float = InputField()\n    enabled: bool = InputField()\n</code></pre>"},{"location":"architecture/signatures/#collections","title":"Collections","text":"<pre><code>class Example(Signature):\n    \"\"\"Example signature.\"\"\"\n    tags: list[str] = InputField()\n    metadata: dict[str, Any] = InputField()\n</code></pre>"},{"location":"architecture/signatures/#pydantic-models","title":"Pydantic Models","text":"<pre><code>from pydantic import BaseModel\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\nclass Example(Signature):\n    \"\"\"Example signature.\"\"\"\n    person: Person = InputField()\n    related: list[Person] = OutputField()\n</code></pre>"},{"location":"architecture/signatures/#validation","title":"Validation","text":"<p>Signatures use Pydantic for validation:</p> <pre><code>class Sentiment(Signature):\n    \"\"\"Analyze sentiment.\"\"\"\n    text: str = InputField()\n    sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = OutputField()\n\n# Output will be validated to match literal values\n</code></pre>"},{"location":"architecture/signatures/#multi-output-signatures","title":"Multi-Output Signatures","text":"<p>Signatures can have multiple outputs:</p> <pre><code>class ReasonedQA(Signature):\n    \"\"\"Answer with step-by-step reasoning.\"\"\"\n    question: str = InputField()\n    reasoning: str = OutputField(description=\"Reasoning process\")\n    answer: str = OutputField(description=\"Final answer\")\n</code></pre>"},{"location":"architecture/signatures/#best-practices","title":"Best Practices","text":""},{"location":"architecture/signatures/#1-clear-descriptions","title":"1. Clear Descriptions","text":"<pre><code># Good\nquestion: str = InputField(description=\"User's question about the product\")\n\n# Bad\nquestion: str = InputField()\n</code></pre>"},{"location":"architecture/signatures/#2-specific-instructions","title":"2. Specific Instructions","text":"<pre><code># Good\nclass Summarize(Signature):\n    \"\"\"Summarize in exactly 3 bullet points, each under 20 words.\"\"\"\n\n# Bad\nclass Summarize(Signature):\n    \"\"\"Summarize.\"\"\"\n</code></pre>"},{"location":"architecture/signatures/#3-structured-outputs","title":"3. Structured Outputs","text":"<pre><code># Good - use Pydantic models for complex outputs\nclass Analysis(BaseModel):\n    sentiment: str\n    confidence: float\n    keywords: list[str]\n\nclass Analyze(Signature):\n    \"\"\"Analyze text.\"\"\"\n    text: str = InputField()\n    analysis: Analysis = OutputField()\n\n# Bad - use many separate fields\nclass Analyze(Signature):\n    \"\"\"Analyze text.\"\"\"\n    text: str = InputField()\n    sentiment: str = OutputField()\n    confidence: float = OutputField()\n    keywords: list[str] = OutputField()\n</code></pre>"},{"location":"architecture/signatures/#choosing-the-right-approach","title":"Choosing the Right Approach","text":""},{"location":"architecture/signatures/#string-signatures","title":"String Signatures","text":"<p>Use when: - Prototyping quickly - All fields are strings - Signature is simple - You don't need field descriptions</p> <pre><code># Perfect for quick tests\npredictor = Predict(\"question -&gt; answer\")\n</code></pre>"},{"location":"architecture/signatures/#class-based-signatures","title":"Class-based Signatures","text":"<p>Use when: - Building production code - You need custom types - You want field descriptions - Signature is complex</p> <pre><code>class QA(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField(description=\"User's question\")\n    answer: str = OutputField(description=\"Concise answer\")\n</code></pre>"},{"location":"architecture/signatures/#dynamic-signatures","title":"Dynamic Signatures","text":"<p>Use when: - Creating signatures at runtime - Building signature builders/factories - Signature structure depends on config</p> <pre><code># Build signature based on config\nfields = load_field_config()\nMySignature = make_signature(fields[\"inputs\"], fields[\"outputs\"])\n</code></pre>"},{"location":"architecture/signatures/#modules-accept-all-formats","title":"Modules Accept All Formats","text":"<p>All modules automatically recognize string signatures:</p> <pre><code>from udspy import Predict, ChainOfThought, ReAct\n\n# All of these work:\npredictor1 = Predict(\"question -&gt; answer\")\npredictor2 = Predict(QA)  # Class-based\npredictor3 = Predict(make_signature(...))  # Dynamic\n\ncot = ChainOfThought(\"question -&gt; answer\")\nagent = ReAct(\"question -&gt; answer\", tools=[...])\n</code></pre>"},{"location":"architecture/signatures/#api-reference","title":"API Reference","text":"<p>See API: Signatures for detailed API documentation including the full <code>Signature.from_string()</code> reference.</p>"},{"location":"architecture/streaming/","title":"Streaming","text":"<p>Streaming support allows incremental processing of LLM outputs, providing real-time feedback to users.</p>"},{"location":"architecture/streaming/#overview","title":"Overview","text":"<p>All modules support streaming out of the box through the <code>astream()</code> method:</p> <pre><code>from udspy import Predict, OutputStreamChunk, Prediction\n\npredictor = Predict(\"question -&gt; answer\")\n\nasync for event in predictor.astream(question=\"Explain AI\"):\n    if isinstance(event, OutputStreamChunk):\n        print(event.delta, end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        print(f\"\\n\\nFinal result: {event.answer}\")\n</code></pre>"},{"location":"architecture/streaming/#stream-events","title":"Stream Events","text":"<p>The streaming API yields two types of events:</p>"},{"location":"architecture/streaming/#outputstreamchunk","title":"OutputStreamChunk","text":"<p>Incremental text updates for a specific field:</p> <pre><code>class OutputStreamChunk(StreamEvent):\n    module: Module          # Module that generated this chunk\n    field_name: str        # Which output field (e.g., \"answer\")\n    delta: str             # New text since last chunk\n    content: str           # Full accumulated text so far\n    is_complete: bool      # Whether field is done streaming\n</code></pre> <p>Example: <pre><code>async for event in predictor.astream(question=\"...\"):\n    if isinstance(event, OutputStreamChunk):\n        print(f\"[{event.field_name}] {event.delta}\", end=\"\", flush=True)\n        if event.is_complete:\n            print(f\"\\n--- {event.field_name} complete ---\")\n</code></pre></p>"},{"location":"architecture/streaming/#prediction","title":"Prediction","text":"<p>Final result with all output fields:</p> <pre><code>class Prediction(StreamEvent, dict):\n    module: Module | None   # Module that produced this prediction\n    is_final: bool          # True for final result, False for intermediate\n    # Dict with all output fields\n    # Supports both dict and attribute access\n</code></pre> <p>The <code>module</code> and <code>is_final</code> attributes help distinguish predictions in nested module scenarios:</p> <pre><code>async for event in predictor.astream(question=\"...\"):\n    if isinstance(event, Prediction):\n        # Access output fields\n        print(f\"Answer: {event.answer}\")\n        print(f\"Same: {event['answer']}\")\n\n        # Check which module produced this\n        module_name = event.module.__class__.__name__\n        print(f\"From: {module_name}\")\n\n        # Distinguish final vs intermediate predictions\n        if event.is_final:\n            print(\"This is the final result!\")\n</code></pre>"},{"location":"architecture/streaming/#field-specific-streaming","title":"Field-Specific Streaming","text":"<p>Streaming automatically handles multiple output fields:</p> <pre><code>from udspy import ChainOfThought\n\ncot = ChainOfThought(\"question -&gt; answer\")\n\nasync for event in cot.astream(question=\"What is 157 * 234?\"):\n    if isinstance(event, OutputStreamChunk):\n        if event.field_name == \"reasoning\":\n            print(f\"\ud83d\udcad {event.delta}\", end=\"\", flush=True)\n        elif event.field_name == \"answer\":\n            print(f\"\\n\u2713 {event.delta}\", end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        print(f\"\\n\\nComplete!\")\n</code></pre>"},{"location":"architecture/streaming/#custom-stream-events","title":"Custom Stream Events","text":"<p>You can emit custom events from tools or callbacks:</p> <pre><code>from dataclasses import dataclass\nfrom udspy.streaming import StreamEvent, emit_event\n\n@dataclass\nclass ToolProgress(StreamEvent):\n    tool_name: str\n    message: str\n    progress: float  # 0.0 to 1.0\n\n# In your tool:\nfrom udspy import tool\nfrom pydantic import Field\n\n@tool(name=\"search\")\nasync def search(query: str = Field(...)) -&gt; str:\n    emit_event(ToolProgress(\"search\", \"Starting search...\", 0.0))\n\n    results = await search_api(query)\n\n    emit_event(ToolProgress(\"search\", \"Processing results...\", 0.5))\n\n    processed = process_results(results)\n\n    emit_event(ToolProgress(\"search\", \"Complete!\", 1.0))\n\n    return processed\n\n# In the stream consumer:\nasync for event in predictor.astream(question=\"...\"):\n    if isinstance(event, ToolProgress):\n        print(f\"[{event.tool_name}] {event.message} ({event.progress*100:.0f}%)\")\n    elif isinstance(event, OutputStreamChunk):\n        print(event.delta, end=\"\", flush=True)\n</code></pre>"},{"location":"architecture/streaming/#module-support","title":"Module Support","text":"<p>All built-in modules support streaming:</p>"},{"location":"architecture/streaming/#predict","title":"Predict","text":"<pre><code>predictor = Predict(\"question -&gt; answer\")\nasync for event in predictor.astream(question=\"...\"):\n    ...\n</code></pre>"},{"location":"architecture/streaming/#chainofthought","title":"ChainOfThought","text":"<p>Streams both reasoning and answer:</p> <pre><code>cot = ChainOfThought(\"question -&gt; answer\")\nasync for event in cot.astream(question=\"...\"):\n    if isinstance(event, OutputStreamChunk):\n        if event.field_name == \"reasoning\":\n            # Reasoning streams first\n            ...\n        elif event.field_name == \"answer\":\n            # Answer streams second\n            ...\n</code></pre>"},{"location":"architecture/streaming/#react","title":"ReAct","text":"<p>Streams reasoning and tool interactions:</p> <pre><code>from udspy import ReAct\n\nagent = ReAct(\"question -&gt; answer\", tools=[search])\nasync for event in agent.astream(question=\"...\"):\n    if isinstance(event, OutputStreamChunk):\n        if event.field_name == \"reasoning\":\n            print(f\"\ud83d\udcad {event.delta}\", end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        print(f\"\\n\\n\u2713 {event.answer}\")\n</code></pre> <p>See <code>examples/react_streaming.py</code> for a complete example.</p>"},{"location":"architecture/streaming/#nested-modules","title":"Nested Modules","text":"<p>When modules compose other modules (e.g., ReAct using ChainOfThought), predictions from both parent and child modules are streamed. Use the <code>module</code> and <code>is_final</code> attributes to distinguish them:</p> <pre><code>from udspy import ReAct, ChainOfThought\n\nagent = ReAct(\"question -&gt; answer\", tools=[calculator])\n\nasync for event in agent.astream(question=\"What is 157 * 234?\"):\n    if isinstance(event, Prediction):\n        module_name = event.module.__class__.__name__ if event.module else \"Unknown\"\n\n        if event.is_final:\n            # This is the final result from the top-level ReAct module\n            print(f\"Final answer from {module_name}: {event.answer}\")\n        else:\n            # This is an intermediate prediction from a nested module\n            # (e.g., ChainOfThought extraction step)\n            print(f\"Intermediate result from {module_name}\")\n</code></pre> <p>See <code>examples/stream_with_module_info.py</code> for a complete example.</p>"},{"location":"architecture/streaming/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/streaming/#context-variables","title":"Context Variables","text":"<p>Streaming uses Python's <code>contextvars</code> for thread-safe event queuing:</p> <pre><code>from udspy.streaming import _stream_queue, emit_event\n\n# Internal: stream queue is set when aexecute(stream=True) is called\n# emit_event() checks if a queue exists and puts events there\n</code></pre> <p>This allows tools and nested modules to emit events without explicit queue passing.</p>"},{"location":"architecture/streaming/#event-flow","title":"Event Flow","text":"<ol> <li>Module's <code>astream(**inputs)</code> is called</li> <li>Queue is created and set in context</li> <li>Internal <code>aexecute(stream=True)</code> is called</li> <li>Module yields <code>StreamChunk</code> events as text arrives</li> <li>Tools can call <code>emit_event()</code> to inject custom events</li> <li>Module yields final <code>Prediction</code> when complete</li> <li>Queue is cleaned up</li> </ol>"},{"location":"architecture/streaming/#non-streaming-mode","title":"Non-Streaming Mode","text":"<p>For non-streaming execution, use <code>aforward()</code> instead of <code>astream()</code>:</p> <pre><code># Streaming: iterate over events\nasync for event in predictor.astream(question=\"...\"):\n    if isinstance(event, Prediction):\n        result = event\n\n# Non-streaming: get final result directly\nresult = await predictor.aforward(question=\"...\")\n</code></pre>"},{"location":"architecture/streaming/#best-practices","title":"Best Practices","text":""},{"location":"architecture/streaming/#1-always-handle-both-event-types","title":"1. Always Handle Both Event Types","text":"<pre><code>async for event in module.astream(**inputs):\n    match event:\n        case OutputStreamChunk():\n            # Handle streaming text\n            print(event.delta, end=\"\", flush=True)\n        case Prediction():\n            # Handle final result\n            final_result = event\n</code></pre>"},{"location":"architecture/streaming/#2-check-field-names-for-multi-field-outputs","title":"2. Check Field Names for Multi-field Outputs","text":"<pre><code>async for event in module.astream(**inputs):\n    if isinstance(event, OutputStreamChunk):\n        if event.field_name == \"reasoning\":\n            # Different formatting for reasoning\n            print(f\"\ud83d\udcad {event.delta}\", end=\"\")\n        elif event.field_name == \"answer\":\n            # Different formatting for answer\n            print(f\"\u2713 {event.delta}\", end=\"\")\n</code></pre>"},{"location":"architecture/streaming/#3-use-custom-events-for-progress","title":"3. Use Custom Events for Progress","text":"<pre><code>@dataclass\nclass Progress(StreamEvent):\n    step: str\n    percent: float\n\nasync def long_running_tool():\n    emit_event(Progress(\"Loading data\", 0.3))\n    data = load_data()\n\n    emit_event(Progress(\"Processing\", 0.6))\n    result = process(data)\n\n    emit_event(Progress(\"Complete\", 1.0))\n    return result\n</code></pre>"},{"location":"architecture/streaming/#4-accumulate-chunks-for-display","title":"4. Accumulate Chunks for Display","text":"<pre><code>accumulated = {}\n\nasync for event in module.astream(**inputs):\n    if isinstance(event, OutputStreamChunk):\n        field = event.field_name\n        if field not in accumulated:\n            accumulated[field] = \"\"\n        accumulated[field] += event.delta\n\n        # Update UI with accumulated[field]\n        update_display(field, accumulated[field])\n</code></pre>"},{"location":"architecture/streaming/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/streaming/#latency","title":"Latency","text":"<p>Streaming reduces perceived latency by showing results immediately:</p> <ul> <li>Non-streaming: Wait for full response (~5s), then show all text</li> <li>Streaming: Start showing text after ~500ms, continue as it arrives</li> </ul>"},{"location":"architecture/streaming/#token-usage","title":"Token Usage","text":"<p>Streaming doesn't affect token usage - same number of tokens are generated.</p>"},{"location":"architecture/streaming/#error-handling","title":"Error Handling","text":"<p>Errors can occur mid-stream:</p> <pre><code>try:\n    async for event in module.astream(**inputs):\n        if isinstance(event, OutputStreamChunk):\n            print(event.delta, end=\"\", flush=True)\nexcept Exception as e:\n    print(f\"\\n\\nError during streaming: {e}\")\n</code></pre>"},{"location":"architecture/streaming/#see-also","title":"See Also","text":"<ul> <li>API: Streaming - Full API reference</li> <li>Examples: Streaming - Complete examples</li> <li>Examples: ReAct Streaming - Agent streaming</li> <li>Base Module - Module execution patterns</li> </ul>"},{"location":"architecture/modules/base/","title":"Base Module","text":"<p>The <code>Module</code> class is the foundation for all udspy modules. It provides a standard interface for composable LLM components.</p>"},{"location":"architecture/modules/base/#purpose","title":"Purpose","text":"<p>The base module serves several key purposes:</p> <ol> <li>Unified Interface: All modules implement the same execution methods (<code>aexecute</code>, <code>aforward</code>, <code>__call__</code>)</li> <li>Composition: Modules can be nested and composed to build complex behaviors</li> <li>Streaming Support: Built-in streaming infrastructure for real-time outputs</li> <li>Async-First: Native async/await support for efficient I/O operations</li> </ol>"},{"location":"architecture/modules/base/#core-methods","title":"Core Methods","text":""},{"location":"architecture/modules/base/#init_moduletoolsnone","title":"<code>init_module(tools=None)</code>","text":"<p>Initialize or reinitialize the module with new tools. This is the key method for dynamic tool management.</p> <p>Why it's needed: When tools are added or removed at runtime, the module needs to reconfigure its internal state. Specifically:</p> <ol> <li>Tool schemas must be regenerated - The LLM needs updated JSON schemas for the new/different tools</li> <li>Signatures must be rebuilt - Tool descriptions need to be incorporated into the prompt</li> <li>Tool dictionary must be updated - The module needs to know which tools are available for execution</li> </ol> <p>Without <code>init_module()</code>, adding a tool dynamically would be incomplete - the module would have the tool function but wouldn't know how to describe it to the LLM or include it in requests.</p> <p>Purpose: Allows modules to rebuild their complete state (tools, schemas, signatures) during execution. This enables dynamic tool loading where tools can modify the available toolset.</p> <p>When to use: - Called from module callbacks (decorated with <code>@module_callback</code>) - When you need to add/remove tools during execution - When building adaptive agents that discover needed tools progressively</p> <p>Implementation requirements: 1. Rebuild the tools dictionary 2. Regenerate tool schemas (if applicable) 3. Rebuild signatures with new tool descriptions (if applicable) 4. Preserve built-in tools (like ReAct's <code>finish</code> and user clarification)</p> <pre><code>from udspy import module_callback\n\n@module_callback\ndef add_tools(context):\n    # Get current tools (excluding built-ins)\n    current = [\n        t for t in context.module.tools.values()\n        if t.name not in (\"finish\", \"user_clarification\")\n    ]\n\n    # Add new tools\n    new_tools = [calculator, weather_api]\n\n    # Reinitialize module\n    context.module.init_module(tools=current + new_tools)\n\n    return \"Added calculator and weather tools\"\n</code></pre> <p>See also: Dynamic Tool Management for detailed examples.</p>"},{"location":"architecture/modules/base/#aexecute-stream-bool-false-inputs","title":"<code>aexecute(*, stream: bool = False, **inputs)</code>","text":"<p>The core execution method that all modules must implement. This is the public API for module execution.</p> <ul> <li>stream: If <code>True</code>, enables streaming mode for real-time output</li> <li>inputs: Keyword arguments matching the module's signature input fields</li> <li>Returns: <code>AsyncGenerator[StreamEvent, None]</code> that yields events and ends with a <code>Prediction</code></li> </ul> <pre><code>class CustomModule(Module):\n    async def aexecute(self, *, stream: bool = False, **inputs):\n        # Implementation here\n        ...\n        yield Prediction(result=final_result)\n</code></pre>"},{"location":"architecture/modules/base/#aforwardinputs","title":"<code>aforward(**inputs)</code>","text":"<p>Convenience method that calls <code>aexecute(stream=False)</code> and returns just the final <code>Prediction</code>.</p> <pre><code>result = await module.aforward(question=\"What is Python?\")\nprint(result.answer)\n</code></pre>"},{"location":"architecture/modules/base/#__call__inputs","title":"<code>__call__(**inputs)</code>","text":"<p>Synchronous wrapper that runs <code>aforward</code> and returns the result. This is the most convenient way to use modules in synchronous code.</p> <pre><code>result = module(question=\"What is Python?\")\nprint(result.answer)\n</code></pre>"},{"location":"architecture/modules/base/#streaming-architecture","title":"Streaming Architecture","text":"<p>Modules support streaming through an async generator pattern:</p> <pre><code>async for event in module.aexecute(stream=True, question=\"Explain AI\"):\n    if isinstance(event, OutputStreamChunk):\n        print(event.delta, end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        print(f\"\\nFinal: {event.answer}\")\n</code></pre> <p>The streaming system yields: - <code>StreamChunk</code> events during generation (with <code>field</code> and <code>delta</code>) - A final <code>Prediction</code> object with complete results</p>"},{"location":"architecture/modules/base/#module-composition","title":"Module Composition","text":"<p>Modules can contain other modules, creating powerful compositions:</p> <pre><code>from udspy import Module, Predict, ChainOfThought, Prediction\n\nclass Pipeline(Module):\n    def __init__(self):\n        self.analyzer = Predict(\"text -&gt; analysis\")\n        self.summarizer = ChainOfThought(\"text, analysis -&gt; summary\")\n\n    async def aexecute(self, *, stream: bool = False, **inputs):\n        # First module: get analysis (stream=False since we need full result)\n        analysis = None\n        async for event in self.analyzer.aexecute(stream=False, text=inputs[\"text\"]):\n            if isinstance(event, Prediction):\n                analysis = event\n\n        if not analysis:\n            raise ValueError(\"First module did not produce a result\")\n\n        # Second module: pass down stream parameter\n        async for event in self.summarizer.aexecute(\n            stream=stream,\n            text=inputs[\"text\"],\n            analysis=analysis.analysis\n        ):\n            # Yield all events from second module\n            yield event\n</code></pre>"},{"location":"architecture/modules/base/#design-rationale","title":"Design Rationale","text":""},{"location":"architecture/modules/base/#why-aexecute-instead-of-_aexecute","title":"Why <code>aexecute</code> instead of <code>_aexecute</code>?","text":"<p>The method is named <code>aexecute</code> (public) rather than <code>_aexecute</code> (private) because:</p> <ol> <li>It's the public API: Modules are meant to be executed via this method</li> <li>Subclasses override it: Marking it private would be confusing since it's meant to be overridden</li> <li>Consistency: Follows Python conventions where overridable methods are public</li> </ol> <p>See ADR-006 for detailed rationale.</p>"},{"location":"architecture/modules/base/#async-first-design","title":"Async-First Design","text":"<p>All modules are async-first because:</p> <ol> <li>I/O Bound: LLM calls are network I/O operations</li> <li>Concurrent Operations: Multiple LLM calls can run in parallel</li> <li>Streaming: Async generators are ideal for streaming responses</li> <li>Modern Python: Async/await is the standard for I/O-bound operations</li> </ol> <p>The synchronous <code>__call__</code> wrapper provides convenience but internally uses async operations.</p>"},{"location":"architecture/modules/base/#built-in-modules","title":"Built-in Modules","text":"<p>udspy provides several built-in modules:</p> <ul> <li>Predict: Core module for LLM predictions</li> <li>ChainOfThought: Adds reasoning before outputs</li> <li>ReAct: Reasoning and acting with tools</li> </ul>"},{"location":"architecture/modules/base/#creating-custom-modules","title":"Creating Custom Modules","text":"<p>To create a custom module:</p> <ol> <li>Subclass <code>Module</code></li> <li>Implement <code>aexecute()</code> method</li> <li>Yield <code>StreamEvent</code> objects during execution</li> <li>Yield final <code>Prediction</code> at the end</li> </ol> <pre><code>from udspy import Module, Prediction\n\nclass CustomModule(Module):\n    async def aexecute(self, *, stream: bool = False, **inputs):\n        # Your logic here\n        result = process_inputs(inputs)\n\n        # Return final prediction\n        yield Prediction(output=result)\n</code></pre>"},{"location":"architecture/modules/base/#see-also","title":"See Also","text":"<ul> <li>Predict Module - The core prediction module</li> <li>ChainOfThought Module - Step-by-step reasoning</li> <li>ReAct Module - Agent with tool usage</li> <li>ADR-006: Unified Execution Pattern</li> </ul>"},{"location":"architecture/modules/chain_of_thought/","title":"Chain of Thought","text":"<p>Chain of Thought (CoT) is a prompting technique that improves reasoning by explicitly requesting step-by-step thinking.</p>"},{"location":"architecture/modules/chain_of_thought/#overview","title":"Overview","text":"<p>The <code>ChainOfThought</code> module is a wrapper around <code>Predict</code> that automatically adds a \"reasoning\" field to any signature:</p> <pre><code>class QA(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\n# ChainOfThought extends the signature:\n# question -&gt; reasoning, answer\ncot = ChainOfThought(QA)\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#how-it-works","title":"How It Works","text":""},{"location":"architecture/modules/chain_of_thought/#1-signature-extension","title":"1. Signature Extension","text":"<p>ChainOfThought takes the original signature and creates an extended version with a reasoning field:</p> <pre><code># Original signature\nquestion: str -&gt; answer: str\n\n# Extended signature (automatically)\nquestion: str -&gt; reasoning: str, answer: str\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#2-implementation","title":"2. Implementation","text":"<pre><code>class ChainOfThought(Module):\n    def __init__(self, signature, **kwargs):\n        # Extract input and output fields\n        input_fields = signature.get_input_fields()\n        output_fields = signature.get_output_fields()\n\n        # Prepend reasoning to outputs\n        extended_outputs = {\"reasoning\": str, **output_fields}\n\n        # Create new signature\n        extended_signature = make_signature(\n            input_fields,\n            extended_outputs,\n            signature.get_instructions()\n        )\n\n        # Use Predict with extended signature\n        self.predict = Predict(extended_signature, **kwargs)\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#3-prompt-engineering","title":"3. Prompt Engineering","text":"<p>The reasoning field encourages step-by-step thinking through:</p> <ol> <li>Field description: \"Step-by-step reasoning process\" (customizable)</li> <li>Field ordering: Reasoning comes before the answer</li> <li>Output format: Uses field markers to structure the response</li> </ol> <p>Example prompt structure: <pre><code>[System]\nAnswer questions with clear reasoning.\n\nRequired Outputs:\n- reasoning: Step-by-step reasoning process\n- answer: Final answer\n\n[User]\n[[ ## question ## ]]\nWhat is 15 * 23?\n\n[Assistant]\n[[ ## reasoning ## ]]\nLet me calculate: 15 * 23 = 15 * 20 + 15 * 3 = 300 + 45 = 345\n\n[[ ## answer ## ]]\n345\n</code></pre></p>"},{"location":"architecture/modules/chain_of_thought/#benefits","title":"Benefits","text":""},{"location":"architecture/modules/chain_of_thought/#improved-accuracy","title":"Improved Accuracy","text":"<p>Chain of Thought improves accuracy on:</p> <ul> <li>Math problems: 67% \u2192 92% accuracy (typical improvement)</li> <li>Logic puzzles: Forces explicit reasoning steps</li> <li>Multi-step tasks: Prevents skipping intermediate steps</li> <li>Complex analysis: Organizes thinking</li> </ul>"},{"location":"architecture/modules/chain_of_thought/#transparency","title":"Transparency","text":"<p>Shows the reasoning process:</p> <pre><code>result = cot(question=\"Is 17 prime?\")\n\nprint(result.reasoning)\n# \"To check if 17 is prime, I need to test divisibility\n#  by all primes up to \u221a17 \u2248 4.12.\n#  Testing: 17 \u00f7 2 = 8.5 (not divisible)\n#          17 \u00f7 3 = 5.67 (not divisible)\n#  No divisors found, so 17 is prime.\"\n\nprint(result.answer)\n# \"Yes, 17 is prime\"\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#debugging","title":"Debugging","text":"<p>Easier to identify issues:</p> <pre><code>result = cot(question=\"What is 2^10?\")\n\nif \"1024\" not in result.answer:\n    # Check reasoning to see where it went wrong\n    print(\"Error in reasoning:\", result.reasoning)\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#customization","title":"Customization","text":""},{"location":"architecture/modules/chain_of_thought/#custom-reasoning-description","title":"Custom Reasoning Description","text":"<pre><code>cot = ChainOfThought(\n    signature,\n    reasoning_description=\"Detailed mathematical proof with all steps\"\n)\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#model-parameters","title":"Model Parameters","text":"<pre><code># Deterministic reasoning for math\ncot = ChainOfThought(QA, temperature=0.0)\n\n# Creative reasoning for analysis\ncot = ChainOfThought(Analysis, temperature=0.7)\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#multiple-outputs","title":"Multiple Outputs","text":"<p>Works seamlessly with multiple output fields:</p> <pre><code>class ComplexTask(Signature):\n    \"\"\"Complex task.\"\"\"\n    input: str = InputField()\n    analysis: str = OutputField()\n    recommendation: str = OutputField()\n    confidence: float = OutputField()\n\ncot = ChainOfThought(ComplexTask)\nresult = cot(input=\"...\")\n\n# All outputs available\nresult.reasoning       # Added automatically\nresult.analysis        # Original output\nresult.recommendation  # Original output\nresult.confidence      # Original output\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#when-to-use","title":"When to Use","text":""},{"location":"architecture/modules/chain_of_thought/#good-use-cases","title":"Good Use Cases \u2713","text":"<ul> <li>Math problems requiring calculation</li> <li>Logic puzzles and reasoning tasks</li> <li>Multi-step analysis or planning</li> <li>Tasks where you want to verify reasoning</li> <li>Educational applications (show work)</li> <li>High-stakes decisions requiring justification</li> </ul>"},{"location":"architecture/modules/chain_of_thought/#less-useful","title":"Less Useful \u2717","text":"<ul> <li>Simple factual recall (\"What is the capital of France?\")</li> <li>Binary classification without reasoning</li> <li>Very short outputs where reasoning overhead is large</li> <li>Real-time systems with strict latency requirements</li> </ul>"},{"location":"architecture/modules/chain_of_thought/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/modules/chain_of_thought/#token-usage","title":"Token Usage","text":"<p>Chain of Thought uses more tokens:</p> <pre><code># Without CoT: ~50 tokens\nresult = predict(question=\"What is 2+2?\")\n# answer: \"4\"\n\n# With CoT: ~150 tokens\nresult = cot(question=\"What is 2+2?\")\n# reasoning: \"This is basic arithmetic. 2+2 = 4\"\n# answer: \"4\"\n</code></pre> <p>Trade-off: Higher cost/latency for better accuracy and transparency.</p>"},{"location":"architecture/modules/chain_of_thought/#optimization","title":"Optimization","text":"<p>For cost-sensitive applications:</p> <pre><code># Use CoT only for complex queries\nif is_complex(question):\n    result = cot(question=question)\nelse:\n    result = simple_predict(question=question)\n</code></pre>"},{"location":"architecture/modules/chain_of_thought/#best-practices","title":"Best Practices","text":"<ol> <li>Use appropriate temperature</li> <li><code>0.0</code> for math/logic (deterministic)</li> <li> <p><code>0.3-0.7</code> for analysis/planning</p> </li> <li> <p>Customize reasoning description for domain-specific tasks    <pre><code>ChainOfThought(\n    MedicalDiagnosis,\n    reasoning_description=\"Clinical reasoning with differential diagnosis\"\n)\n</code></pre></p> </li> <li> <p>Validate reasoning quality in production    <pre><code>if len(result.reasoning) &lt; 100:\n    logger.warning(\"Reasoning too brief\")\n</code></pre></p> </li> <li> <p>Cache for repeated queries to save costs    <pre><code>@lru_cache(maxsize=100)\ndef cached_cot(question):\n    return cot(question=question)\n</code></pre></p> </li> </ol> <p>See Examples for more details.</p>"},{"location":"architecture/modules/predict/","title":"Predict Module","text":"<p>The <code>Predict</code> module is the foundational building block of udspy. It takes a signature and generates outputs from inputs using an LLM.</p>"},{"location":"architecture/modules/predict/#overview","title":"Overview","text":"<p><code>Predict</code> is the simplest and most essential module in udspy. It:</p> <ul> <li>Maps signature inputs to outputs via an LLM</li> <li>Supports native tool calling for function execution</li> <li>Handles streaming for real-time output generation</li> <li>Manages conversation history and message formatting</li> </ul>"},{"location":"architecture/modules/predict/#basic-usage","title":"Basic Usage","text":"<pre><code>from udspy import Predict, Signature, InputField, OutputField\n\nclass QA(Signature):\n    \"\"\"Answer questions concisely.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\nresult = predictor(question=\"What is Python?\")\nprint(result.answer)\n</code></pre>"},{"location":"architecture/modules/predict/#string-signatures","title":"String Signatures","text":"<p>For quick prototyping, use string signatures:</p> <pre><code>predictor = Predict(\"question -&gt; answer\")\nresult = predictor(question=\"What is Python?\")\n</code></pre> <p>See Signatures for more details.</p>"},{"location":"architecture/modules/predict/#configuration","title":"Configuration","text":""},{"location":"architecture/modules/predict/#model-selection","title":"Model Selection","text":"<pre><code># Global default\nimport udspy\nfrom udspy import LM\n\nlm = LM(model=\"gpt-4o-mini\", api_key=\"sk-...\")\nudspy.settings.configure(lm=lm)\n\n# Per-module override\npredictor = Predict(QA, model=\"gpt-4o\")\n</code></pre>"},{"location":"architecture/modules/predict/#temperature-and-sampling","title":"Temperature and Sampling","text":"<pre><code>predictor = Predict(\n    QA,\n    temperature=0.7,\n    max_tokens=1000,\n    top_p=0.9,\n)\n</code></pre>"},{"location":"architecture/modules/predict/#custom-adapter","title":"Custom Adapter","text":"<pre><code>from udspy import ChatAdapter\n\nadapter = ChatAdapter()\npredictor = Predict(QA, adapter=adapter)\n</code></pre>"},{"location":"architecture/modules/predict/#tool-calling","title":"Tool Calling","text":"<p>Predict supports native OpenAI tool calling:</p> <pre><code>from udspy import tool\nfrom pydantic import Field\n\n@tool(name=\"search\", description=\"Search for information\")\ndef search(query: str = Field(...)) -&gt; str:\n    return f\"Results for: {query}\"\n\npredictor = Predict(QA, tools=[search])\nresult = predictor(question=\"What is the weather in Tokyo?\")\n\n# Access tool calls\nfor call in result.tool_calls:\n    print(f\"Called {call['name']} with {call['arguments']}\")\n</code></pre>"},{"location":"architecture/modules/predict/#auto-execution-vs-manual","title":"Auto-execution vs Manual","text":"<p>By default, tools are NOT auto-executed. You control execution:</p> <pre><code># Auto-execute tools\nresult = await predictor.aexecute(\n    question=\"Search for Python\",\n    auto_execute_tools=True\n)\n\n# Manual execution\nresult = await predictor.aexecute(\n    question=\"Search for Python\",\n    auto_execute_tools=False\n)\n# Tools are returned in result.tool_calls but not executed\n</code></pre> <p>See Tool Calling for more details.</p>"},{"location":"architecture/modules/predict/#streaming","title":"Streaming","text":"<p>Stream outputs in real-time:</p> <pre><code>async for event in predictor.aexecute(\n    stream=True,\n    question=\"Explain quantum computing\"\n):\n    if isinstance(event, OutputStreamChunk):\n        print(event.delta, end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        print(f\"\\n\\nFinal: {event.answer}\")\n</code></pre>"},{"location":"architecture/modules/predict/#stream-events","title":"Stream Events","text":"<p>The streaming API yields: - <code>StreamChunk(field: str, delta: str)</code> - Incremental text for a field - <code>Prediction(**outputs)</code> - Final result with all fields</p> <pre><code>from udspy import OutputStreamChunk, Prediction\n\nasync for event in predictor.aexecute(stream=True, **inputs):\n    match event:\n        case OutputStreamChunk(field=field, delta=delta):\n            print(f\"[{field}] {delta}\", end=\"\")\n        case Prediction() as pred:\n            print(f\"\\n\\nComplete: {pred}\")\n</code></pre>"},{"location":"architecture/modules/predict/#execution-methods","title":"Execution Methods","text":""},{"location":"architecture/modules/predict/#async-execution","title":"Async Execution","text":"<pre><code># With streaming\nasync for event in predictor.aexecute(stream=True, question=\"...\"):\n    ...\n\n# Without streaming\nresult = await predictor.aforward(question=\"...\")\n</code></pre>"},{"location":"architecture/modules/predict/#synchronous-execution","title":"Synchronous Execution","text":"<pre><code># Synchronous wrapper\nresult = predictor(question=\"...\")\n</code></pre> <p>Internally, the synchronous call runs async code using <code>asyncio.run()</code> or the current event loop.</p>"},{"location":"architecture/modules/predict/#history-management","title":"History Management","text":"<p>Predict automatically manages conversation history:</p> <pre><code>predictor = Predict(QA)\n\n# First call\nresult1 = predictor(question=\"What is Python?\")\n\n# Second call - includes history\nresult2 = predictor(question=\"What are its key features?\")\n\n# Access history\nfor msg in predictor.history:\n    print(f\"{msg.role}: {msg.content}\")\n\n# Clear history\npredictor.history.clear()\n</code></pre> <p>See History API for more details.</p>"},{"location":"architecture/modules/predict/#under-the-hood","title":"Under the Hood","text":""},{"location":"architecture/modules/predict/#message-flow","title":"Message Flow","text":"<ol> <li>Signature \u2192 Messages: Adapter converts signature to system prompt</li> <li>Inputs \u2192 User Message: Input fields become user message</li> <li>LLM Call: OpenAI API generates response</li> <li>Response \u2192 Outputs: Parse response into output fields</li> <li>History Update: Add messages to history</li> </ol>"},{"location":"architecture/modules/predict/#field-parsing","title":"Field Parsing","text":"<p>Outputs are parsed from the LLM response:</p> <ul> <li>JSON Mode: If response is valid JSON, parse it</li> <li>Field Markers: Look for field boundaries like <code>answer: ...</code></li> <li>Fallback: Extract text content</li> </ul> <p>The <code>Prediction</code> object makes all output fields accessible as attributes:</p> <pre><code>result = predictor(question=\"...\")\nprint(result.answer)  # Access via attribute\nprint(result[\"answer\"])  # Access via dict key\n</code></pre>"},{"location":"architecture/modules/predict/#error-handling-and-retry","title":"Error Handling and Retry","text":""},{"location":"architecture/modules/predict/#automatic-retry-on-parse-errors","title":"Automatic Retry on Parse Errors","text":"<p><code>Predict</code> automatically retries when LLM responses fail to parse correctly. This handles common issues like: - Missing or malformed field markers - Invalid JSON in structured outputs - Format inconsistencies</p> <p>Retry behavior: - Max attempts: 3 (1 initial + 2 retries) - Backoff strategy: Exponential backoff (0.1s, 0.2s, up to 3s) - Only retries: <code>AdapterParseError</code> (not network errors or other exceptions) - Applies to: Both streaming and non-streaming execution</p> <pre><code># This will automatically retry if parse fails\ntry:\n    result = predictor(question=\"...\")\nexcept tenacity.RetryError as e:\n    # All 3 attempts failed\n    print(f\"Failed after retries: {e}\")\n</code></pre> <p>Why automatic retry? - LLM format errors are usually transient and succeed on retry - Reduces boilerplate error handling code - Improves reliability without user intervention</p> <p>See ADR-007: Automatic Retry on Parse Errors for full details.</p>"},{"location":"architecture/modules/predict/#error-types","title":"Error Types","text":"<p>AdapterParseError: LLM response doesn't match expected format - Automatically retried up to 3 times - Check signature output fields match what LLM is generating</p> <p>ValidationError: Output doesn't match Pydantic field types - Not retried (indicates signature mismatch) - Adjust signature types or field descriptions</p> <p>OpenAI API errors: Network issues, rate limits, etc. - Not retried automatically (use your own retry logic for these) - Consider using exponential backoff for rate limits</p>"},{"location":"architecture/modules/predict/#design-rationale","title":"Design Rationale","text":""},{"location":"architecture/modules/predict/#why-not-auto-execute-tools-by-default","title":"Why Not Auto-execute Tools by Default?","text":"<p>Tools are not auto-executed by default because:</p> <ol> <li>Safety: User should control when external functions run</li> <li>Flexibility: Sometimes you want to inspect/modify tool calls</li> <li>Explicit is better: Makes behavior clear and predictable</li> </ol>"},{"location":"architecture/modules/predict/#why-native-tool-calling","title":"Why Native Tool Calling?","text":"<p>Unlike DSPy which uses custom adapters, udspy uses OpenAI's native function calling:</p> <ol> <li>Simplicity: Less code to maintain</li> <li>Performance: Optimized by OpenAI</li> <li>Reliability: Well-tested and production-ready</li> <li>Features: Access to latest tool calling improvements</li> </ol> <p>See ADR-001: Native Tool Calling for full rationale.</p>"},{"location":"architecture/modules/predict/#common-patterns","title":"Common Patterns","text":""},{"location":"architecture/modules/predict/#simple-qa","title":"Simple Q&amp;A","text":"<pre><code>qa = Predict(\"question -&gt; answer\")\nresult = qa(question=\"What is AI?\")\n</code></pre>"},{"location":"architecture/modules/predict/#multi-field-output","title":"Multi-field Output","text":"<pre><code>analyzer = Predict(\"text -&gt; summary, sentiment, keywords\")\nresult = analyzer(text=\"I love this product!\")\n</code></pre>"},{"location":"architecture/modules/predict/#with-context","title":"With Context","text":"<pre><code>contextual = Predict(\"context, question -&gt; answer\")\nresult = contextual(\n    context=\"Python is a programming language\",\n    question=\"What is it used for?\"\n)\n</code></pre>"},{"location":"architecture/modules/predict/#with-tools","title":"With Tools","text":"<pre><code>@tool(name=\"calculator\")\ndef calc(expression: str = Field(...)) -&gt; str:\n    return str(eval(expression))\n\nmath_solver = Predict(\"problem -&gt; solution\", tools=[calc])\nresult = await math_solver.aexecute(\n    problem=\"What is 157 * 234?\",\n    auto_execute_tools=True\n)\n</code></pre>"},{"location":"architecture/modules/predict/#see-also","title":"See Also","text":"<ul> <li>Base Module - Module foundation</li> <li>ChainOfThought - Add reasoning</li> <li>ReAct - Agent with tool usage</li> <li>Signatures - Define inputs/outputs</li> <li>Tool Calling - Function calling API</li> <li>Streaming - Real-time output</li> </ul>"},{"location":"architecture/modules/react/","title":"ReAct Module","text":"<p>The <code>ReAct</code> (Reasoning and Acting) module implements an agent that iteratively reasons about tasks and uses tools to accomplish goals.</p>"},{"location":"architecture/modules/react/#overview","title":"Overview","text":"<p>ReAct combines:</p> <ul> <li>Reasoning: Step-by-step thinking about what to do next</li> <li>Acting: Calling tools to perform actions</li> <li>Iteration: Repeating until the task is complete</li> </ul> <p>This creates an agent that can break down complex tasks, use available tools, and ask for help when needed.</p>"},{"location":"architecture/modules/react/#basic-usage","title":"Basic Usage","text":"<pre><code>from udspy import ReAct, Signature, InputField, OutputField, tool\nfrom pydantic import Field\n\n@tool(name=\"search\", description=\"Search for information\")\ndef search(query: str = Field(...)) -&gt; str:\n    return f\"Results for: {query}\"\n\nclass QA(Signature):\n    \"\"\"Answer questions using available tools.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\nagent = ReAct(QA, tools=[search])\nresult = agent(question=\"What is the weather in Tokyo?\")\n\nprint(result.answer)\nprint(result.trajectory)  # Full reasoning history\n</code></pre>"},{"location":"architecture/modules/react/#string-signatures","title":"String Signatures","text":"<p>For quick prototyping:</p> <pre><code>agent = ReAct(\"question -&gt; answer\", tools=[search])\nresult = agent(question=\"What is Python?\")\n</code></pre>"},{"location":"architecture/modules/react/#how-react-works","title":"How ReAct Works","text":""},{"location":"architecture/modules/react/#iteration-loop","title":"Iteration Loop","text":"<ol> <li>Reason: Agent thinks about current situation</li> <li>Act: Agent calls a tool (or finish)</li> <li>Observe: Agent sees tool result</li> <li>Repeat: Until agent calls <code>finish</code> tool or max iterations</li> </ol>"},{"location":"architecture/modules/react/#built-in-tools","title":"Built-in Tools","text":"<p>ReAct automatically provides:</p> <ul> <li>finish: Call when task is complete</li> <li>user_clarification: Ask user for clarification (if enabled)</li> </ul> <pre><code># The agent automatically has these tools available:\n# - finish(answer: str) - Complete the task\n# - user_clarification(question: str) - Ask user for help\n</code></pre>"},{"location":"architecture/modules/react/#trajectory","title":"Trajectory","text":"<p>The trajectory records every step:</p> <pre><code>result = agent(question=\"Calculate 15 * 23\")\n\n# Access trajectory\nprint(result.trajectory)\n# {\n#   \"reasoning_0\": \"I need to calculate 15 * 23\",\n#   \"tool_name_0\": \"calculator\",\n#   \"tool_args_0\": {\"expression\": \"15 * 23\"},\n#   \"observation_0\": \"345\",\n#   \"reasoning_1\": \"I have the answer\",\n#   \"tool_name_1\": \"finish\",\n#   ...\n# }\n</code></pre>"},{"location":"architecture/modules/react/#configuration","title":"Configuration","text":""},{"location":"architecture/modules/react/#maximum-iterations","title":"Maximum Iterations","text":"<pre><code>agent = ReAct(QA, tools=[search], max_iters=10)\nresult = agent(question=\"...\", max_iters=5)  # Override per call\n</code></pre>"},{"location":"architecture/modules/react/#disable-ask-to-user","title":"Disable Ask-to-User","text":"<pre><code>agent = ReAct(QA, tools=[search])\n</code></pre>"},{"location":"architecture/modules/react/#human-in-the-loop","title":"Human-in-the-Loop","text":"<p>ReAct supports tools with require_confirmation that require human confirmation:</p> <pre><code>from udspy import ConfirmationRequired, tool\n\n@tool(name=\"delete_file\", require_confirmation=True)\ndef delete_file(path: str = Field(...)) -&gt; str:\n    return f\"Deleted {path}\"\n\nagent = ReAct(QA, tools=[delete_file])\n\n# Note: aresume() is not yet implemented in ReAct\n# Use respond_to_confirmation() instead\nfrom udspy import respond_to_confirmation\n\ntry:\n    result = await agent.aforward(question=\"Delete /tmp/test.txt\")\nexcept ConfirmationRequired as e:\n    print(f\"Confirm: {e.question}\")\n    print(f\"Tool: {e.tool_call.name}\")\n    print(f\"Args: {e.tool_call.args}\")\n\n    # User approves\n    respond_to_confirmation(e.confirmation_id, approved=True)\n    result = await agent.aforward(question=\"Delete /tmp/test.txt\")\n\n    # Or user rejects\n    respond_to_confirmation(e.confirmation_id, approved=False, status=\"rejected\")\n</code></pre>"},{"location":"architecture/modules/react/#resumption-flow","title":"Resumption Flow","text":"<p>When a confirmation is requested:</p> <ol> <li>Agent pauses and raises <code>ConfirmationRequired</code></li> <li>Exception contains saved state and pending tool call</li> <li>User reviews and responds</li> <li>Call <code>aresume(response, saved_state)</code> to continue</li> </ol> <p>See Confirmation API for details.</p>"},{"location":"architecture/modules/react/#streaming","title":"Streaming","text":"<p>Stream the agent's reasoning in real-time:</p> <pre><code>async for event in agent.aexecute(\n    stream=True,\n    question=\"What is quantum computing?\"\n):\n    if isinstance(event, OutputStreamChunk):\n        if event.field == \"reasoning\":\n            print(f\"Thinking: {event.delta}\", end=\"\", flush=True)\n    elif isinstance(event, Prediction):\n        print(f\"\\n\\nAnswer: {event.answer}\")\n</code></pre> <p>See <code>examples/react_streaming.py</code> for a complete example.</p>"},{"location":"architecture/modules/react/#architecture","title":"Architecture","text":""},{"location":"architecture/modules/react/#internal-signatures","title":"Internal Signatures","text":"<p>ReAct uses two internal signatures:</p> <ol> <li>react_signature: For reasoning and tool selection</li> <li>Inputs: Original inputs + trajectory</li> <li>Outputs: reasoning</li> <li> <p>Tools: All provided tools + finish</p> </li> <li> <p>extract_signature: For extracting final answer</p> </li> <li>Inputs: Original inputs + trajectory</li> <li>Outputs: Original outputs</li> <li>Uses ChainOfThought for extraction</li> </ol>"},{"location":"architecture/modules/react/#modules","title":"Modules","text":"<p>ReAct composes two modules:</p> <ul> <li><code>react_module</code>: Predict with tools for reasoning/acting</li> <li><code>extract_module</code>: ChainOfThought for final answer extraction</li> </ul>"},{"location":"architecture/modules/react/#example-flow","title":"Example Flow","text":"<pre><code>User: \"What is the capital of France?\"\n\nIteration 0:\n  Reasoning: \"I need to search for France's capital\"\n  Tool: search\n  Args: {\"query\": \"capital of France\"}\n  Observation: \"Paris is the capital of France\"\n\nIteration 1:\n  Reasoning: \"I have the answer, I can finish\"\n  Tool: finish\n  Args: {}\n  Observation: \"Task completed\"\n\nExtract:\n  Reasoning: \"Based on the search, Paris is the capital\"\n  Answer: \"Paris\"\n</code></pre>"},{"location":"architecture/modules/react/#advanced-usage","title":"Advanced Usage","text":""},{"location":"architecture/modules/react/#custom-tools","title":"Custom Tools","text":"<pre><code>from pydantic import Field\n\n@tool(\n    name=\"calculator\",\n    description=\"Evaluate mathematical expressions\"\n)\ndef calc(expression: str = Field(description=\"Math expression\")) -&gt; str:\n    return str(eval(expression))\n\n@tool(\n    name=\"web_search\",\n    description=\"Search the web for information\"\n)\nasync def web_search(query: str = Field(...)) -&gt; str:\n    # Async tools are supported\n    return await search_api(query)\n\nagent = ReAct(QA, tools=[calc, web_search])\n</code></pre>"},{"location":"architecture/modules/react/#multiple-outputs","title":"Multiple Outputs","text":"<pre><code>class Research(Signature):\n    \"\"\"Research a topic thoroughly.\"\"\"\n    topic: str = InputField()\n    summary: str = OutputField()\n    sources: str = OutputField()\n    confidence: str = OutputField()\n\nagent = ReAct(Research, tools=[search])\nresult = agent(topic=\"Quantum Computing\")\n\nprint(result.summary)\nprint(result.sources)\nprint(result.confidence)\n</code></pre>"},{"location":"architecture/modules/react/#tool-error-handling","title":"Tool Error Handling","text":"<p>Tools can raise exceptions - they're caught and added to observations:</p> <pre><code>@tool(name=\"divide\")\ndef divide(a: int = Field(...), b: int = Field(...)) -&gt; str:\n    return str(a / b)\n\nagent = ReAct(QA, tools=[divide])\nresult = agent(question=\"What is 10 divided by 0?\")\n\n# Agent sees: \"Error executing divide: division by zero\"\n# Agent can reason about the error and try alternative approaches\n</code></pre>"},{"location":"architecture/modules/react/#design-rationale","title":"Design Rationale","text":""},{"location":"architecture/modules/react/#why-two-phases-react-extract","title":"Why Two Phases (React + Extract)?","text":"<ol> <li>react_module: Focuses on tool usage and reasoning</li> <li>extract_module: Focuses on clean output formatting</li> </ol> <p>This separation ensures: - Tool-using prompts stay focused on actions - Final outputs are well-formatted - Trajectory doesn't pollute final answer</p>"},{"location":"architecture/modules/react/#why-user_clarification-tool","title":"Why user_clarification Tool?","text":"<p>The built-in user clarification tool allows agents to: - Request clarification when ambiguous - Ask for additional information - Interact naturally with users</p> <p>It's implemented as a tool with require_confirmation, so users can provide responses that the agent incorporates into its reasoning.</p>"},{"location":"architecture/modules/react/#why-finish-tool","title":"Why finish Tool?","text":"<p>The <code>finish</code> tool signals task completion: - Explicit end condition (vs implicit max iterations) - Agent decides when it has enough information - More natural than counting iterations</p>"},{"location":"architecture/modules/react/#common-patterns","title":"Common Patterns","text":""},{"location":"architecture/modules/react/#research-agent","title":"Research Agent","text":"<pre><code>@tool(name=\"search\")\ndef search(query: str = Field(...)) -&gt; str:\n    return search_web(query)\n\n@tool(name=\"summarize\")\ndef summarize(text: str = Field(...)) -&gt; str:\n    return llm_summarize(text)\n\nresearcher = ReAct(\n    \"topic -&gt; summary, sources\",\n    tools=[search, summarize]\n)\nresult = researcher(topic=\"AI Safety\")\n</code></pre>"},{"location":"architecture/modules/react/#task-automation","title":"Task Automation","text":"<pre><code>@tool(name=\"read_file\")\ndef read_file(path: str = Field(...)) -&gt; str:\n    return open(path).read()\n\n@tool(name=\"write_file\", require_confirmation=True)\ndef write_file(path: str = Field(...), content: str = Field(...)) -&gt; str:\n    with open(path, 'w') as f:\n        f.write(content)\n    return f\"Wrote to {path}\"\n\nassistant = ReAct(\n    \"task -&gt; result\",\n    tools=[read_file, write_file]\n)\n</code></pre>"},{"location":"architecture/modules/react/#multi-tool-problem-solving","title":"Multi-tool Problem Solving","text":"<pre><code>@tool(name=\"calculator\")\ndef calc(expr: str = Field(...)) -&gt; str:\n    return str(eval(expr))\n\n@tool(name=\"unit_converter\")\ndef convert(value: float = Field(...), from_unit: str = Field(...), to_unit: str = Field(...)) -&gt; str:\n    # Conversion logic\n    return f\"{result} {to_unit}\"\n\nsolver = ReAct(\n    \"problem -&gt; solution\",\n    tools=[calc, convert]\n)\nresult = solver(problem=\"Convert 100 fahrenheit to celsius and add 10\")\n</code></pre>"},{"location":"architecture/modules/react/#limitations","title":"Limitations","text":"<ol> <li>Token Usage: Each iteration adds to token count</li> <li>Latency: Multiple LLM calls increase response time</li> <li>Reliability: Agent may not always pick the right tool</li> <li>Max Iterations: Tasks may not complete within iteration limit</li> </ol>"},{"location":"architecture/modules/react/#see-also","title":"See Also","text":"<ul> <li>Base Module - Module foundation</li> <li>Predict Module - Core prediction</li> <li>Tool API - Creating tools</li> <li>Confirmation API - Human-in-the-loop</li> <li>ADR-005: ReAct Module</li> <li>ADR-004: Confirmation System</li> </ul>"},{"location":"examples/advanced/","title":"Advanced Examples","text":"<p>Advanced patterns and techniques.</p>"},{"location":"examples/advanced/#module-composition","title":"Module Composition","text":"<p>Build complex modules from simpler ones:</p> <pre><code>from udspy import Module, Predict, Prediction\n\nclass ChainOfThought(Module):\n    \"\"\"Answer questions with explicit reasoning.\"\"\"\n\n    def __init__(self, signature):\n        # Create intermediate signature for reasoning\n        self.reason = Predict(make_signature(\n            signature.get_input_fields(),\n            {\"reasoning\": str},\n            \"Think step-by-step about this problem\",\n        ))\n\n        # Final answer with reasoning context\n        self.answer = Predict(signature)\n\n    def forward(self, **inputs):\n        # Generate reasoning\n        thought = self.reason(**inputs)\n\n        # Generate answer (could inject reasoning into prompt)\n        result = self.answer(**inputs)\n        result[\"reasoning\"] = thought.reasoning\n\n        return Prediction(**result)\n</code></pre>"},{"location":"examples/advanced/#retry-logic","title":"Retry Logic","text":"<p>Implement retry with validation:</p> <pre><code>from pydantic import ValidationError\n\nclass ValidatedPredict(Module):\n    def __init__(self, signature, max_retries=3):\n        self.predictor = Predict(signature)\n        self.signature = signature\n        self.max_retries = max_retries\n\n    def forward(self, **inputs):\n        for attempt in range(self.max_retries):\n            try:\n                result = self.predictor(**inputs)\n                # Validate result matches expected schema\n                return result\n            except ValidationError as e:\n                if attempt == self.max_retries - 1:\n                    raise\n                # Could inject error message to help LLM correct\n                continue\n</code></pre>"},{"location":"examples/advanced/#prompt-caching","title":"Prompt Caching","text":"<p>Cache prompts for repeated queries:</p> <pre><code>from functools import lru_cache\n\nclass CachedPredict(Module):\n    def __init__(self, signature):\n        self.predictor = Predict(signature)\n        self._cached_predict = lru_cache(maxsize=128)(self._predict)\n\n    def _predict(self, **inputs):\n        # Convert inputs to hashable tuple\n        key = tuple(sorted(inputs.items()))\n        return self.predictor(**dict(key))\n\n    def forward(self, **inputs):\n        return self._cached_predict(**inputs)\n</code></pre>"},{"location":"examples/advanced/#custom-adapters","title":"Custom Adapters","text":"<p>Create custom formatting:</p> <pre><code>from udspy import ChatAdapter\n\nclass VerboseAdapter(ChatAdapter):\n    def format_instructions(self, signature):\n        base = super().format_instructions(signature)\n        return f\"{base}\\n\\nIMPORTANT: Be extremely detailed in your response.\"\n\n    def format_inputs(self, signature, inputs):\n        base = super().format_inputs(signature, inputs)\n        return f\"Input data:\\n{base}\\n\\nAnalyze thoroughly:\"\n\npredictor = Predict(signature, adapter=VerboseAdapter())\n</code></pre>"},{"location":"examples/advanced/#testing-helpers","title":"Testing Helpers","text":"<p>Utilities for testing:</p> <pre><code>from unittest.mock import MagicMock\nfrom openai.types.chat import ChatCompletion, ChatCompletionMessage, Choice\n\ndef mock_openai_response(content: str) -&gt; ChatCompletion:\n    \"\"\"Create a mock OpenAI response.\"\"\"\n    return ChatCompletion(\n        id=\"test\",\n        model=\"gpt-4o-mini\",\n        object=\"chat.completion\",\n        created=1234567890,\n        choices=[\n            Choice(\n                index=0,\n                message=ChatCompletionMessage(\n                    role=\"assistant\",\n                    content=content,\n                ),\n                finish_reason=\"stop\",\n            )\n        ],\n    )\n\ndef test_with_mock():\n    mock_client = MagicMock()\n    mock_client.chat.completions.create.return_value = mock_openai_response(\n        \"[[ ## answer ## ]]\\nTest answer\"\n    )\n\n    udspy.settings.configure(client=mock_client)\n\n    predictor = Predict(QA)\n    result = predictor(question=\"Test?\")\n    assert result.answer == \"Test answer\"\n</code></pre>"},{"location":"examples/basic_usage/","title":"Basic Usage","text":"<p>This guide covers the fundamentals of using udspy.</p>"},{"location":"examples/basic_usage/#setup","title":"Setup","text":"<p>First, configure with an LM instance:</p> <pre><code>import udspy\nfrom udspy import OpenAILM\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(api_key=\"sk-...\")\nlm = OpenAILM(client=client, default_model=\"gpt-4o-mini\")\nudspy.settings.configure(lm=lm)\n</code></pre> <p>Or use environment variables:</p> <pre><code>import os\nimport udspy\nfrom udspy import OpenAILM\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nlm = OpenAILM(client=client, default_model=\"gpt-4o-mini\")\nudspy.settings.configure(lm=lm)\n</code></pre> <p>Or configure from environment variables directly (set <code>UDSPY_LM_MODEL</code> and <code>UDSPY_LM_API_KEY</code>):</p> <pre><code>import udspy\n\nudspy.settings.configure()  # Reads from environment\n</code></pre>"},{"location":"examples/basic_usage/#direct-lm-usage","title":"Direct LM Usage","text":"<p>The simplest way to use udspy is to call the LM directly with a string:</p> <pre><code>from udspy import OpenAILM\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(api_key=\"sk-...\")\nlm = OpenAILM(client=client, default_model=\"gpt-4o-mini\")\n\n# Simple string prompt - returns just the text\nanswer = lm(\"What is the capital of France?\")\nprint(answer)  # \"Paris\"\n\n# Override the model\nanswer = lm(\"Explain quantum physics\", model=\"gpt-4\")\nprint(answer)\n\n# With additional parameters\nanswer = lm(\"Write a haiku about coding\", temperature=0.9, max_tokens=100)\nprint(answer)\n</code></pre> <p>This is perfect for quick queries and prototyping. For more structured outputs, use signatures and modules (see below).</p>"},{"location":"examples/basic_usage/#simple-question-answering-with-signatures","title":"Simple Question Answering with Signatures","text":"<pre><code>from udspy import Signature, InputField, OutputField, Predict\n\nclass QA(Signature):\n    \"\"\"Answer questions concisely.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\nresult = predictor(question=\"What is the capital of France?\")\nprint(result.answer)  # \"Paris\"\n</code></pre>"},{"location":"examples/basic_usage/#with-reasoning","title":"With Reasoning","text":"<pre><code>class ReasonedQA(Signature):\n    \"\"\"Answer questions with reasoning.\"\"\"\n    question: str = InputField()\n    reasoning: str = OutputField(description=\"Step-by-step reasoning\")\n    answer: str = OutputField(description=\"Final answer\")\n\npredictor = Predict(ReasonedQA)\nresult = predictor(question=\"What is 15 * 23?\")\nprint(f\"Reasoning: {result.reasoning}\")\nprint(f\"Answer: {result.answer}\")\n</code></pre>"},{"location":"examples/basic_usage/#custom-model-parameters","title":"Custom Model Parameters","text":"<pre><code>predictor = Predict(\n    signature=QA,\n    model=\"gpt-4\",\n    temperature=0.7,\n    max_tokens=100,\n)\n</code></pre>"},{"location":"examples/basic_usage/#global-defaults","title":"Global Defaults","text":"<pre><code>udspy.settings.configure(\n    api_key=\"sk-...\",\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n)\n\n# All predictors use these defaults unless overridden\npredictor = Predict(QA)\n</code></pre>"},{"location":"examples/basic_usage/#error-handling","title":"Error Handling","text":"<pre><code>from pydantic import ValidationError\n\ntry:\n    result = predictor(question=\"What is AI?\")\nexcept ValidationError as e:\n    print(f\"Output validation failed: {e}\")\nexcept Exception as e:\n    print(f\"API error: {e}\")\n</code></pre>"},{"location":"examples/basic_usage/#testing","title":"Testing","text":"<p>Mock the OpenAI client for testing:</p> <pre><code>from unittest.mock import MagicMock\nfrom openai.types.chat import ChatCompletion, ChatCompletionMessage, Choice\n\ndef test_qa():\n    # Mock response\n    mock_response = ChatCompletion(\n        id=\"test\",\n        model=\"gpt-4o-mini\",\n        object=\"chat.completion\",\n        created=1234567890,\n        choices=[\n            Choice(\n                index=0,\n                message=ChatCompletionMessage(\n                    role=\"assistant\",\n                    content=\"[[ ## answer ## ]]\\nParis\",\n                ),\n                finish_reason=\"stop\",\n            )\n        ],\n    )\n\n    # Configure mock client\n    mock_client = MagicMock()\n    mock_client.chat.completions.create.return_value = mock_response\n\n    udspy.settings.configure(client=mock_client)\n\n    # Test\n    predictor = Predict(QA)\n    result = predictor(question=\"What is the capital of France?\")\n    assert result.answer == \"Paris\"\n</code></pre>"},{"location":"examples/basic_usage/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Streaming</li> <li>Explore Tool Calling</li> <li>See Advanced Examples</li> </ul>"},{"location":"examples/chain_of_thought/","title":"Chain of Thought Examples","text":"<p>Chain of Thought (CoT) is a prompting technique that improves LLM reasoning by explicitly requesting step-by-step thinking before producing the final answer.</p>"},{"location":"examples/chain_of_thought/#overview","title":"Overview","text":"<p>The <code>ChainOfThought</code> module automatically adds a \"reasoning\" field to any signature, encouraging the LLM to show its work:</p> <pre><code>from udspy import ChainOfThought, Signature, InputField, OutputField\n\nclass QA(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\n# Automatically adds reasoning step\ncot = ChainOfThought(QA)\nresult = cot(question=\"What is 15 * 23?\")\n\nprint(result.reasoning)  # Step-by-step calculation\nprint(result.answer)     # \"345\"\n</code></pre>"},{"location":"examples/chain_of_thought/#basic-usage","title":"Basic Usage","text":""},{"location":"examples/chain_of_thought/#simple-question-answering","title":"Simple Question Answering","text":"<pre><code>import udspy\nfrom udspy import LM\n\nlm = LM(model=\"gpt-4o-mini\", api_key=\"your-key\")\nudspy.settings.configure(lm=lm)\n\nclass QA(Signature):\n    \"\"\"Answer questions clearly.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = ChainOfThought(QA)\nresult = predictor(question=\"What is the capital of France?\")\n\nprint(\"Reasoning:\", result.reasoning)\n# \"Let me recall the capital cities of European countries.\n#  France is a major European nation, and its capital is Paris.\"\n\nprint(\"Answer:\", result.answer)\n# \"Paris\"\n</code></pre>"},{"location":"examples/chain_of_thought/#math-problems","title":"Math Problems","text":"<p>Chain of Thought excels at mathematical reasoning:</p> <pre><code>result = predictor(question=\"What is 17 * 24?\")\n\nprint(\"Reasoning:\", result.reasoning)\n# \"I'll break this down: 17 * 24 = 17 * 20 + 17 * 4 = 340 + 68 = 408\"\n\nprint(\"Answer:\", result.answer)\n# \"408\"\n</code></pre>"},{"location":"examples/chain_of_thought/#advanced-usage","title":"Advanced Usage","text":""},{"location":"examples/chain_of_thought/#custom-reasoning-description","title":"Custom Reasoning Description","text":"<p>Customize how the reasoning field is described:</p> <pre><code>cot = ChainOfThought(\n    QA,\n    reasoning_description=\"Detailed mathematical proof with all intermediate steps\"\n)\n\nresult = cot(question=\"Prove that the sum of angles in a triangle is 180 degrees\")\n</code></pre>"},{"location":"examples/chain_of_thought/#multiple-output-fields","title":"Multiple Output Fields","text":"<p>Chain of Thought works with signatures that have multiple outputs:</p> <pre><code>class Analysis(Signature):\n    \"\"\"Analyze text comprehensively.\"\"\"\n    text: str = InputField()\n    summary: str = OutputField(description=\"Brief summary\")\n    sentiment: str = OutputField(description=\"Sentiment analysis\")\n    keywords: list[str] = OutputField(description=\"Key terms\")\n\nanalyzer = ChainOfThought(Analysis)\nresult = analyzer(text=\"Long article text here...\")\n\n# Access all outputs plus reasoning\nprint(result.reasoning)   # Analysis process\nprint(result.summary)     # Summary\nprint(result.sentiment)   # Sentiment\nprint(result.keywords)    # Keywords\n</code></pre>"},{"location":"examples/chain_of_thought/#with-custom-model-parameters","title":"With Custom Model Parameters","text":"<pre><code># Use with specific model and temperature\ncot = ChainOfThought(\n    QA,\n    model=\"gpt-4\",\n    temperature=0.0,  # Deterministic for math\n)\n\nresult = cot(question=\"What is the square root of 144?\")\n</code></pre>"},{"location":"examples/chain_of_thought/#comparison-with-vs-without-cot","title":"Comparison: With vs Without CoT","text":""},{"location":"examples/chain_of_thought/#without-chain-of-thought","title":"Without Chain of Thought","text":"<pre><code>from udspy import Predict\n\npredictor = Predict(QA)\nresult = predictor(question=\"Why is the sky blue?\")\n\nprint(result.answer)\n# \"The sky is blue due to Rayleigh scattering.\"\n</code></pre>"},{"location":"examples/chain_of_thought/#with-chain-of-thought","title":"With Chain of Thought","text":"<pre><code>cot_predictor = ChainOfThought(QA)\nresult = cot_predictor(question=\"Why is the sky blue?\")\n\nprint(result.reasoning)\n# \"Let me explain the physics: Sunlight contains all colors. As it enters\n#  the atmosphere, it interacts with air molecules. Blue light has shorter\n#  wavelengths and scatters more than other colors (Rayleigh scattering).\n#  This scattered blue light reaches our eyes from all directions.\"\n\nprint(result.answer)\n# \"The sky appears blue because blue light scatters more in the atmosphere\n#  due to its shorter wavelength (Rayleigh scattering).\"\n</code></pre> <p>Benefits: - More detailed and accurate answers - Shows the reasoning process - Better for complex or multi-step problems - Easier to verify correctness</p>"},{"location":"examples/chain_of_thought/#best-practices","title":"Best Practices","text":""},{"location":"examples/chain_of_thought/#1-use-for-complex-tasks","title":"1. Use for Complex Tasks","text":"<p>Chain of Thought shines for tasks requiring reasoning:</p> <pre><code># Good use cases\n- Math problems\n- Logic puzzles\n- Multi-step analysis\n- Proof generation\n- Planning tasks\n\n# Less useful for\n- Simple factual recall (\"What is 2+2?\")\n- Classification without reasoning\n- Direct information retrieval\n</code></pre>"},{"location":"examples/chain_of_thought/#2-adjust-temperature","title":"2. Adjust Temperature","text":"<pre><code># For deterministic tasks (math, logic)\ncot = ChainOfThought(QA, temperature=0.0)\n\n# For creative reasoning\ncot = ChainOfThought(QA, temperature=0.7)\n</code></pre>"},{"location":"examples/chain_of_thought/#3-review-reasoning-quality","title":"3. Review Reasoning Quality","text":"<p>Always check if reasoning makes sense:</p> <pre><code>result = cot(question=\"Complex problem\")\n\nif \"step\" in result.reasoning.lower():\n    print(\"\u2713 Good reasoning structure\")\n\nif len(result.reasoning) &lt; 50:\n    print(\"\u26a0 Reasoning might be too brief\")\n</code></pre>"},{"location":"examples/chain_of_thought/#real-world-examples","title":"Real-World Examples","text":""},{"location":"examples/chain_of_thought/#code-review-reasoning","title":"Code Review Reasoning","text":"<pre><code>class CodeReview(Signature):\n    \"\"\"Review code for issues.\"\"\"\n    code: str = InputField()\n    issues: list[str] = OutputField()\n    severity: str = OutputField()\n\nreviewer = ChainOfThought(CodeReview)\nresult = reviewer(code=\"\"\"\ndef divide(a, b):\n    return a / b\n\"\"\")\n\nprint(result.reasoning)\n# \"Let me analyze this code:\n#  1. No error handling for division by zero\n#  2. No type checking\n#  3. No documentation\n#  These are significant issues.\"\n\nprint(result.issues)\n# [\"Division by zero not handled\", \"Missing type hints\", \"No docstring\"]\n\nprint(result.severity)\n# \"High - can cause runtime errors\"\n</code></pre>"},{"location":"examples/chain_of_thought/#decision-making","title":"Decision Making","text":"<pre><code>class Decision(Signature):\n    \"\"\"Make informed decisions.\"\"\"\n    situation: str = InputField()\n    options: list[str] = InputField()\n    decision: str = OutputField()\n    justification: str = OutputField()\n\ndecider = ChainOfThought(Decision)\nresult = decider(\n    situation=\"Need to scale database, budget is tight\",\n    options=[\"Vertical scaling\", \"Horizontal scaling\", \"Managed service\"]\n)\n\nprint(result.reasoning)\n# \"Let me evaluate each option:\n#  - Vertical: Quick but limited and expensive long-term\n#  - Horizontal: Complex but scalable\n#  - Managed: Higher cost but less maintenance\n#  Given budget constraints...\"\n\nprint(result.decision)\nprint(result.justification)\n</code></pre> <p>See the full example in the repository.</p>"},{"location":"examples/confirmation/","title":"Confirmation Examples","text":"<p>This guide provides practical examples of using udspy's confirmation system for human-in-the-loop workflows.</p>"},{"location":"examples/confirmation/#quick-start","title":"Quick Start","text":""},{"location":"examples/confirmation/#basic-confirmation-loop","title":"Basic Confirmation Loop","text":"<p>The simplest pattern for handling confirmations is a loop with try/except:</p> <pre><code>from udspy import ReAct, ConfirmationRequired, ResumeState, tool\nfrom pydantic import Field\n\n# Define a tool that requires confirmation\n@tool(name=\"delete_file\", require_confirmation=True)\ndef delete_file(path: str = Field(description=\"File path\")) -&gt; str:\n    import os\n    os.remove(path)\n    return f\"Deleted {path}\"\n\n# Create agent\nagent = ReAct(\"question -&gt; answer\", tools=[delete_file])\n\n# Handle confirmations in a loop\nresume_state = None\n\nwhile True:\n    try:\n        result = agent(\n            question=\"Delete the file /tmp/old_data.txt\",\n            resume_state=resume_state\n        )\n        print(f\"Success: {result.answer}\")\n        break\n\n    except ConfirmationRequired as e:\n        print(f\"Confirmation needed: {e.question}\")\n        response = input(\"Your response (yes/no): \")\n        resume_state = ResumeState(e, response)\n</code></pre>"},{"location":"examples/confirmation/#interactive-cli-examples","title":"Interactive CLI Examples","text":""},{"location":"examples/confirmation/#example-1-basic-interactive-loop","title":"Example 1: Basic Interactive Loop","text":"<pre><code>import asyncio\nfrom udspy import ReAct, ConfirmationRequired, ResumeState, ConfirmationRejected, tool\nfrom pydantic import Field\n\n@tool(name=\"delete_file\", require_confirmation=True)\ndef delete_file(path: str = Field(description=\"File path\")) -&gt; str:\n    return f\"Would delete {path}\"\n\n@tool(name=\"search\")\ndef search(query: str = Field(description=\"Search query\")) -&gt; str:\n    return f\"Results for: {query}\"\n\nasync def interactive_agent():\n    \"\"\"Run agent with interactive confirmation handling.\"\"\"\n    agent = ReAct(\n        \"task -&gt; result\",\n        tools=[search, delete_file],\n        max_iters=5\n    )\n\n    task = \"Search for Python tutorials and delete /tmp/cache.txt\"\n    resume_state = None\n\n    while True:\n        try:\n            result = await agent.aforward(\n                task=task,\n                resume_state=resume_state\n            )\n            print(f\"\\n\u2713 Task completed!\")\n            print(f\"  Result: {result.result}\")\n            return result\n\n        except ConfirmationRequired as e:\n            print(f\"\\n\u26a0\ufe0f  Confirmation Required\")\n            print(f\"   {e.question}\")\n\n            if e.tool_call:\n                print(f\"   Tool: {e.tool_call.name}\")\n                print(f\"   Args: {e.tool_call.args}\")\n\n            response = input(\"\\n   Response (yes/no): \").strip()\n            user_response = response\n            resume_state = ResumeState(e, response)\n\n        except ConfirmationRejected as e:\n            print(f\"\\n\u2717 Operation rejected: {e.message}\")\n            return None\n\nif __name__ == \"__main__\":\n    asyncio.run(interactive_agent())\n</code></pre>"},{"location":"examples/confirmation/#example-2-enhanced-interactive-loop-with-argument-editing","title":"Example 2: Enhanced Interactive Loop with Argument Editing","text":"<pre><code>import json\nfrom udspy import ReAct, ConfirmationRequired, ResumeState, tool\nfrom pydantic import Field\n\n@tool(name=\"write_file\", require_confirmation=True)\ndef write_file(\n    path: str = Field(description=\"File path\"),\n    content: str = Field(description=\"File content\")\n) -&gt; str:\n    with open(path, \"w\") as f:\n        f.write(content)\n    return f\"Wrote to {path}\"\n\ndef run_with_editing(agent, question):\n    \"\"\"Interactive loop with argument editing support.\"\"\"\n    resume_state = None\n\n    while True:\n        try:\n            result = agent(\n                question=question,\n                resume_state=resume_state\n            )\n            print(f\"\\n\u2713 Success: {result.answer}\")\n            return result\n\n        except ConfirmationRequired as e:\n            print(f\"\\n\u26a0\ufe0f  Confirmation Required\")\n            print(f\"   {e.question}\")\n\n            if e.tool_call:\n                print(f\"\\n   Tool: {e.tool_call.name}\")\n                print(f\"   Args:\")\n                for key, value in e.tool_call.args.items():\n                    print(f\"     {key}: {value}\")\n\n            print(\"\\n   Options:\")\n            print(\"   - 'yes' to approve\")\n            print(\"   - 'no' to reject\")\n            print(\"   - 'edit' to modify arguments\")\n\n            response = input(\"\\n   Your choice: \").strip().lower()\n\n            if response == \"edit\" and e.tool_call:\n                # Let user edit arguments\n                print(\"\\n   Current arguments:\")\n                print(f\"   {json.dumps(e.tool_call.args, indent=2)}\")\n                print(\"\\n   Enter new arguments as JSON:\")\n                new_args_str = input(\"   &gt; \")\n\n                try:\n                    new_args = json.loads(new_args_str)\n                    user_response = json.dumps(new_args)\n                except json.JSONDecodeError:\n                    print(\"   Invalid JSON, treating as feedback\")\n                    user_response = new_args_str\n            else:\n                user_response = response\n\n            resume_state = ResumeState(e, response)\n\n# Usage\nagent = ReAct(\n    \"task -&gt; answer\",\n    tools=[write_file],\n    max_iters=5\n)\n\nresult = run_with_editing(\n    agent,\n    \"Write 'Hello World' to /tmp/greeting.txt\"\n)\n</code></pre>"},{"location":"examples/confirmation/#example-3-helper-function-for-common-pattern","title":"Example 3: Helper Function for Common Pattern","text":"<pre><code>from typing import Callable\nfrom udspy import Module, ConfirmationRequired, ResumeState, ConfirmationRejected\n\ndef run_with_confirmations(\n    agent: Module,\n    max_confirmations: int = 10,\n    on_confirmation: Callable[[ConfirmationRequired], str] | None = None,\n    **inputs\n):\n    \"\"\"\n    Helper function to run an agent with automatic confirmation handling.\n\n    Args:\n        agent: The agent/module to run\n        max_confirmations: Maximum number of confirmation rounds\n        on_confirmation: Optional callback for handling confirmations.\n                        If None, uses default CLI input.\n        **inputs: Input arguments for the agent\n\n    Returns:\n        The final prediction result\n\n    Raises:\n        RuntimeError: If max_confirmations is exceeded\n        ConfirmationRejected: If user rejects the operation\n    \"\"\"\n    resume_state = None\n\n    for attempt in range(max_confirmations):\n        try:\n            return agent(\n                resume_state=resume_state,\n                **inputs\n            )\n\n        except ConfirmationRequired as e:\n            if on_confirmation:\n                # Use custom handler\n                user_response = on_confirmation(e)\n            else:\n                # Default CLI handler\n                print(f\"\\n\u26a0\ufe0f  {e.question}\")\n                if e.tool_call:\n                    print(f\"   Tool: {e.tool_call.name}({e.tool_call.args})\")\n                response = input(\"   Response (yes/no): \").strip()\n\n            resume_state = ResumeState(e, response)\n\n    raise RuntimeError(f\"Exceeded {max_confirmations} confirmation requests\")\n\n# Usage example 1: Simple CLI\nagent = ReAct(\"task -&gt; result\", tools=[delete_file])\nresult = run_with_confirmations(agent, task=\"Delete old logs\")\n\n# Usage example 2: Custom handler\ndef auto_approve_safe(e: ConfirmationRequired) -&gt; str:\n    # Auto-approve if path is in /tmp\n    if e.tool_call and \"/tmp\" in str(e.tool_call.args.get(\"path\", \"\")):\n        print(f\"Auto-approved: {e.tool_call.name}\")\n        return \"yes\"\n    # Otherwise ask user\n    print(f\"Manual review needed: {e.question}\")\n    return input(\"Approve? (yes/no): \")\n\nresult = run_with_confirmations(\n    agent,\n    task=\"Delete /tmp/cache and /important/data\",\n    on_confirmation=auto_approve_safe\n)\n</code></pre>"},{"location":"examples/confirmation/#web-api-examples","title":"Web API Examples","text":""},{"location":"examples/confirmation/#example-4-fastapi-with-session-state","title":"Example 4: FastAPI with Session State","text":"<pre><code>from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import Any, Dict\nfrom udspy import ReAct, ConfirmationRequired, ResumeState, tool\nimport uuid\n\napp = FastAPI()\n\n# In-memory session storage (use Redis/DB in production)\nsessions: Dict[str, Dict[str, Any]] = {}\n\nclass StartRequest(BaseModel):\n    question: str\n\nclass ResumeRequest(BaseModel):\n    session_id: str\n    user_response: str\n\nclass StartResponse(BaseModel):\n    status: str\n    session_id: str | None = None\n    question: str | None = None\n    tool_call: Dict | None = None\n    result: str | None = None\n\n@tool(name=\"delete_file\", require_confirmation=True)\ndef delete_file(path: str) -&gt; str:\n    return f\"Deleted {path}\"\n\n# Create agent\nagent = ReAct(\"question -&gt; answer\", tools=[delete_file])\n\n@app.post(\"/agent/start\", response_model=StartResponse)\nasync def start_agent(request: StartRequest):\n    \"\"\"Start a new agent execution.\"\"\"\n    try:\n        result = await agent.aforward(question=request.question)\n        return StartResponse(\n            status=\"completed\",\n            result=result.answer\n        )\n\n    except ConfirmationRequired as e:\n        # Save state in session\n        session_id = str(uuid.uuid4())\n        sessions[session_id] = {\n            \"state\": e,\n            \"question\": request.question\n        }\n\n        return StartResponse(\n            status=\"confirmation_required\",\n            session_id=session_id,\n            question=e.question,\n            tool_call=e.tool_call.__dict__ if e.tool_call else None\n        )\n\n@app.post(\"/agent/resume\", response_model=StartResponse)\nasync def resume_agent(request: ResumeRequest):\n    \"\"\"Resume agent execution after confirmation.\"\"\"\n    if request.session_id not in sessions:\n        raise HTTPException(status_code=404, detail=\"Session not found\")\n\n    session = sessions[request.session_id]\n\n    try:\n        result = await agent.aforward(\n            question=session[\"question\"],\n            resume_state=session[\"state\"],\n            user_response=request.user_response\n        )\n\n        # Clean up session on success\n        del sessions[request.session_id]\n\n        return StartResponse(\n            status=\"completed\",\n            result=result.answer\n        )\n\n    except ConfirmationRequired as e:\n        # Need another confirmation - update session\n        session[\"state\"] = e\n\n        return StartResponse(\n            status=\"confirmation_required\",\n            session_id=request.session_id,\n            question=e.question,\n            tool_call=e.tool_call.__dict__ if e.tool_call else None\n        )\n</code></pre>"},{"location":"examples/confirmation/#example-5-async-queue-pattern","title":"Example 5: Async Queue Pattern","text":"<pre><code>import asyncio\nfrom asyncio import Queue\nfrom typing import Any\nfrom udspy import ReAct, ConfirmationRequired, ResumeState\n\nclass ConfirmationRequest:\n    def __init__(self, exception: ConfirmationRequired):\n        self.exception = exception\n        self.response_future = asyncio.Future()\n\n    async def get_response(self) -&gt; str:\n        return await self.response_future\n\n    def set_response(self, response: str):\n        self.response_future.set_result(response)\n\nasync def agent_worker(\n    agent: ReAct,\n    question: str,\n    confirmation_queue: Queue[ConfirmationRequest]\n):\n    \"\"\"Worker that processes tasks and sends confirmations to queue.\"\"\"\n    resume_state = None\n\n    while True:\n        try:\n            result = await agent.aforward(\n                question=question,\n                resume_state=resume_state\n            )\n            return result\n\n        except ConfirmationRequired as e:\n            # Send confirmation request to queue\n            req = ConfirmationRequest(e)\n            await confirmation_queue.put(req)\n\n            # Wait for user response\n            user_response = await req.get_response()\n            resume_state = ResumeState(e, response)\n\nasync def confirmation_handler(confirmation_queue: Queue[ConfirmationRequest]):\n    \"\"\"Handler that processes confirmation requests from queue.\"\"\"\n    while True:\n        req = await confirmation_queue.get()\n        e = req.exception\n\n        print(f\"\\n\u26a0\ufe0f  Confirmation: {e.question}\")\n        if e.tool_call:\n            print(f\"   Tool: {e.tool_call.name}({e.tool_call.args})\")\n\n        # Get user input (in real app, might be from WebSocket, UI, etc.)\n        response = input(\"   Response: \")\n        req.set_response(response)\n\nasync def run_with_queue(agent, question):\n    \"\"\"Run agent with async confirmation handling.\"\"\"\n    queue = Queue()\n\n    # Start both worker and handler\n    worker_task = asyncio.create_task(agent_worker(agent, question, queue))\n    handler_task = asyncio.create_task(confirmation_handler(queue))\n\n    try:\n        result = await worker_task\n        handler_task.cancel()\n        return result\n    except Exception as e:\n        handler_task.cancel()\n        raise\n</code></pre>"},{"location":"examples/confirmation/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"examples/confirmation/#example-6-multi-agent-with-shared-confirmations","title":"Example 6: Multi-Agent with Shared Confirmations","text":"<pre><code>import asyncio\nfrom typing import List\nfrom udspy import ReAct, ConfirmationRequired, ResumeState\n\nasync def run_agent_with_confirmations(\n    agent: ReAct,\n    agent_id: int,\n    question: str\n) -&gt; Any:\n    \"\"\"Run single agent with confirmation handling.\"\"\"\n    resume_state = None\n\n    for attempt in range(5):\n        try:\n            result = await agent.aforward(\n                question=question,\n                resume_state=resume_state\n            )\n            return {\n                \"agent_id\": agent_id,\n                \"result\": result.answer,\n                \"status\": \"success\"\n            }\n\n        except ConfirmationRequired as e:\n            print(f\"\\n[Agent {agent_id}] {e.question}\")\n            response = input(f\"[Agent {agent_id}] Response: \")\n            resume_state = ResumeState(e, response)\n\n    return {\n        \"agent_id\": agent_id,\n        \"result\": None,\n        \"status\": \"too_many_confirmations\"\n    }\n\nasync def run_multi_agent(questions: List[str]):\n    \"\"\"Run multiple agents concurrently with interactive confirmations.\"\"\"\n    agent = ReAct(\"question -&gt; answer\", tools=[...])\n\n    tasks = [\n        run_agent_with_confirmations(agent, i, q)\n        for i, q in enumerate(questions)\n    ]\n\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    return results\n\n# Usage\nquestions = [\n    \"Analyze log files\",\n    \"Clean up temp directory\",\n    \"Generate report\"\n]\n\nresults = asyncio.run(run_multi_agent(questions))\n</code></pre>"},{"location":"examples/confirmation/#example-7-conditional-auto-approval","title":"Example 7: Conditional Auto-Approval","text":"<pre><code>from typing import Callable\nfrom udspy import ConfirmationRequired, ResumeState, ReAct\n\ndef create_smart_handler(\n    auto_approve: Callable[[ConfirmationRequired], bool]\n) -&gt; Callable[[ConfirmationRequired], str]:\n    \"\"\"\n    Create a handler that auto-approves based on custom logic.\n\n    Args:\n        auto_approve: Function that returns True if safe to auto-approve\n\n    Returns:\n        Handler function for confirmations\n    \"\"\"\n    def handler(e: ConfirmationRequired) -&gt; str:\n        if auto_approve(e):\n            print(f\"\u2713 Auto-approved: {e.tool_call.name if e.tool_call else e.question}\")\n            return \"yes\"\n        else:\n            print(f\"\u26a0\ufe0f  Manual review: {e.question}\")\n            if e.tool_call:\n                print(f\"   Tool: {e.tool_call.name}({e.tool_call.args})\")\n            return input(\"   Approve? (yes/no): \")\n\n    return handler\n\n# Define approval rules\ndef is_safe_operation(e: ConfirmationRequired) -&gt; bool:\n    \"\"\"Check if operation is safe to auto-approve.\"\"\"\n    if not e.tool_call:\n        return False\n\n    # Auto-approve reads\n    if e.tool_call.name in [\"search\", \"read_file\", \"list_files\"]:\n        return True\n\n    # Auto-approve writes to /tmp\n    if e.tool_call.name in [\"write_file\", \"delete_file\"]:\n        path = e.tool_call.args.get(\"path\", \"\")\n        if path.startswith(\"/tmp/\"):\n            return True\n\n    return False\n\n# Use the smart handler\nhandler = create_smart_handler(is_safe_operation)\n\nresult = run_with_confirmations(\n    agent,\n    question=\"Search logs, delete /tmp/cache, delete /prod/database\",\n    on_confirmation=handler\n)\n</code></pre>"},{"location":"examples/confirmation/#example-8-timeout-based-confirmation","title":"Example 8: Timeout-based Confirmation","text":"<pre><code>import asyncio\nfrom udspy import ConfirmationRequired, ResumeState, ReAct\n\nasync def get_user_input_with_timeout(\n    question: str,\n    timeout_seconds: int = 30,\n    default_response: str = \"no\"\n) -&gt; str:\n    \"\"\"Get user input with timeout, returning default if no response.\"\"\"\n    print(f\"\\n\u26a0\ufe0f  {question}\")\n    print(f\"   (Timeout in {timeout_seconds}s, default: {default_response})\")\n\n    try:\n        # Run input in executor to avoid blocking\n        loop = asyncio.get_event_loop()\n        response = await asyncio.wait_for(\n            loop.run_in_executor(None, lambda: input(\"   Response: \")),\n            timeout=timeout_seconds\n        )\n        return response.strip()\n\n    except asyncio.TimeoutError:\n        print(f\"   \u23f1\ufe0f  Timeout - using default: {default_response}\")\n        return default_response\n\nasync def run_with_timeout(agent, question):\n    \"\"\"Run agent with timeout-based confirmations.\"\"\"\n    resume_state = None\n\n    while True:\n        try:\n            result = await agent.aforward(\n                question=question,\n                resume_state=resume_state\n            )\n            return result\n\n        except ConfirmationRequired as e:\n            # Timeout after 30 seconds, default to \"no\" (safe)\n            user_response = await get_user_input_with_timeout(\n                e.question,\n                timeout_seconds=30,\n                default_response=\"no\"\n            )\n            resume_state = ResumeState(e, response)\n</code></pre>"},{"location":"examples/confirmation/#testing-confirmations","title":"Testing Confirmations","text":""},{"location":"examples/confirmation/#example-9-testing-with-mock-confirmations","title":"Example 9: Testing with Mock Confirmations","text":"<pre><code>from udspy import ConfirmationRequired, ResumeState, ReAct, respond_to_confirmation\n\nasync def test_agent_with_confirmation():\n    \"\"\"Test agent behavior with confirmations.\"\"\"\n    agent = ReAct(\"question -&gt; answer\", tools=[delete_file])\n\n    # First call - should raise confirmation\n    try:\n        result = await agent.aforward(question=\"Delete /tmp/test.txt\")\n        assert False, \"Should have raised ConfirmationRequired\"\n    except ConfirmationRequired as e:\n        assert \"delete\" in e.question.lower()\n        assert e.tool_call.name == \"delete_file\"\n\n        # Programmatically approve\n        respond_to_confirmation(e.confirmation_id, approved=True)\n\n    # Resume - should complete\n    result = await agent.aforward(\n        question=\"Delete /tmp/test.txt\",\n        resume_state=e,\n        user_response=\"yes\"\n    )\n\n    assert \"deleted\" in result.answer.lower()\n</code></pre>"},{"location":"examples/confirmation/#see-also","title":"See Also","text":"<ul> <li>Confirmation Architecture - Design and implementation details</li> <li>Confirmation API - API reference</li> <li>ReAct Examples - ReAct-specific confirmation examples</li> <li>Tool API - Creating confirmable tools</li> </ul>"},{"location":"examples/context_settings/","title":"Context-Specific Settings","text":"<p>Learn how to use different API keys, models, and settings in different contexts.</p>"},{"location":"examples/context_settings/#overview","title":"Overview","text":"<p>The <code>settings.context()</code> context manager allows you to temporarily override global settings for specific operations. This is useful for:</p> <ul> <li>Multi-tenant applications with different API keys per user</li> <li>Testing with different models</li> <li>Varying temperature or other parameters per request</li> <li>Isolating settings in async operations</li> </ul> <p>The context manager is thread-safe using Python's <code>contextvars</code>, making it safe for concurrent operations.</p>"},{"location":"examples/context_settings/#basic-usage","title":"Basic Usage","text":""},{"location":"examples/context_settings/#override-model","title":"Override Model","text":"<pre><code>import udspy\nfrom udspy import LM\n\n# Configure global settings\nglobal_lm = LM(model=\"gpt-4o-mini\", api_key=\"sk-global\")\nudspy.settings.configure(lm=global_lm)\n\n# Temporarily use a different model\ngpt4_lm = LM(model=\"gpt-4\", api_key=\"sk-global\")\nwith udspy.settings.context(lm=gpt4_lm):\n    predictor = Predict(QA)\n    result = predictor(question=\"What is AI?\")\n    # Uses gpt-4\n\n# Back to global settings (gpt-4o-mini)\nresult = predictor(question=\"What is ML?\")\n</code></pre>"},{"location":"examples/context_settings/#override-api-key","title":"Override API Key","text":"<pre><code># Use a different API key for specific requests\nuser_lm = LM(model=\"gpt-4o-mini\", api_key=\"sk-user-specific\")\nwith udspy.settings.context(lm=user_lm):\n    result = predictor(question=\"User-specific query\")\n    # Uses the user-specific API key\n</code></pre>"},{"location":"examples/context_settings/#override-multiple-settings","title":"Override Multiple Settings","text":"<pre><code>gpt4_lm = LM(model=\"gpt-4\", api_key=\"sk-...\")\nwith udspy.settings.context(\n    lm=gpt4_lm,\n    temperature=0.0,\n    max_tokens=500\n):\n    result = predictor(question=\"Deterministic response needed\")\n</code></pre>"},{"location":"examples/context_settings/#multi-tenant-applications","title":"Multi-Tenant Applications","text":"<p>Handle different users with different API keys:</p> <pre><code>from udspy import LM\n\ndef handle_user_request(user_id: str, question: str):\n    \"\"\"Handle a request from a specific user.\"\"\"\n    # Get user-specific API key from database\n    user_api_key = get_user_api_key(user_id)\n\n    # Use user's API key for this request\n    user_lm = LM(model=\"gpt-4o-mini\", api_key=user_api_key)\n    with udspy.settings.context(lm=user_lm):\n        predictor = Predict(QA)\n        result = predictor(question=question)\n\n    return result.answer\n\n# Each user's request uses their own API key\nanswer1 = handle_user_request(\"user1\", \"What is Python?\")\nanswer2 = handle_user_request(\"user2\", \"What is Rust?\")\n</code></pre>"},{"location":"examples/context_settings/#nested-contexts","title":"Nested Contexts","text":"<p>Contexts can be nested, with inner contexts overriding outer ones:</p> <pre><code>from udspy import LM\n\nglobal_lm = LM(model=\"gpt-4o-mini\", api_key=\"sk-...\")\nudspy.settings.configure(lm=global_lm, temperature=0.7)\n\ngpt4_lm = LM(model=\"gpt-4\", api_key=\"sk-...\")\nwith udspy.settings.context(lm=gpt4_lm, temperature=0.5):\n    # Uses gpt-4, temp=0.5\n\n    with udspy.settings.context(temperature=0.0):\n        # Uses gpt-4 (inherited), temp=0.0 (overridden)\n        pass\n\n    # Back to gpt-4, temp=0.5\n\n# Back to gpt-4o-mini, temp=0.7\n</code></pre>"},{"location":"examples/context_settings/#async-support","title":"Async Support","text":"<p>Context managers work seamlessly with async code:</p> <pre><code>import asyncio\nfrom udspy import LM\n\nasync def generate_response(question: str, user_api_key: str):\n    user_lm = LM(model=\"gpt-4o-mini\", api_key=user_api_key)\n    with udspy.settings.context(lm=user_lm):\n        predictor = StreamingPredict(QA)\n        async for chunk in predictor.stream(question=question):\n            yield chunk\n\n# Handle multiple users concurrently\nasync def main():\n    tasks = [\n        generate_response(\"Question 1\", \"sk-user1\"),\n        generate_response(\"Question 2\", \"sk-user2\"),\n    ]\n    await asyncio.gather(*tasks)\n</code></pre>"},{"location":"examples/context_settings/#testing","title":"Testing","text":"<p>Use contexts to isolate test settings:</p> <pre><code>from udspy import LM\n\ndef test_with_specific_model():\n    \"\"\"Test behavior with a specific model.\"\"\"\n    test_lm = LM(model=\"gpt-4\", api_key=\"sk-test\")\n    with udspy.settings.context(\n        lm=test_lm,\n        temperature=0.0,  # Deterministic for testing\n    ):\n        predictor = Predict(QA)\n        result = predictor(question=\"2+2\")\n        assert \"4\" in result.answer\n</code></pre>"},{"location":"examples/context_settings/#custom-endpoints","title":"Custom Endpoints","text":"<p>You can use custom endpoints with the LM factory:</p> <pre><code>from udspy import LM\n\n# Use custom endpoint\ncustom_lm = LM(\n    model=\"custom-model\",\n    api_key=\"sk-custom\",\n    base_url=\"https://custom-endpoint.example.com/v1\",\n)\n\nwith udspy.settings.context(lm=custom_lm):\n    # Uses custom endpoint\n    result = predictor(question=\"...\")\n</code></pre>"},{"location":"examples/context_settings/#complete-example","title":"Complete Example","text":"<pre><code>import udspy\nfrom udspy import Signature, InputField, OutputField, Predict, LM\n\n# Global configuration\ndefault_lm = LM(model=\"gpt-4o-mini\", api_key=\"sk-default\")\nudspy.settings.configure(lm=default_lm, temperature=0.7)\n\nclass QA(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\n\n# Scenario 1: Default settings\nresult = predictor(question=\"What is AI?\")\n\n# Scenario 2: High-quality request (use GPT-4)\ngpt4_lm = LM(model=\"gpt-4\", api_key=\"sk-default\")\nwith udspy.settings.context(lm=gpt4_lm):\n    result = predictor(question=\"Explain quantum computing\")\n\n# Scenario 3: Deterministic response\nwith udspy.settings.context(temperature=0.0):\n    result = predictor(question=\"What is 2+2?\")\n\n# Scenario 4: User-specific API key\nuser_lm = LM(model=\"gpt-4o-mini\", api_key=user.api_key)\nwith udspy.settings.context(lm=user_lm):\n    result = predictor(question=user.question)\n</code></pre> <p>See the full example in the repository.</p>"},{"location":"examples/dynamic_tools/","title":"Dynamic Tool Management","text":"<p>Dynamic tool management allows tools to modify the module during execution by adding or removing other tools. This is useful when you need specialized tools that should only be loaded on demand.</p>"},{"location":"examples/dynamic_tools/#overview","title":"Overview","text":"<p>Tools can return module callbacks - special callables decorated with <code>@module_callback</code> that receive execution context and can call <code>init_module()</code> to modify available tools.</p>"},{"location":"examples/dynamic_tools/#key-concepts","title":"Key Concepts","text":"<ol> <li>Module Callbacks: Functions decorated with <code>@module_callback</code> that can modify module state</li> <li>Tool Loaders: Tools that return module callbacks to add other tools</li> <li>Context Objects: Provide access to the module instance and execution state</li> <li>Dynamic Loading: Tools are added during execution and persist until completion</li> </ol>"},{"location":"examples/dynamic_tools/#basic-example-calculator","title":"Basic Example: Calculator","text":"<p>Here's a complete example showing a ReAct agent that dynamically loads a calculator tool:</p> <pre><code>from pydantic import Field\nimport udspy\nfrom udspy import ReAct, Signature, tool, module_callback, InputField, OutputField\n\n# Define the calculator tool (not initially available)\n@tool(name=\"calculator\", description=\"Perform mathematical calculations\")\ndef calculator(expression: str = Field(...)) -&gt; str:\n    \"\"\"Evaluate a math expression.\"\"\"\n    try:\n        result = eval(expression, {\"__builtins__\": {}}, {})\n        return f\"Result: {result}\"\n    except Exception as e:\n        return f\"Error: {e}\"\n\n# Define the tool loader\n@tool(name=\"load_calculator\", description=\"Load the calculator tool\")\ndef load_calculator() -&gt; callable:\n    \"\"\"Load calculator tool dynamically.\"\"\"\n\n    @module_callback\n    def add_calculator(context):\n        # Get current tools (excluding built-ins)\n        current_tools = [\n            t for t in context.module.tools.values()\n            if t.name not in (\"finish\", \"user_clarification\")\n        ]\n\n        # Add calculator to available tools\n        context.module.init_module(tools=current_tools + [calculator])\n\n        return \"Calculator loaded successfully\"\n\n    return add_calculator\n\n# Create agent with only the loader\nclass Question(Signature):\n    \"\"\"Answer questions. Load tools if needed.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\nagent = ReAct(Question, tools=[load_calculator])\n\n# Agent will load calculator when needed\nresult = agent(question=\"What is 157 * 834?\")\nprint(result.answer)  # 130938\n</code></pre>"},{"location":"examples/dynamic_tools/#trajectory-breakdown","title":"Trajectory Breakdown","text":"<p>When the agent processes \"What is 157 * 834?\", the trajectory looks like:</p> <ol> <li>Thought: \"I need to calculate 157 * 834. I don't have a calculator tool, so I'll load it first.\"</li> <li>Tool Call: <code>load_calculator()</code> \u2192 Returns module callback</li> <li>Callback Execution: Calculator tool is added to available tools</li> <li>Observation: \"Calculator loaded successfully\"</li> <li>Thought: \"Now I can use the calculator\"</li> <li>Tool Call: <code>calculator(expression=\"157 * 834\")</code> \u2192 Returns \"Result: 130938\"</li> <li>Observation: \"Result: 130938\"</li> <li>Thought: \"I have the answer\"</li> <li>Tool Call: <code>finish()</code> with answer \"130938\"</li> </ol>"},{"location":"examples/dynamic_tools/#context-objects","title":"Context Objects","text":"<p>Module callbacks receive a context object with access to the module and execution state:</p>"},{"location":"examples/dynamic_tools/#reactcontext","title":"ReactContext","text":"<p>For ReAct modules, the context includes the trajectory:</p> <pre><code>@module_callback\ndef my_callback(context: ReactContext):\n    # Access the module\n    module = context.module\n\n    # Access current tools\n    current_tools = list(context.module.tools.values())\n\n    # Access trajectory history\n    trajectory = context.trajectory\n    thoughts = [v for k, v in trajectory.items() if k.startswith(\"thought_\")]\n\n    # Modify tools\n    context.module.init_module(tools=current_tools + [new_tool])\n\n    return \"Tools updated\"\n</code></pre>"},{"location":"examples/dynamic_tools/#predictcontext","title":"PredictContext","text":"<p>For Predict modules, the context includes conversation history:</p> <pre><code>@module_callback\ndef my_callback(context: PredictContext):\n    # Access the module\n    module = context.module\n\n    # Access conversation history\n    history = context.history\n    if history:\n        messages = history.messages\n\n    # Modify tools\n    context.module.init_module(tools=[...])\n\n    return \"Tools updated\"\n</code></pre>"},{"location":"examples/dynamic_tools/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"examples/dynamic_tools/#category-based-loading","title":"Category-Based Loading","text":"<p>Load different tool sets based on categories:</p> <pre><code>@tool(name=\"load_tools\", description=\"Load specialized tools\")\ndef load_tools(category: str = Field(...)) -&gt; callable:\n    \"\"\"Load tools for a specific category.\"\"\"\n\n    @module_callback\n    def add_tools(context):\n        current = [\n            t for t in context.module.tools.values()\n            if t.name not in (\"finish\", \"user_clarification\", \"load_tools\")\n        ]\n\n        new_tools = []\n        if category == \"math\":\n            new_tools = [calculator, statistics_tool]\n        elif category == \"web\":\n            new_tools = [search_tool, scrape_tool]\n        elif category == \"data\":\n            new_tools = [csv_tool, json_tool]\n\n        context.module.init_module(tools=current + new_tools)\n\n        tool_names = [t.name for t in new_tools]\n        return f\"Loaded {len(new_tools)} tools: {', '.join(tool_names)}\"\n\n    return add_tools\n</code></pre>"},{"location":"examples/dynamic_tools/#conditional-tool-loading","title":"Conditional Tool Loading","text":"<p>Load tools based on context analysis:</p> <pre><code>@tool(name=\"analyze_and_load\", description=\"Analyze question and load needed tools\")\ndef analyze_and_load(question: str = Field(...)) -&gt; callable:\n    \"\"\"Analyze question and load appropriate tools.\"\"\"\n\n    @module_callback\n    def smart_load(context):\n        current = list(context.module.tools.values())\n        new_tools = []\n\n        # Analyze what's needed\n        if any(word in question.lower() for word in [\"calculate\", \"multiply\", \"add\"]):\n            new_tools.append(calculator)\n        if any(word in question.lower() for word in [\"weather\", \"temperature\"]):\n            new_tools.append(weather_tool)\n        if any(word in question.lower() for word in [\"search\", \"find\", \"lookup\"]):\n            new_tools.append(search_tool)\n\n        context.module.init_module(tools=current + new_tools)\n\n        return f\"Loaded {len(new_tools)} tools based on question analysis\"\n\n    return smart_load\n</code></pre>"},{"location":"examples/dynamic_tools/#tool-replacement","title":"Tool Replacement","text":"<p>Replace existing tools with updated versions:</p> <pre><code>@tool(name=\"upgrade_tools\", description=\"Upgrade to advanced versions\")\ndef upgrade_tools() -&gt; callable:\n    \"\"\"Replace basic tools with advanced versions.\"\"\"\n\n    @module_callback\n    def do_upgrade(context):\n        # Remove basic tools\n        current = [\n            t for t in context.module.tools.values()\n            if not t.name.startswith(\"basic_\")\n        ]\n\n        # Add advanced tools\n        advanced = [advanced_calculator, advanced_search]\n\n        context.module.init_module(tools=current + advanced)\n\n        return \"Upgraded to advanced tools\"\n\n    return do_upgrade\n</code></pre>"},{"location":"examples/dynamic_tools/#important-notes","title":"Important Notes","text":""},{"location":"examples/dynamic_tools/#tool-persistence","title":"Tool Persistence","text":"<p>Tools loaded via module callbacks persist for the entire execution:</p> <pre><code>agent = ReAct(Question, tools=[load_calculator])\n\n# First call - loads calculator\nresult1 = agent(question=\"What is 10 * 5?\")\n# Calculator is now in agent.tools\n\n# Second call - calculator still available from before\nresult2 = agent(question=\"What is 20 + 15?\")\n</code></pre> <p>If you want fresh tool state, create a new agent instance.</p>"},{"location":"examples/dynamic_tools/#return-values","title":"Return Values","text":"<p>Module callbacks must return a string that becomes the observation in the trajectory:</p> <pre><code>@module_callback\ndef callback(context):\n    context.module.init_module(tools=[...])\n    return \"This string appears in the trajectory\"  # Required!\n</code></pre>"},{"location":"examples/dynamic_tools/#built-in-tools","title":"Built-in Tools","text":"<p>ReAct's built-in tools (<code>finish</code>, user clarification) are automatically preserved when you call <code>init_module()</code>. You don't need to include them manually.</p>"},{"location":"examples/dynamic_tools/#error-handling","title":"Error Handling","text":"<p>Handle errors gracefully in your callbacks:</p> <pre><code>@module_callback\ndef safe_callback(context):\n    try:\n        # Load tools\n        context.module.init_module(tools=[...])\n        return \"Success\"\n    except Exception as e:\n        return f\"Failed to load tools: {e}\"\n</code></pre>"},{"location":"examples/dynamic_tools/#use-cases","title":"Use Cases","text":""},{"location":"examples/dynamic_tools/#1-on-demand-capabilities","title":"1. On-Demand Capabilities","text":"<p>Load expensive or specialized tools only when needed:</p> <pre><code># Start lightweight, load heavy tools on demand\nagent = ReAct(Task, tools=[load_nlp_tools, load_vision_tools])\n</code></pre>"},{"location":"examples/dynamic_tools/#2-progressive-tool-discovery","title":"2. Progressive Tool Discovery","text":"<p>Agent discovers what tools it needs as it works:</p> <pre><code># Agent figures out it needs web search, then calculator, then data analysis\nagent = ReAct(Task, tools=[load_tools])\n</code></pre>"},{"location":"examples/dynamic_tools/#3-multi-tenant-applications","title":"3. Multi-Tenant Applications","text":"<p>Load user-specific or permission-based tools:</p> <pre><code>@tool(name=\"load_user_tools\")\ndef load_user_tools(user_id: str) -&gt; callable:\n    @module_callback\n    def add_user_tools(context):\n        user_tools = get_tools_for_user(user_id)\n        context.module.init_module(tools=user_tools)\n        return f\"Loaded tools for user {user_id}\"\n    return add_user_tools\n</code></pre>"},{"location":"examples/dynamic_tools/#4-adaptive-tool-sets","title":"4. Adaptive Tool Sets","text":"<p>Adjust tools based on task complexity:</p> <pre><code>@tool(name=\"assess_and_load\")\ndef assess_and_load(task: str) -&gt; callable:\n    @module_callback\n    def adaptive_load(context):\n        if is_complex(task):\n            tools = advanced_tools\n        else:\n            tools = basic_tools\n        context.module.init_module(tools=tools)\n        return \"Tools loaded based on task complexity\"\n    return adaptive_load\n</code></pre>"},{"location":"examples/dynamic_tools/#see-also","title":"See Also","text":"<ul> <li>ReAct API Reference</li> <li>Tool Calling Guide</li> <li>Module Callbacks API</li> </ul> <p>Example Code: See <code>examples/dynamic_calculator.py</code> and <code>examples/dynamic_tools.py</code> in the GitHub repository</p>"},{"location":"examples/history/","title":"Conversation History","text":"<p>The <code>History</code> class manages conversation history for multi-turn interactions. When passed to <code>Predict</code>, it automatically maintains context across multiple calls.</p>"},{"location":"examples/history/#basic-usage","title":"Basic Usage","text":"<pre><code>from udspy import History, Predict, Signature, InputField, OutputField\n\nclass QA(Signature):\n    '''Answer questions.'''\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\nhistory = History()\n\n# First turn\nresult = predictor(question=\"What is Python?\", history=history)\nprint(result.answer)\n\n# Second turn - context is maintained\nresult = predictor(question=\"What are its main features?\", history=history)\nprint(result.answer)  # Assistant knows we're still talking about Python\n</code></pre>"},{"location":"examples/history/#how-it-works","title":"How It Works","text":"<p><code>History</code> stores messages in OpenAI format and automatically: - Manages the system prompt - Always keeps it at position 0, derived from your signature - Adds user messages when you call the predictor - Adds assistant responses after generation - Maintains tool calls and results (when using tool calling) - Preserves conversation context across turns</p> <p>The system prompt is automatically set based on your signature, so you typically only need to track user/assistant messages. This makes managing conversation history much simpler!</p>"},{"location":"examples/history/#api","title":"API","text":""},{"location":"examples/history/#creating-history","title":"Creating History","text":"<pre><code># Empty history\nhistory = History()\n\n# With initial messages\nhistory = History(messages=[\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"}\n])\n</code></pre>"},{"location":"examples/history/#adding-messages","title":"Adding Messages","text":"<pre><code># Add user message\nhistory.add_user_message(\"What is AI?\")\n\n# Add assistant message\nhistory.add_assistant_message(\"AI stands for Artificial Intelligence...\")\n\n# Set system message (always at position 0, replaces existing)\nhistory.set_system_message(\"You are a helpful tutor\")\n\n# Add system message (appends to end - use set_system_message() instead)\nhistory.add_system_message(\"You are a helpful tutor\")  # Not recommended\n\n# Add tool result\nhistory.add_tool_result(tool_call_id=\"call_123\", content=\"Result: 42\")\n\n# Add generic message\nhistory.add_message(\"user\", \"Custom message\")\n</code></pre> <p>Note: Use <code>set_system_message()</code> instead of <code>add_system_message()</code> to ensure the system prompt is always at position 0. When using <code>Predict</code>, the system prompt is automatically managed based on your signature, so you rarely need to set it manually.</p>"},{"location":"examples/history/#managing-history","title":"Managing History","text":"<pre><code># Get number of messages\nprint(len(history))  # e.g., 5\n\n# Clear all messages\nhistory.clear()\n\n# Copy history (for branching conversations)\nbranch = history.copy()\n\n# Access messages directly\nfor msg in history.messages:\n    print(f\"{msg['role']}: {msg['content']}\")\n\n# String representation\nprint(history)  # Shows formatted conversation\n</code></pre>"},{"location":"examples/history/#automatic-system-prompt-management","title":"Automatic System Prompt Management","text":"<p>One of History's key features is automatic system prompt management. When you pass a History to Predict, the system prompt is automatically:</p> <ol> <li>Set from your signature - The prompt is derived from your signature's docstring and fields</li> <li>Placed at position 0 - Always the first message in the conversation</li> <li>Updated on each call - Keeps in sync with your signature if you change predictors</li> </ol> <p>This means you can focus on tracking the actual conversation (user/assistant messages) and let udspy handle the system prompt:</p> <pre><code>from udspy import History, Predict, Signature, InputField, OutputField\n\nclass QA(Signature):\n    \"\"\"Answer questions about programming.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\npredictor = Predict(QA)\n\n# Start with an empty history\nhistory = History()\nprint(f\"Messages: {len(history)}\")  # 0\n\n# First call - system prompt automatically added\nresult = predictor(question=\"What is Python?\", history=history)\nprint(f\"Messages: {len(history)}\")  # 2 (system + user)\nprint(f\"First message role: {history.messages[0]['role']}\")  # \"system\"\n\n# Second call - system prompt stays at position 0\nresult = predictor(question=\"What about JavaScript?\", history=history)\nprint(f\"Messages: {len(history)}\")  # 4 (system + user + assistant + user)\nprint(f\"First message role: {history.messages[0]['role']}\")  # Still \"system\"\n</code></pre>"},{"location":"examples/history/#pre-populating-with-user-messages-only","title":"Pre-populating with User Messages Only","text":"<p>You can create a history with just conversation context, and the system prompt will be automatically prepended:</p> <pre><code># Load previous conversation from database (user/assistant only)\nhistory = History()\nhistory.add_user_message(\"Tell me about Python\")\nhistory.add_assistant_message(\"Python is a programming language...\")\nhistory.add_user_message(\"Is it beginner friendly?\")\nhistory.add_assistant_message(\"Yes! Python is great for beginners...\")\n\nprint(f\"First message role: {history.messages[0]['role']}\")  # \"user\"\n\n# Pass to Predict - system prompt prepended automatically\nresult = predictor(question=\"What about advanced features?\", history=history)\n\nprint(f\"First message role: {history.messages[0]['role']}\")  # Now \"system\"!\n</code></pre>"},{"location":"examples/history/#use-cases","title":"Use Cases","text":""},{"location":"examples/history/#multi-turn-conversations","title":"Multi-Turn Conversations","text":"<pre><code>predictor = Predict(QA)\nhistory = History()\n\n# Each call maintains context\npredictor(question=\"What is machine learning?\", history=history)\npredictor(question=\"How does it differ from traditional programming?\", history=history)\npredictor(question=\"Can you give me an example?\", history=history)\n</code></pre>"},{"location":"examples/history/#pre-populating-context","title":"Pre-Populating Context","text":"<pre><code>history = History()\n\n# Pre-populate with previous conversation (user/assistant only)\n# System prompt will be automatically added by Predict\nhistory.add_user_message(\"I'm learning Python\")\nhistory.add_assistant_message(\"Great! I'm here to help.\")\n\n# System prompt is automatically prepended at position 0\nresult = predictor(question=\"How do I use list comprehensions?\", history=history)\n\n# history.messages[0] is now the system prompt from the signature\n# history.messages[1] is \"I'm learning Python\"\n# history.messages[2] is \"Great! I'm here to help.\"\n</code></pre> <p>Tip: You only need to track user/assistant messages. The system prompt is automatically managed based on your signature.</p>"},{"location":"examples/history/#branching-conversations","title":"Branching Conversations","text":"<pre><code>main_history = History()\n\n# Start main conversation\npredictor(question=\"Tell me about programming languages\", history=main_history)\n\n# Branch 1: Explore Python\npython_branch = main_history.copy()\npredictor(question=\"Tell me more about Python\", history=python_branch)\n\n# Branch 2: Explore JavaScript\njs_branch = main_history.copy()\npredictor(question=\"Tell me more about JavaScript\", history=js_branch)\n\n# Each branch maintains independent context\n</code></pre>"},{"location":"examples/history/#conversation-reset","title":"Conversation Reset","text":"<pre><code>history = History()\n\n# First conversation\npredictor(question=\"What is Python?\", history=history)\n\n# Reset for new topic\nhistory.clear()\n\n# New conversation with no context\npredictor(question=\"What is JavaScript?\", history=history)\n</code></pre>"},{"location":"examples/history/#history-with-tool-calling","title":"History with Tool Calling","text":"<pre><code>from udspy import tool\nfrom pydantic import Field\n\n@tool(name=\"Calculator\", description=\"Perform calculations\")\ndef calculator(operation: str = Field(...), a: float = Field(...), b: float = Field(...)) -&gt; float:\n    ops = {\"add\": a + b, \"subtract\": a - b, \"multiply\": a * b, \"divide\": a / b}\n    return ops[operation]\n\npredictor = Predict(QA, tools=[calculator])\nhistory = History()\n\n# Tool calls are automatically recorded in history\nresult = predictor(question=\"What is 15 times 23?\", history=history)\n# History now contains: user message, assistant tool call, tool result, final assistant answer\n\n# Next turn has full context including tool usage\nresult = predictor(question=\"Now add 100 to that\", history=history)\n</code></pre>"},{"location":"examples/history/#best-practices","title":"Best Practices","text":"<ol> <li>One History per Conversation Thread: Create a new <code>History</code> instance for each independent conversation</li> <li>Use <code>copy()</code> for Branching: When you want to explore different paths from the same starting point</li> <li>Clear When Changing Topics: Use <code>history.clear()</code> when starting a completely new conversation</li> <li>Pre-populate for Context: Add system messages or previous conversation history to set context</li> <li>Inspect Messages: Access <code>history.messages</code> directly when you need to debug or log conversations</li> </ol>"},{"location":"examples/history/#async-support","title":"Async Support","text":"<p>History works seamlessly with all async patterns:</p> <pre><code># Async streaming\nasync for event in predictor.astream(question=\"...\", history=history):\n    if isinstance(event, OutputStreamChunk):\n        print(event.delta, end=\"\", flush=True)\n\n# Async non-streaming\nresult = await predictor.aforward(question=\"...\", history=history)\n\n# Sync (uses asyncio.run internally)\nresult = predictor(question=\"...\", history=history)\n</code></pre>"},{"location":"examples/history/#examples","title":"Examples","text":"<p>See history.py for complete working examples.</p>"},{"location":"examples/react/","title":"ReAct (Reasoning and Acting)","text":"<p>ReAct is a powerful pattern for building LLM agents that can reason through multi-step problems and use tools to accomplish tasks. The name comes from combining Reasoning and Acting.</p>"},{"location":"examples/react/#overview","title":"Overview","text":"<p>The ReAct module enables you to build agents that:</p> <ul> <li>Reason iteratively: Think through problems step-by-step</li> <li>Use multiple tools: Call different tools to gather information or perform actions</li> <li>Handle ambiguity: Ask users for clarification when needed</li> <li>Require confirmation: Request user approval for destructive operations</li> <li>Save and restore state: Pause execution for user input and resume seamlessly</li> </ul>"},{"location":"examples/react/#how-react-works","title":"How ReAct Works","text":"<p>ReAct follows a thought \u2192 action \u2192 observation loop:</p> <ol> <li>Thought: The agent reasons about what to do next</li> <li>Action: The agent selects a tool and specifies arguments</li> <li>Observation: The tool returns a result</li> <li>Repeat: Continue until the task is complete</li> </ol> <p>All reasoning steps are tracked in a trajectory that provides context for subsequent decisions.</p>"},{"location":"examples/react/#basic-usage","title":"Basic Usage","text":"<pre><code>from pydantic import Field\nfrom udspy import InputField, OutputField, ReAct, Signature, tool\n\n# Define tools\n@tool(name=\"search\", description=\"Search for information\")\ndef search(query: str = Field(description=\"Search query\")) -&gt; str:\n    # Call search API\n    return f\"Search results for: {query}\"\n\n@tool(name=\"calculator\", description=\"Perform calculations\")\ndef calculator(expression: str = Field(description=\"Math expression\")) -&gt; str:\n    return str(eval(expression))\n\n# Define task signature\nclass ResearchTask(Signature):\n    \"\"\"Research a topic and provide a comprehensive answer.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\n# Create ReAct agent\nagent = ReAct(\n    ResearchTask,\n    tools=[search, calculator],\n    max_iters=10\n)\n\n# Execute\nresult = agent(question=\"What is Python and how many letters are in 'Python'?\")\nprint(result.answer)\n# The agent will:\n# 1. Search for \"Python\"\n# 2. Calculate len(\"Python\") = 6\n# 3. Synthesize an answer combining both results\n</code></pre>"},{"location":"examples/react/#string-signatures","title":"String Signatures","text":"<p>For quick prototyping, you can use string signatures:</p> <pre><code>agent = ReAct(\n    \"task -&gt; result\",  # Simple format: inputs -&gt; outputs\n    tools=[search, calculator]\n)\n\nresult = agent(task=\"Find information about React\")\nprint(result.result)\n</code></pre>"},{"location":"examples/react/#user-clarification-with-user-clarification","title":"User Clarification with user clarification","text":"<p>When the user's request is ambiguous, the agent can ask for clarification:</p> <pre><code>from udspy import ConfirmationRequired, ResumeState\n\nagent = ReAct(\n    ResearchTask,\n    tools=[search],\n    enable_user_clarification=True  # Enable clarification requests\n)\n\ntry:\n    result = agent(question=\"Tell me about it\")\nexcept ConfirmationRequired as e:\n    # Agent needs clarification\n    print(f\"Agent asks: {e.question}\")\n    # \"What topic would you like to know about?\"\n\n    # User provides clarification\n    response = \"The Python programming language\"\n\n    # Resume execution\n    result = agent.resume(user_response, e)\n    print(result.answer)\n</code></pre>"},{"location":"examples/react/#configuring-user-clarification","title":"Configuring user clarification","text":"<p>The user clarification tool is enabled by default and can be called by the agent whenever clarification is needed:</p> <pre><code>agent = ReAct(\n    ResearchTask,\n    tools=[search],\n    enable_user_clarification=True,  # Default: enabled\n)\n</code></pre> <p>To disable user clarification entirely:</p> <pre><code>agent = ReAct(\n    ResearchTask,\n    tools=[search],\n    enable_user_clarification=False  # No clarification requests\n)\n</code></pre>"},{"location":"examples/react/#tool-confirmation","title":"Tool Confirmation","text":"<p>For destructive or sensitive operations, you can require user confirmation:</p> <pre><code>@tool(\n    name=\"delete_file\",\n    description=\"Delete a file\",\n    require_confirmation=True  # Require confirmation\n)\ndef delete_file(path: str = Field(description=\"File path\")) -&gt; str:\n    os.remove(path)\n    return f\"Deleted {path}\"\n\nagent = ReAct(\n    ResearchTask,\n    tools=[delete_file]\n)\n\ntry:\n    result = agent(question=\"Delete /tmp/old_data.txt\")\nexcept ConfirmationRequired as e:\n    # Agent asks for confirmation\n    print(f\"Confirm: {e.question}\")\n    # \"Confirm execution of delete_file with args: {'path': '/tmp/old_data.txt'}? (yes/no)\"\n\n    # Check tool call info\n    if e.tool_call:\n        print(f\"Tool: {e.tool_call.name}\")\n        print(f\"Args: {e.tool_call.args}\")\n\n    # User confirms\n    result = agent.resume(\"yes\", e)\n</code></pre>"},{"location":"examples/react/#accessing-the-trajectory","title":"Accessing the Trajectory","text":"<p>The trajectory contains all reasoning steps and tool calls:</p> <pre><code>result = agent(question=\"What is 2 + 2?\")\n\n# Access trajectory\nfor i in range(10):  # Max iterations\n    observation_key = f\"observation_{i}\"\n    if observation_key not in result.trajectory:\n        break\n\n    print(f\"Step {i + 1}:\")\n    print(f\"  Reasoning: {result.trajectory.get(f'reasoning_{i}', '')}\")\n    print(f\"  Tool: {result.trajectory[f'tool_name_{i}']}\")\n    print(f\"  Args: {result.trajectory[f'tool_args_{i}']}\")\n    print(f\"  Observation: {result.trajectory[observation_key]}\")\n</code></pre> <p>Example trajectory: <pre><code>Step 1:\n  Reasoning: I need to calculate 2 + 2\n  Tool: calculator\n  Args: {'expression': '2 + 2'}\n  Observation: 4\n\nStep 2:\n  Reasoning: I have the answer\n  Tool: finish\n  Args: {}\n  Observation: Task completed\n</code></pre></p>"},{"location":"examples/react/#configuration-options","title":"Configuration Options","text":"<pre><code>agent = ReAct(\n    signature=ResearchTask,       # Task signature\n    tools=[search, calculator],   # Available tools\n    max_iters=10,                 # Maximum reasoning steps (default: 10)\n    enable_user_clarification=True       # Enable user clarification (default: True)\n)\n</code></pre>"},{"location":"examples/react/#parameters","title":"Parameters","text":"<ul> <li><code>signature</code>: Signature class or string format (<code>\"input -&gt; output\"</code>)</li> <li><code>tools</code>: List of tool functions (decorated with <code>@tool</code>) or <code>Tool</code> objects</li> <li><code>max_iters</code>: Maximum number of reasoning iterations before stopping</li> <li><code>enable_user_clarification</code>: Whether to enable the user clarification tool</li> </ul>"},{"location":"examples/react/#async-support","title":"Async Support","text":"<p>ReAct fully supports async execution:</p> <pre><code>import asyncio\n\nasync def main():\n    agent = ReAct(ResearchTask, tools=[search])\n\n    # Async forward\n    result = await agent.aforward(question=\"What is Python?\")\n    print(result.answer)\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/react/#built-in-tools","title":"Built-in Tools","text":"<p>Every ReAct agent automatically includes these tools:</p>"},{"location":"examples/react/#finish","title":"<code>finish</code>","text":"<p>Signals that the agent has collected enough information to answer:</p> <pre><code># Agent internally calls:\n# Tool: finish\n# Args: {}\n</code></pre> <p>This is automatically selected by the LLM when it has sufficient information.</p>"},{"location":"examples/react/#user-clarification-if-enabled","title":"user clarification (if enabled)","text":"<p>Requests clarification from the user:</p> <pre><code># Agent internally calls:\n# Tool: user_clarification\n# Args: {\"question\": \"What topic would you like to know about?\"}\n</code></pre>"},{"location":"examples/react/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"examples/react/#multi-tool-research","title":"Multi-Tool Research","text":"<pre><code>@tool(name=\"search_papers\", description=\"Search academic papers\")\ndef search_papers(query: str = Field(...)) -&gt; str:\n    return f\"Papers about: {query}\"\n\n@tool(name=\"summarize\", description=\"Summarize text\")\ndef summarize(text: str = Field(...)) -&gt; str:\n    return f\"Summary of: {text[:100]}...\"\n\nclass DeepResearch(Signature):\n    \"\"\"Conduct deep research on a scientific topic.\"\"\"\n    topic: str = InputField()\n    summary: str = OutputField()\n\nagent = ReAct(\n    DeepResearch,\n    tools=[search_papers, summarize],\n    max_iters=15  # More steps for complex research\n)\n\nresult = agent(topic=\"quantum computing\")\n</code></pre>"},{"location":"examples/react/#error-recovery","title":"Error Recovery","text":"<p>The agent automatically handles tool errors and can recover:</p> <pre><code>@tool(name=\"api_call\", description=\"Call external API\")\ndef api_call(endpoint: str = Field(...)) -&gt; str:\n    try:\n        # Simulated API call that might fail\n        if endpoint == \"invalid\":\n            raise ValueError(\"Invalid endpoint\")\n        return \"API response\"\n    except Exception as e:\n        # Error is returned as observation\n        raise\n\n# Agent will see error in observation and can:\n# 1. Try a different tool\n# 2. Retry with different args\n# 3. Ask user for help (using user_clarification tool)\n</code></pre>"},{"location":"examples/react/#state-management","title":"State Management","text":"<p>Save and restore execution state:</p> <pre><code>try:\n    result = agent(question=\"Delete important files\")\nexcept ConfirmationRequired as e:\n    # Save state\n    saved_state = e\n    saved_question = e.question\n    # Access ReAct-specific state from context\n    saved_trajectory = e.context[\"trajectory\"]\n    saved_iteration = e.context[\"iteration\"]\n    saved_input_args = e.context[\"input_args\"]\n\n    # Later, restore and continue\n    response = input(f\"{saved_question} \")\n    result = agent.resume(user_response, saved_state)\n</code></pre>"},{"location":"examples/react/#dspy-compatibility","title":"DSPy Compatibility","text":"<p>The <code>Tool</code> class includes DSPy-compatible aliases:</p> <pre><code>from udspy import Tool\n\n@tool(name=\"search\", description=\"Search tool\")\ndef search(query: str = Field(...)) -&gt; str:\n    return \"results\"\n\n# DSPy-style access\nprint(search.desc)   # Same as search.description\nprint(search.args)   # Dict of argument specs\n</code></pre>"},{"location":"examples/react/#best-practices","title":"Best Practices","text":"<ol> <li>Provide clear tool descriptions: The LLM uses descriptions to select tools</li> <li>Use Field() for parameters: Provide descriptions for all tool parameters</li> <li>Limit max_iters: Prevent infinite loops with reasonable iteration limits</li> <li>Enable confirmation for destructive ops: Use <code>require_confirmation=True</code></li> <li>Handle ConfirmationRequired: Always catch and handle clarification requests</li> <li>Use specific signatures: Clear input/output fields help the agent understand the task</li> <li>Test with mock tools: Use simple mock tools to validate agent logic</li> </ol>"},{"location":"examples/react/#common-patterns","title":"Common Patterns","text":""},{"location":"examples/react/#research-and-summarize","title":"Research and Summarize","text":"<pre><code>agent = ReAct(\n    \"query -&gt; summary\",\n    tools=[search, summarize]\n)\nresult = agent(query=\"Latest AI developments\")\n</code></pre>"},{"location":"examples/react/#data-analysis","title":"Data Analysis","text":"<pre><code>@tool(name=\"load_data\", description=\"Load dataset\")\ndef load_data(path: str = Field(...)) -&gt; str:\n    return \"data loaded\"\n\n@tool(name=\"analyze\", description=\"Analyze data\")\ndef analyze(metric: str = Field(...)) -&gt; str:\n    return \"analysis results\"\n\nagent = ReAct(\n    \"dataset, question -&gt; insights\",\n    tools=[load_data, analyze]\n)\n</code></pre>"},{"location":"examples/react/#multi-step-workflows","title":"Multi-Step Workflows","text":"<pre><code>@tool(name=\"step1\", description=\"First step\")\ndef step1() -&gt; str: return \"step1 done\"\n\n@tool(name=\"step2\", description=\"Second step\")\ndef step2(input: str = Field(...)) -&gt; str: return \"step2 done\"\n\n@tool(name=\"step3\", description=\"Third step\")\ndef step3(input: str = Field(...)) -&gt; str: return \"step3 done\"\n\nagent = ReAct(\n    \"task -&gt; result\",\n    tools=[step1, step2, step3],\n    max_iters=20\n)\n</code></pre>"},{"location":"examples/react/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/react/#agent-doesnt-finish","title":"Agent doesn't finish","text":"<p>Increase <code>max_iters</code> or simplify the task:</p> <pre><code>agent = ReAct(signature, tools=tools, max_iters=20)\n</code></pre>"},{"location":"examples/react/#too-many-tool-calls","title":"Too many tool calls","text":"<p>Reduce <code>max_iters</code> or improve tool descriptions:</p> <pre><code>@tool(\n    name=\"search\",\n    description=\"Search ONLY when you need external information. Use for factual queries.\"\n)\n</code></pre>"},{"location":"examples/react/#agent-asks-for-clarification-too-often","title":"Agent asks for clarification too often","text":"<p>Disable or restrict user clarification:</p> <pre><code>agent = ReAct(\n    signature,\n    tools=tools,\n    enable_user_clarification=False  # Disable entirely\n)\n</code></pre> <p>Or disable it completely if the agent should never ask for clarification:</p> <pre><code>agent = ReAct(\n    signature,\n    tools=tools,\n    enable_user_clarification=False  # Disable user clarification\n)\n</code></pre>"},{"location":"examples/react/#loop-based-confirmation-handling","title":"Loop-Based Confirmation Handling","text":"<p>For applications requiring multiple confirmations or interactive workflows, use the loop-based pattern with <code>resume_state</code>:</p>"},{"location":"examples/react/#basic-loop-pattern","title":"Basic Loop Pattern","text":"<pre><code>from udspy import ConfirmationRequired, ResumeState\n\nresume_state = None\n\nwhile True:\n    try:\n        result = agent(\n            question=\"Your task here\",\n            resume_state=resume_state\n        )\n        print(f\"Success: {result.answer}\")\n        break\n\n    except ConfirmationRequired as e:\n        print(f\"Confirmation needed: {e.question}\")\n        response = input(\"Your response (yes/no): \")\n        resume_state = ResumeState(e, response)\n</code></pre>"},{"location":"examples/react/#advantages-of-loop-pattern","title":"Advantages of Loop Pattern","text":"<ul> <li>Multiple confirmations: Handle as many confirmations as needed</li> <li>Uniform handling: Same code path for all confirmations</li> <li>Stateless API calls: Ideal for web APIs and distributed systems</li> <li>Simpler control flow: No need to track separate resume calls</li> </ul>"},{"location":"examples/react/#alternative-confirmation-patterns","title":"Alternative Confirmation Patterns","text":"<p>Using <code>respond_to_confirmation()</code> (current approach): <pre><code>from udspy import respond_to_confirmation\n\ntry:\n    result = await agent.aforward(question=\"Delete files\")\nexcept ConfirmationRequired as e:\n    respond_to_confirmation(e.confirmation_id, approved=True)\n    result = await agent.aforward(question=\"Delete files\")\n</code></pre></p> <p>Using loop pattern (uniform): <pre><code>resume_state = None\n\nwhile True:\n    try:\n        result = await agent.aforward(\n            question=\"Delete files\",\n            resume_state=resume_state\n        )\n        break\n    except ConfirmationRequired as e:\n        response = input(f\"{e.question} (yes/no): \")\n        resume_state = ResumeState(e, response)\n</code></pre></p> <p>The loop pattern is especially useful when: - You expect multiple confirmations - Building interactive CLIs - Implementing web API endpoints - Want predictable, uniform handling</p> <p>For comprehensive examples and patterns, see the Confirmation Examples guide.</p>"},{"location":"examples/react/#see-also","title":"See Also","text":"<ul> <li>Confirmation Examples - Detailed confirmation patterns</li> <li>Confirmation Architecture - Design and implementation</li> <li>Tool Calling Guide - Creating custom tools</li> <li>Chain of Thought - Simpler reasoning module</li> <li>ReAct API Reference - Full API documentation</li> </ul>"},{"location":"examples/streaming/","title":"Streaming Examples","text":"<p>Learn how to use streaming for better user experience.</p>"},{"location":"examples/streaming/#basic-streaming","title":"Basic Streaming","text":"<pre><code>import asyncio\nfrom udspy import StreamingPredict, Signature, InputField, OutputField\n\nclass QA(Signature):\n    \"\"\"Answer questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\nasync def main():\n    predictor = StreamingPredict(QA)\n\n    async for chunk in predictor.stream(question=\"What is AI?\"):\n        if isinstance(chunk, OutputStreamChunk):\n            print(chunk.delta, end=\"\", flush=True)\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/streaming/#multi-field-streaming","title":"Multi-Field Streaming","text":"<p>Stream reasoning and answer separately:</p> <pre><code>class ReasonedQA(Signature):\n    \"\"\"Answer with reasoning.\"\"\"\n    question: str = InputField()\n    reasoning: str = OutputField()\n    answer: str = OutputField()\n\nasync def main():\n    predictor = StreamingPredict(ReasonedQA)\n\n    print(\"Question: What is the sum of first 10 primes?\\n\")\n\n    async for item in predictor.stream(\n        question=\"What is the sum of first 10 primes?\"\n    ):\n        if isinstance(item, OutputStreamChunk):\n            if item.field_name == \"reasoning\":\n                print(f\"\ud83d\udcad {item.delta}\", end=\"\", flush=True)\n            elif item.field_name == \"answer\":\n                print(f\"\\n\u2713 {item.delta}\", end=\"\", flush=True)\n\n            if item.is_complete:\n                print()  # Newline after field completes\n\n        elif isinstance(item, Prediction):\n            print(f\"\\n\\nFinal: {item.answer}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/streaming/#web-application-integration","title":"Web Application Integration","text":"<p>Use streaming in a web application:</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\n\napp = FastAPI()\n\n@app.get(\"/ask\")\nasync def ask_question(question: str):\n    async def generate():\n        predictor = StreamingPredict(QA)\n        async for chunk in predictor.stream(question=question):\n            if isinstance(chunk, OutputStreamChunk) and not chunk.is_complete:\n                # chunk.delta contains the new incremental text\n                # chunk.content contains the full accumulated text so far\n                yield f\"data: {chunk.delta}\\n\\n\"\n\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n</code></pre> <p>See the full example in the repository.</p>"},{"location":"examples/tool_calling/","title":"Tool Calling","text":"<p>Learn how to use OpenAI's native tool calling with udspy.</p>"},{"location":"examples/tool_calling/#overview","title":"Overview","text":"<p>Tool calling in udspy follows the OpenAI tool calling pattern - a multi-turn conversation where the LLM requests tool execution and you provide the results:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Step 1:   \u2502  You: \"What is 157 \u00d7 234?\"\n\u2502  First Call \u2502  LLM: \"I need to call Calculator(multiply, 157, 234)\"\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Step 2:   \u2502  You execute: calculator(\"multiply\", 157, 234) \u2192 36738\n\u2502  Execute    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Step 3:   \u2502  You: \"Calculator returned 36738\"\n\u2502 Second Call \u2502  LLM: \"The answer is 36,738\"\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"examples/tool_calling/#two-ways-to-use-tools","title":"Two Ways to Use Tools","text":"<p>udspy supports two approaches to tool calling:</p> <ol> <li>Automatic Execution with <code>@tool</code> decorator (Recommended) - Tools are automatically executed</li> <li>Manual Execution with Pydantic models - You handle tool execution yourself</li> </ol>"},{"location":"examples/tool_calling/#automatic-tool-execution-recommended","title":"Automatic Tool Execution (Recommended)","text":"<p>Use the <code>@tool</code> decorator to mark functions as executable tools. udspy will automatically execute them and handle multi-turn conversations:</p> <pre><code>from pydantic import Field\nfrom udspy import tool, Predict, Signature, InputField, OutputField\n\n@tool(name=\"Calculator\", description=\"Perform arithmetic operations\")\ndef calculator(\n    operation: str = Field(description=\"add, subtract, multiply, or divide\"),\n    a: float = Field(description=\"First number\"),\n    b: float = Field(description=\"Second number\"),\n) -&gt; float:\n    \"\"\"Execute calculator operation.\"\"\"\n    ops = {\"add\": a + b, \"subtract\": a - b, \"multiply\": a * b, \"divide\": a / b}\n    return ops[operation]\n\nclass MathQuery(Signature):\n    \"\"\"Answer math questions.\"\"\"\n    question: str = InputField()\n    answer: str = OutputField()\n\n# Tools decorated with @tool are automatically executed\npredictor = Predict(MathQuery, tools=[calculator])\nresult = predictor(question=\"What is 157 times 234?\")\nprint(result.answer)  # \"The answer is 36738\"\n</code></pre> <p>The predictor automatically: 1. Detects when the LLM wants to call a tool 2. Executes the tool function 3. Sends the result back to the LLM 4. Returns the final answer</p>"},{"location":"examples/tool_calling/#optional-tool-execution","title":"Optional Tool Execution","text":"<p>You can control whether tools are automatically executed:</p> <pre><code># Default: auto_execute_tools=True\nresult = predictor(question=\"What is 5 + 3?\")\nprint(result.answer)  # \"The answer is 8\"\n\n# Get tool calls without execution\nresult = predictor(question=\"What is 5 + 3?\", auto_execute_tools=False)\nif result.native_tool_calls:\n    print(f\"LLM wants to call: {result.native_tool_calls[0].name}\")\n    print(f\"With arguments: {result.native_tool_calls[0].args}\")\n    # Now you can execute manually or log/analyze the tool calls\n</code></pre> <p>This is useful for: - Requiring user approval before executing tools (see confirmation examples) - Logging or analyzing tool usage patterns - Implementing custom execution logic - Rate limiting or caching tool results</p>"},{"location":"examples/tool_calling/#manual-tool-execution","title":"Manual Tool Execution","text":"<p>Define tools as Pydantic models when you want full control:</p>"},{"location":"examples/tool_calling/#1-define-the-tool-schema","title":"1. Define the Tool Schema","text":"<p>This describes the tool to the LLM - what parameters it takes:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass Calculator(BaseModel):\n    \"\"\"Perform arithmetic operations.\"\"\"\n    operation: str = Field(description=\"add, subtract, multiply, divide\")\n    a: float = Field(description=\"First number\")\n    b: float = Field(description=\"Second number\")\n</code></pre>"},{"location":"examples/tool_calling/#2-implement-the-tool-function","title":"2. Implement the Tool Function","text":"<p>This is the actual Python code that executes:</p> <pre><code>def calculator(operation: str, a: float, b: float) -&gt; float:\n    \"\"\"Execute calculator operation.\"\"\"\n    ops = {\n        \"add\": a + b,\n        \"subtract\": a - b,\n        \"multiply\": a * b,\n        \"divide\": a / b if b != 0 else float(\"inf\"),\n    }\n    return ops[operation]\n</code></pre>"},{"location":"examples/tool_calling/#3-handle-the-multi-turn-conversation","title":"3. Handle the Multi-Turn Conversation","text":"<pre><code># First call - LLM decides what to do\npredictor = Predict(QA, tools=[Calculator])\nresult = predictor(question=\"What is 5 + 3?\", auto_execute_tools=False)\n\nif result.native_tool_calls:\n    # LLM requested a tool call\n    for tool_call in result.native_tool_calls:\n        # Execute YOUR implementation\n        tool_result = calculator(**tool_call.args)\n\n        # Send result back to LLM (requires manual message construction)\n        # See examples/tool_calling_manual.py for complete implementation\n</code></pre>"},{"location":"examples/tool_calling/#multiple-tools","title":"Multiple Tools","text":"<p>Provide multiple tools for different operations:</p> <pre><code>@tool(name=\"Calculator\", description=\"Perform arithmetic operations\")\ndef calculator(operation: str, a: float, b: float) -&gt; float:\n    ops = {\"add\": a + b, \"subtract\": a - b, \"multiply\": a * b, \"divide\": a / b}\n    return ops[operation]\n\n@tool(name=\"WebSearch\", description=\"Search the web\")\ndef web_search(query: str = Field(description=\"Search query\")) -&gt; str:\n    # Your web search implementation\n    return f\"Search results for: {query}\"\n\n@tool(name=\"DateInfo\", description=\"Get current date/time\")\ndef date_info(timezone: str = Field(description=\"Timezone name\")) -&gt; str:\n    # Your date/time implementation\n    return f\"Current time in {timezone}\"\n\npredictor = Predict(\n    signature,\n    tools=[calculator, web_search, date_info],\n)\n</code></pre>"},{"location":"examples/tool_calling/#key-points","title":"Key Points","text":"<ol> <li>The Schema != The Implementation</li> <li>Schema (Pydantic model or <code>@tool</code> params): Describes the tool to the LLM</li> <li> <p>Implementation (Python function): Your actual code</p> </li> <li> <p>It's Multi-Turn</p> </li> <li>Call 1: LLM decides to use a tool</li> <li>You execute the tool</li> <li> <p>Call 2: Send results back to get final answer</p> </li> <li> <p>You Control Execution</p> </li> <li>LLM only requests tool calls</li> <li>YOU decide if/how to execute them</li> <li>YOU send results back</li> </ol>"},{"location":"examples/tool_calling/#why-this-design","title":"Why This Design?","text":"<p>This gives you full control: - Validate tool calls before executing - Handle errors gracefully - Implement tools however you want (API calls, database queries, etc.) - Add logging, rate limiting, security checks, etc.</p> <p>The LLM just requests the tool - you're in charge of everything else!</p>"},{"location":"examples/tool_calling/#complete-examples","title":"Complete Examples","text":"<p>See the example files:</p> <ul> <li><code>tool_calling_auto.py</code> - Automatic tool execution with @tool decorator (recommended)</li> <li><code>tool_calling_manual.py</code> - Manual tool execution with full control</li> <li><code>confirmation_loop.py</code> - Requiring user approval before tool execution</li> </ul>"},{"location":"examples/tool_calling/#advanced-features","title":"Advanced Features","text":""},{"location":"examples/tool_calling/#async-tools","title":"Async Tools","text":"<p>Tools can be async functions:</p> <pre><code>@tool(name=\"AsyncSearch\", description=\"Search with async API\")\nasync def async_search(query: str) -&gt; str:\n    # Async implementation\n    result = await some_async_api_call(query)\n    return result\n</code></pre>"},{"location":"examples/tool_calling/#tool-confirmation","title":"Tool Confirmation","text":"<p>Require user confirmation before executing certain tools:</p> <pre><code>@tool(\n    name=\"DeleteFile\",\n    description=\"Delete a file\",\n    require_confirmation=True  # Requires user approval\n)\ndef delete_file(path: str) -&gt; str:\n    # Will only execute after user confirmation\n    os.remove(path)\n    return f\"Deleted {path}\"\n</code></pre> <p>See <code>examples/confirmation_loop.py</code> for a complete example.</p>"}]}